{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics \n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>...</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24438</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>International</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24439</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>International</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24440</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>International</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24441</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>International</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24442</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>International</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>696.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24443 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      iso_code continent       location        date  total_cases  new_cases  \\\n",
       "0          AFG      Asia    Afghanistan  2019-12-31          0.0        0.0   \n",
       "1          AFG      Asia    Afghanistan  2020-01-01          0.0        0.0   \n",
       "2          AFG      Asia    Afghanistan  2020-01-02          0.0        0.0   \n",
       "3          AFG      Asia    Afghanistan  2020-01-03          0.0        0.0   \n",
       "4          AFG      Asia    Afghanistan  2020-01-04          0.0        0.0   \n",
       "...        ...       ...            ...         ...          ...        ...   \n",
       "24438      NaN       NaN  International  2020-02-28        705.0        0.0   \n",
       "24439      NaN       NaN  International  2020-02-29        705.0        0.0   \n",
       "24440      NaN       NaN  International  2020-03-01        705.0        0.0   \n",
       "24441      NaN       NaN  International  2020-03-02        705.0        0.0   \n",
       "24442      NaN       NaN  International  2020-03-10        696.0       -9.0   \n",
       "\n",
       "       total_deaths  new_deaths  total_cases_per_million  \\\n",
       "0               0.0         0.0                      0.0   \n",
       "1               0.0         0.0                      0.0   \n",
       "2               0.0         0.0                      0.0   \n",
       "3               0.0         0.0                      0.0   \n",
       "4               0.0         0.0                      0.0   \n",
       "...             ...         ...                      ...   \n",
       "24438           4.0         0.0                      NaN   \n",
       "24439           6.0         2.0                      NaN   \n",
       "24440           6.0         0.0                      NaN   \n",
       "24441           6.0         0.0                      NaN   \n",
       "24442           7.0         1.0                      NaN   \n",
       "\n",
       "       new_cases_per_million  ...  aged_70_older  gdp_per_capita  \\\n",
       "0                        0.0  ...          1.337        1803.987   \n",
       "1                        0.0  ...          1.337        1803.987   \n",
       "2                        0.0  ...          1.337        1803.987   \n",
       "3                        0.0  ...          1.337        1803.987   \n",
       "4                        0.0  ...          1.337        1803.987   \n",
       "...                      ...  ...            ...             ...   \n",
       "24438                    NaN  ...            NaN             NaN   \n",
       "24439                    NaN  ...            NaN             NaN   \n",
       "24440                    NaN  ...            NaN             NaN   \n",
       "24441                    NaN  ...            NaN             NaN   \n",
       "24442                    NaN  ...            NaN             NaN   \n",
       "\n",
       "       extreme_poverty  cvd_death_rate  diabetes_prevalence  female_smokers  \\\n",
       "0                  NaN         597.029                 9.59             NaN   \n",
       "1                  NaN         597.029                 9.59             NaN   \n",
       "2                  NaN         597.029                 9.59             NaN   \n",
       "3                  NaN         597.029                 9.59             NaN   \n",
       "4                  NaN         597.029                 9.59             NaN   \n",
       "...                ...             ...                  ...             ...   \n",
       "24438              NaN             NaN                  NaN             NaN   \n",
       "24439              NaN             NaN                  NaN             NaN   \n",
       "24440              NaN             NaN                  NaN             NaN   \n",
       "24441              NaN             NaN                  NaN             NaN   \n",
       "24442              NaN             NaN                  NaN             NaN   \n",
       "\n",
       "       male_smokers  handwashing_facilities hospital_beds_per_thousand  \\\n",
       "0               NaN                  37.746                        0.5   \n",
       "1               NaN                  37.746                        0.5   \n",
       "2               NaN                  37.746                        0.5   \n",
       "3               NaN                  37.746                        0.5   \n",
       "4               NaN                  37.746                        0.5   \n",
       "...             ...                     ...                        ...   \n",
       "24438           NaN                     NaN                        NaN   \n",
       "24439           NaN                     NaN                        NaN   \n",
       "24440           NaN                     NaN                        NaN   \n",
       "24441           NaN                     NaN                        NaN   \n",
       "24442           NaN                     NaN                        NaN   \n",
       "\n",
       "       life_expectancy  \n",
       "0                64.83  \n",
       "1                64.83  \n",
       "2                64.83  \n",
       "3                64.83  \n",
       "4                64.83  \n",
       "...                ...  \n",
       "24438              NaN  \n",
       "24439              NaN  \n",
       "24440              NaN  \n",
       "24441              NaN  \n",
       "24442              NaN  \n",
       "\n",
       "[24443 rows x 34 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(file):\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "dataFrame = load('owid-covid-data.csv')\n",
    "dataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame\n",
    "dataFrame.drop(dataFrame[dataFrame.location == \"International\"].index, inplace=True)\n",
    "dataFrame.drop(dataFrame[dataFrame.location == \"World\"].index, inplace=True)\n",
    "dataFrame = dataFrame.drop(columns=['iso_code'])\n",
    "dataFrame = dataFrame.drop(columns=['continent'])\n",
    "# dataFrame = dataFrame.drop(columns=['new_cases'])\n",
    "# dataFrame = dataFrame.drop(columns=['new_deaths'])\n",
    "dataFrame = dataFrame.drop(columns=['total_cases'])\n",
    "dataFrame = dataFrame.drop(columns=['total_deaths'])\n",
    "\n",
    "dataFrame = dataFrame.drop(columns=['total_cases_per_million'])\n",
    "dataFrame = dataFrame.drop(columns=['new_deaths_per_million'])\n",
    "dataFrame = dataFrame.drop(columns=['total_tests'])\n",
    "dataFrame = dataFrame.drop(columns=['new_tests'])\n",
    "dataFrame = dataFrame.drop(columns=['total_tests_per_thousand'])\n",
    "dataFrame = dataFrame.drop(columns=['new_tests_per_thousand'])\n",
    "dataFrame = dataFrame.drop(columns=['new_tests_smoothed'])\n",
    "dataFrame = dataFrame.drop(columns=['tests_units'])\n",
    "dataFrame = dataFrame.drop(columns=['life_expectancy'])\n",
    "dataFrame = dataFrame.drop(columns=['new_cases_per_million'])\n",
    "dataFrame = dataFrame.drop(columns=['new_tests_smoothed_per_thousand'])\n",
    "dataFrame = dataFrame.drop(columns=['total_deaths_per_million'])\n",
    "\n",
    "\n",
    "\n",
    "# dataFrame.insert(0, 'Days', range(1, 1 + len(dataFrame)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24204</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24205</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24206</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-15</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24207</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24208</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24209 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          location        date  new_cases  new_deaths  stringency_index  \\\n",
       "0      Afghanistan  2019-12-31        0.0         0.0               NaN   \n",
       "1      Afghanistan  2020-01-01        0.0         0.0               0.0   \n",
       "2      Afghanistan  2020-01-02        0.0         0.0               0.0   \n",
       "3      Afghanistan  2020-01-03        0.0         0.0               0.0   \n",
       "4      Afghanistan  2020-01-04        0.0         0.0               0.0   \n",
       "...            ...         ...        ...         ...               ...   \n",
       "24204     Zimbabwe  2020-06-13       11.0         0.0               NaN   \n",
       "24205     Zimbabwe  2020-06-14       13.0         0.0               NaN   \n",
       "24206     Zimbabwe  2020-06-15       27.0         0.0               NaN   \n",
       "24207     Zimbabwe  2020-06-16        4.0         0.0               NaN   \n",
       "24208     Zimbabwe  2020-06-17        7.0         0.0               NaN   \n",
       "\n",
       "       population  population_density  median_age  aged_65_older  \\\n",
       "0      38928341.0              54.422        18.6          2.581   \n",
       "1      38928341.0              54.422        18.6          2.581   \n",
       "2      38928341.0              54.422        18.6          2.581   \n",
       "3      38928341.0              54.422        18.6          2.581   \n",
       "4      38928341.0              54.422        18.6          2.581   \n",
       "...           ...                 ...         ...            ...   \n",
       "24204  14862927.0              42.729        19.6          2.822   \n",
       "24205  14862927.0              42.729        19.6          2.822   \n",
       "24206  14862927.0              42.729        19.6          2.822   \n",
       "24207  14862927.0              42.729        19.6          2.822   \n",
       "24208  14862927.0              42.729        19.6          2.822   \n",
       "\n",
       "       aged_70_older  gdp_per_capita  extreme_poverty  cvd_death_rate  \\\n",
       "0              1.337        1803.987              NaN         597.029   \n",
       "1              1.337        1803.987              NaN         597.029   \n",
       "2              1.337        1803.987              NaN         597.029   \n",
       "3              1.337        1803.987              NaN         597.029   \n",
       "4              1.337        1803.987              NaN         597.029   \n",
       "...              ...             ...              ...             ...   \n",
       "24204          1.882        1899.775             21.4         307.846   \n",
       "24205          1.882        1899.775             21.4         307.846   \n",
       "24206          1.882        1899.775             21.4         307.846   \n",
       "24207          1.882        1899.775             21.4         307.846   \n",
       "24208          1.882        1899.775             21.4         307.846   \n",
       "\n",
       "       diabetes_prevalence  female_smokers  male_smokers  \\\n",
       "0                     9.59             NaN           NaN   \n",
       "1                     9.59             NaN           NaN   \n",
       "2                     9.59             NaN           NaN   \n",
       "3                     9.59             NaN           NaN   \n",
       "4                     9.59             NaN           NaN   \n",
       "...                    ...             ...           ...   \n",
       "24204                 1.82             1.6          30.7   \n",
       "24205                 1.82             1.6          30.7   \n",
       "24206                 1.82             1.6          30.7   \n",
       "24207                 1.82             1.6          30.7   \n",
       "24208                 1.82             1.6          30.7   \n",
       "\n",
       "       handwashing_facilities  hospital_beds_per_thousand  \n",
       "0                      37.746                         0.5  \n",
       "1                      37.746                         0.5  \n",
       "2                      37.746                         0.5  \n",
       "3                      37.746                         0.5  \n",
       "4                      37.746                         0.5  \n",
       "...                       ...                         ...  \n",
       "24204                  36.791                         1.7  \n",
       "24205                  36.791                         1.7  \n",
       "24206                  36.791                         1.7  \n",
       "24207                  36.791                         1.7  \n",
       "24208                  36.791                         1.7  \n",
       "\n",
       "[24209 rows x 18 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location                      24209\n",
       "date                          24209\n",
       "new_cases                     23991\n",
       "new_deaths                    23991\n",
       "stringency_index              19463\n",
       "population                    24209\n",
       "population_density            23229\n",
       "median_age                    21954\n",
       "aged_65_older                 21645\n",
       "aged_70_older                 21841\n",
       "gdp_per_capita                21713\n",
       "extreme_poverty               14448\n",
       "cvd_death_rate                21976\n",
       "diabetes_prevalence           22691\n",
       "female_smokers                17660\n",
       "male_smokers                  17458\n",
       "handwashing_facilities         9713\n",
       "hospital_beds_per_thousand    20129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location                          0\n",
       "date                              0\n",
       "new_cases                       218\n",
       "new_deaths                      218\n",
       "stringency_index               4746\n",
       "population                        0\n",
       "population_density              980\n",
       "median_age                     2255\n",
       "aged_65_older                  2564\n",
       "aged_70_older                  2368\n",
       "gdp_per_capita                 2496\n",
       "extreme_poverty                9761\n",
       "cvd_death_rate                 2233\n",
       "diabetes_prevalence            1518\n",
       "female_smokers                 6549\n",
       "male_smokers                   6751\n",
       "handwashing_facilities        14496\n",
       "hospital_beds_per_thousand     4080\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24204</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24205</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24206</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-15</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24207</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24208</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24209 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          location        date  new_cases  new_deaths  stringency_index  \\\n",
       "0      Afghanistan  2019-12-31        0.0         0.0               NaN   \n",
       "1      Afghanistan  2020-01-01        0.0         0.0               0.0   \n",
       "2      Afghanistan  2020-01-02        0.0         0.0               0.0   \n",
       "3      Afghanistan  2020-01-03        0.0         0.0               0.0   \n",
       "4      Afghanistan  2020-01-04        0.0         0.0               0.0   \n",
       "...            ...         ...        ...         ...               ...   \n",
       "24204     Zimbabwe  2020-06-13       11.0         0.0               NaN   \n",
       "24205     Zimbabwe  2020-06-14       13.0         0.0               NaN   \n",
       "24206     Zimbabwe  2020-06-15       27.0         0.0               NaN   \n",
       "24207     Zimbabwe  2020-06-16        4.0         0.0               NaN   \n",
       "24208     Zimbabwe  2020-06-17        7.0         0.0               NaN   \n",
       "\n",
       "       population  population_density  median_age  aged_65_older  \\\n",
       "0      38928341.0              54.422        18.6          2.581   \n",
       "1      38928341.0              54.422        18.6          2.581   \n",
       "2      38928341.0              54.422        18.6          2.581   \n",
       "3      38928341.0              54.422        18.6          2.581   \n",
       "4      38928341.0              54.422        18.6          2.581   \n",
       "...           ...                 ...         ...            ...   \n",
       "24204  14862927.0              42.729        19.6          2.822   \n",
       "24205  14862927.0              42.729        19.6          2.822   \n",
       "24206  14862927.0              42.729        19.6          2.822   \n",
       "24207  14862927.0              42.729        19.6          2.822   \n",
       "24208  14862927.0              42.729        19.6          2.822   \n",
       "\n",
       "       aged_70_older  gdp_per_capita  extreme_poverty  cvd_death_rate  \\\n",
       "0              1.337        1803.987              NaN         597.029   \n",
       "1              1.337        1803.987              NaN         597.029   \n",
       "2              1.337        1803.987              NaN         597.029   \n",
       "3              1.337        1803.987              NaN         597.029   \n",
       "4              1.337        1803.987              NaN         597.029   \n",
       "...              ...             ...              ...             ...   \n",
       "24204          1.882        1899.775             21.4         307.846   \n",
       "24205          1.882        1899.775             21.4         307.846   \n",
       "24206          1.882        1899.775             21.4         307.846   \n",
       "24207          1.882        1899.775             21.4         307.846   \n",
       "24208          1.882        1899.775             21.4         307.846   \n",
       "\n",
       "       diabetes_prevalence  female_smokers  male_smokers  \\\n",
       "0                     9.59             NaN           NaN   \n",
       "1                     9.59             NaN           NaN   \n",
       "2                     9.59             NaN           NaN   \n",
       "3                     9.59             NaN           NaN   \n",
       "4                     9.59             NaN           NaN   \n",
       "...                    ...             ...           ...   \n",
       "24204                 1.82             1.6          30.7   \n",
       "24205                 1.82             1.6          30.7   \n",
       "24206                 1.82             1.6          30.7   \n",
       "24207                 1.82             1.6          30.7   \n",
       "24208                 1.82             1.6          30.7   \n",
       "\n",
       "       handwashing_facilities  hospital_beds_per_thousand  Days  \n",
       "0                      37.746                         0.5     1  \n",
       "1                      37.746                         0.5     1  \n",
       "2                      37.746                         0.5     1  \n",
       "3                      37.746                         0.5     1  \n",
       "4                      37.746                         0.5     1  \n",
       "...                       ...                         ...   ...  \n",
       "24204                  36.791                         1.7     1  \n",
       "24205                  36.791                         1.7     1  \n",
       "24206                  36.791                         1.7     1  \n",
       "24207                  36.791                         1.7     1  \n",
       "24208                  36.791                         1.7     1  \n",
       "\n",
       "[24209 rows x 19 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(dataFrame[dataFrame[\"location\"] == \"United States\"])\n",
    "dataFrame[\"Days\"] = 1\n",
    "# dataFrame['stringency_index'].fillna(0,inplace=True)\n",
    "dataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24204</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24205</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24206</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-15</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24207</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24208</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-06-17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24209 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          location        date  new_cases  new_deaths  stringency_index  \\\n",
       "0      Afghanistan  2019-12-31        0.0         0.0               NaN   \n",
       "1      Afghanistan  2020-01-01        0.0         0.0               0.0   \n",
       "2      Afghanistan  2020-01-02        0.0         0.0               0.0   \n",
       "3      Afghanistan  2020-01-03        0.0         0.0               0.0   \n",
       "4      Afghanistan  2020-01-04        0.0         0.0               0.0   \n",
       "...            ...         ...        ...         ...               ...   \n",
       "24204     Zimbabwe  2020-06-13       11.0         0.0               NaN   \n",
       "24205     Zimbabwe  2020-06-14       13.0         0.0               NaN   \n",
       "24206     Zimbabwe  2020-06-15       27.0         0.0               NaN   \n",
       "24207     Zimbabwe  2020-06-16        4.0         0.0               NaN   \n",
       "24208     Zimbabwe  2020-06-17        7.0         0.0               NaN   \n",
       "\n",
       "       population  population_density  median_age  aged_65_older  \\\n",
       "0      38928341.0              54.422        18.6          2.581   \n",
       "1      38928341.0              54.422        18.6          2.581   \n",
       "2      38928341.0              54.422        18.6          2.581   \n",
       "3      38928341.0              54.422        18.6          2.581   \n",
       "4      38928341.0              54.422        18.6          2.581   \n",
       "...           ...                 ...         ...            ...   \n",
       "24204  14862927.0              42.729        19.6          2.822   \n",
       "24205  14862927.0              42.729        19.6          2.822   \n",
       "24206  14862927.0              42.729        19.6          2.822   \n",
       "24207  14862927.0              42.729        19.6          2.822   \n",
       "24208  14862927.0              42.729        19.6          2.822   \n",
       "\n",
       "       aged_70_older  gdp_per_capita  extreme_poverty  cvd_death_rate  \\\n",
       "0              1.337        1803.987              NaN         597.029   \n",
       "1              1.337        1803.987              NaN         597.029   \n",
       "2              1.337        1803.987              NaN         597.029   \n",
       "3              1.337        1803.987              NaN         597.029   \n",
       "4              1.337        1803.987              NaN         597.029   \n",
       "...              ...             ...              ...             ...   \n",
       "24204          1.882        1899.775             21.4         307.846   \n",
       "24205          1.882        1899.775             21.4         307.846   \n",
       "24206          1.882        1899.775             21.4         307.846   \n",
       "24207          1.882        1899.775             21.4         307.846   \n",
       "24208          1.882        1899.775             21.4         307.846   \n",
       "\n",
       "       diabetes_prevalence  female_smokers  male_smokers  \\\n",
       "0                     9.59             NaN           NaN   \n",
       "1                     9.59             NaN           NaN   \n",
       "2                     9.59             NaN           NaN   \n",
       "3                     9.59             NaN           NaN   \n",
       "4                     9.59             NaN           NaN   \n",
       "...                    ...             ...           ...   \n",
       "24204                 1.82             1.6          30.7   \n",
       "24205                 1.82             1.6          30.7   \n",
       "24206                 1.82             1.6          30.7   \n",
       "24207                 1.82             1.6          30.7   \n",
       "24208                 1.82             1.6          30.7   \n",
       "\n",
       "       handwashing_facilities  hospital_beds_per_thousand  Days  \n",
       "0                      37.746                         0.5     1  \n",
       "1                      37.746                         0.5     1  \n",
       "2                      37.746                         0.5     1  \n",
       "3                      37.746                         0.5     1  \n",
       "4                      37.746                         0.5     1  \n",
       "...                       ...                         ...   ...  \n",
       "24204                  36.791                         1.7     1  \n",
       "24205                  36.791                         1.7     1  \n",
       "24206                  36.791                         1.7     1  \n",
       "24207                  36.791                         1.7     1  \n",
       "24208                  36.791                         1.7     1  \n",
       "\n",
       "[24209 rows x 19 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupByLocation = dataFrame.loc[:, dataFrame.columns.intersection(['location','new_cases', \"new_deaths\"])]\n",
    "stringencyDF = dataFrame.loc[:, dataFrame.columns.intersection(['location', \"stringency_index\"])]\n",
    "ByDays = dataFrame.loc[:, dataFrame.columns.intersection(['location', \"Days\"])]\n",
    "groupByLocationAverage = groupByLocation.groupby(['location']).mean()\n",
    "groupByLocation = groupByLocation.groupby(['location']).sum()\n",
    "groupByStringency = stringencyDF.groupby(['location']).mean()\n",
    "groupByRest = dataFrame.drop(columns=['date', \"new_cases\", \"new_deaths\", \"stringency_index\"])\n",
    "groupByRest = groupByRest.groupby(['location']).mean()\n",
    "groupByDays = ByDays.groupby(['location']).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>DaysTotal_Days</th>\n",
       "      <th>AverageInfectionRate</th>\n",
       "      <th>AverageDeathRate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>26310.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>44.956867</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.50</td>\n",
       "      <td>160</td>\n",
       "      <td>164.437500</td>\n",
       "      <td>3.068750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>1672.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>81.544747</td>\n",
       "      <td>2877800.0</td>\n",
       "      <td>104.871</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.188</td>\n",
       "      <td>8.643</td>\n",
       "      <td>11803.431</td>\n",
       "      <td>1.1</td>\n",
       "      <td>304.195</td>\n",
       "      <td>10.08</td>\n",
       "      <td>7.1</td>\n",
       "      <td>51.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.89</td>\n",
       "      <td>101</td>\n",
       "      <td>16.554455</td>\n",
       "      <td>0.366337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>11147.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>43851043.0</td>\n",
       "      <td>17.348</td>\n",
       "      <td>29.1</td>\n",
       "      <td>6.211</td>\n",
       "      <td>3.857</td>\n",
       "      <td>13913.839</td>\n",
       "      <td>0.5</td>\n",
       "      <td>278.364</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.7</td>\n",
       "      <td>30.4</td>\n",
       "      <td>83.741</td>\n",
       "      <td>1.90</td>\n",
       "      <td>165</td>\n",
       "      <td>67.557576</td>\n",
       "      <td>4.775758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andorra</th>\n",
       "      <td>854.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>47.253667</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>163.755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109.135</td>\n",
       "      <td>7.97</td>\n",
       "      <td>29.0</td>\n",
       "      <td>37.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>8.895833</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>142.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.319770</td>\n",
       "      <td>32866268.0</td>\n",
       "      <td>23.890</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.405</td>\n",
       "      <td>1.362</td>\n",
       "      <td>5819.495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276.045</td>\n",
       "      <td>3.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>1.613636</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.593533</td>\n",
       "      <td>97338583.0</td>\n",
       "      <td>308.127</td>\n",
       "      <td>32.6</td>\n",
       "      <td>7.150</td>\n",
       "      <td>4.718</td>\n",
       "      <td>6171.884</td>\n",
       "      <td>2.0</td>\n",
       "      <td>245.465</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.9</td>\n",
       "      <td>85.847</td>\n",
       "      <td>2.60</td>\n",
       "      <td>170</td>\n",
       "      <td>2.018072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western Sahara</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597330.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.018868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yemen</th>\n",
       "      <td>889.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>51.665000</td>\n",
       "      <td>29825968.0</td>\n",
       "      <td>53.508</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2.922</td>\n",
       "      <td>1.583</td>\n",
       "      <td>1479.147</td>\n",
       "      <td>18.8</td>\n",
       "      <td>495.003</td>\n",
       "      <td>5.35</td>\n",
       "      <td>7.6</td>\n",
       "      <td>29.2</td>\n",
       "      <td>49.542</td>\n",
       "      <td>0.70</td>\n",
       "      <td>69</td>\n",
       "      <td>12.884058</td>\n",
       "      <td>3.115942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zambia</th>\n",
       "      <td>1405.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47.145393</td>\n",
       "      <td>18383956.0</td>\n",
       "      <td>22.995</td>\n",
       "      <td>17.7</td>\n",
       "      <td>2.480</td>\n",
       "      <td>1.542</td>\n",
       "      <td>3689.251</td>\n",
       "      <td>57.5</td>\n",
       "      <td>234.499</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.1</td>\n",
       "      <td>24.7</td>\n",
       "      <td>13.938</td>\n",
       "      <td>2.00</td>\n",
       "      <td>91</td>\n",
       "      <td>15.439560</td>\n",
       "      <td>0.120879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zimbabwe</th>\n",
       "      <td>394.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.550132</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.70</td>\n",
       "      <td>89</td>\n",
       "      <td>4.426966</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                new_cases  new_deaths  stringency_index  population  \\\n",
       "location                                                              \n",
       "Afghanistan       26310.0       491.0         44.956867  38928341.0   \n",
       "Albania            1672.0        37.0         81.544747   2877800.0   \n",
       "Algeria           11147.0       788.0         46.875000  43851043.0   \n",
       "Andorra             854.0        52.0         47.253667     77265.0   \n",
       "Angola              142.0         6.0         78.319770  32866268.0   \n",
       "...                   ...         ...               ...         ...   \n",
       "Vietnam             335.0         0.0         51.593533  97338583.0   \n",
       "Western Sahara       23.0         1.0               NaN    597330.0   \n",
       "Yemen               889.0       215.0         51.665000  29825968.0   \n",
       "Zambia             1405.0        11.0         47.145393  18383956.0   \n",
       "Zimbabwe            394.0         4.0         83.550132  14862927.0   \n",
       "\n",
       "                population_density  median_age  aged_65_older  aged_70_older  \\\n",
       "location                                                                       \n",
       "Afghanistan                 54.422        18.6          2.581          1.337   \n",
       "Albania                    104.871        38.0         13.188          8.643   \n",
       "Algeria                     17.348        29.1          6.211          3.857   \n",
       "Andorra                    163.755         NaN            NaN            NaN   \n",
       "Angola                      23.890        16.8          2.405          1.362   \n",
       "...                            ...         ...            ...            ...   \n",
       "Vietnam                    308.127        32.6          7.150          4.718   \n",
       "Western Sahara                 NaN        28.4            NaN          1.380   \n",
       "Yemen                       53.508        20.3          2.922          1.583   \n",
       "Zambia                      22.995        17.7          2.480          1.542   \n",
       "Zimbabwe                    42.729        19.6          2.822          1.882   \n",
       "\n",
       "                gdp_per_capita  extreme_poverty  cvd_death_rate  \\\n",
       "location                                                          \n",
       "Afghanistan           1803.987              NaN         597.029   \n",
       "Albania              11803.431              1.1         304.195   \n",
       "Algeria              13913.839              0.5         278.364   \n",
       "Andorra                    NaN              NaN         109.135   \n",
       "Angola                5819.495              NaN         276.045   \n",
       "...                        ...              ...             ...   \n",
       "Vietnam               6171.884              2.0         245.465   \n",
       "Western Sahara             NaN              NaN             NaN   \n",
       "Yemen                 1479.147             18.8         495.003   \n",
       "Zambia                3689.251             57.5         234.499   \n",
       "Zimbabwe              1899.775             21.4         307.846   \n",
       "\n",
       "                diabetes_prevalence  female_smokers  male_smokers  \\\n",
       "location                                                            \n",
       "Afghanistan                    9.59             NaN           NaN   \n",
       "Albania                       10.08             7.1          51.2   \n",
       "Algeria                        6.73             0.7          30.4   \n",
       "Andorra                        7.97            29.0          37.8   \n",
       "Angola                         3.94             NaN           NaN   \n",
       "...                             ...             ...           ...   \n",
       "Vietnam                        6.00             1.0          45.9   \n",
       "Western Sahara                  NaN             NaN           NaN   \n",
       "Yemen                          5.35             7.6          29.2   \n",
       "Zambia                         3.94             3.1          24.7   \n",
       "Zimbabwe                       1.82             1.6          30.7   \n",
       "\n",
       "                handwashing_facilities  hospital_beds_per_thousand  \\\n",
       "location                                                             \n",
       "Afghanistan                     37.746                        0.50   \n",
       "Albania                            NaN                        2.89   \n",
       "Algeria                         83.741                        1.90   \n",
       "Andorra                            NaN                         NaN   \n",
       "Angola                          26.664                         NaN   \n",
       "...                                ...                         ...   \n",
       "Vietnam                         85.847                        2.60   \n",
       "Western Sahara                     NaN                         NaN   \n",
       "Yemen                           49.542                        0.70   \n",
       "Zambia                          13.938                        2.00   \n",
       "Zimbabwe                        36.791                        1.70   \n",
       "\n",
       "                DaysTotal_Days  AverageInfectionRate  AverageDeathRate  \n",
       "location                                                                \n",
       "Afghanistan                160            164.437500          3.068750  \n",
       "Albania                    101             16.554455          0.366337  \n",
       "Algeria                    165             67.557576          4.775758  \n",
       "Andorra                     96              8.895833          0.541667  \n",
       "Angola                      88              1.613636          0.068182  \n",
       "...                        ...                   ...               ...  \n",
       "Vietnam                    170              2.018072          0.000000  \n",
       "Western Sahara              53              0.433962          0.018868  \n",
       "Yemen                       69             12.884058          3.115942  \n",
       "Zambia                      91             15.439560          0.120879  \n",
       "Zimbabwe                    89              4.426966          0.044944  \n",
       "\n",
       "[210 rows x 19 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "newDataFrame = groupByLocation.join(groupByStringency)\n",
    "newDataFrame = newDataFrame.join(groupByRest)\n",
    "newDataFrame = newDataFrame.join(groupByDays, lsuffix='_caller', rsuffix='Total_Days')\n",
    "newDataFrame = newDataFrame.drop(columns=['Days_caller'])\n",
    "newDataFrame = newDataFrame.join(groupByLocationAverage,lsuffix='_caller', rsuffix='AverageDeathRate')\n",
    "newDataFrame.rename(columns={'new_cases_caller':'new_cases',\n",
    "                          'new_deaths_caller':'new_deaths',\n",
    "                          'new_casesAverageDeathRate':'AverageInfectionRate', \n",
    "                          'new_deathsAverageDeathRate':'AverageDeathRate'}, \n",
    "                 inplace=True)\n",
    "\n",
    "newDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotMaps(x):\n",
    "#     plt.scatter(x,newDataFrame[\"new_deaths\"])\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     plt.scatter(x,newDataFrame[\"new_cases\"])\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     print(x)\n",
    "#     mean_train = x.mean(axis = 0)\n",
    "#     std_dev_train = x.std(axis = 0)\n",
    "\n",
    "#     x = (x - mean_train)/std_dev_train\n",
    "#     x = (x - mean_train)/std_dev_train\n",
    "#     print(x)\n",
    "\n",
    "    plt.bar(x,newDataFrame[\"new_deaths\"])\n",
    "    plt.show()\n",
    "\n",
    "    plt.bar(x,newDataFrame[\"new_cases\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.bar(x,newDataFrame[\"AverageDeathRate\"])\n",
    "    plt.show()\n",
    "\n",
    "    plt.bar(x,newDataFrame[\"AverageInfectionRate\"])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plotMaps(newDataFrame[\"diabetes_prevalence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_cases                       0\n",
      "new_deaths                      0\n",
      "stringency_index               38\n",
      "population                      0\n",
      "population_density             11\n",
      "median_age                     24\n",
      "aged_65_older                  27\n",
      "aged_70_older                  25\n",
      "gdp_per_capita                 27\n",
      "extreme_poverty                89\n",
      "cvd_death_rate                 25\n",
      "diabetes_prevalence            17\n",
      "female_smokers                 70\n",
      "male_smokers                   72\n",
      "handwashing_facilities        119\n",
      "hospital_beds_per_thousand     46\n",
      "DaysTotal_Days                  0\n",
      "AverageInfectionRate            1\n",
      "AverageDeathRate                1\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "new_cases                     0\n",
       "new_deaths                    0\n",
       "stringency_index              0\n",
       "population                    0\n",
       "population_density            0\n",
       "median_age                    0\n",
       "aged_65_older                 0\n",
       "aged_70_older                 0\n",
       "gdp_per_capita                0\n",
       "cvd_death_rate                0\n",
       "diabetes_prevalence           0\n",
       "hospital_beds_per_thousand    0\n",
       "DaysTotal_Days                0\n",
       "AverageInfectionRate          0\n",
       "AverageDeathRate              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newDataFrame.to_csv(\"AggregatedData.csv\")\n",
    "print(newDataFrame.isnull().sum())\n",
    "newDataFrame = newDataFrame.drop(columns=['handwashing_facilities'])\n",
    "newDataFrame = newDataFrame.drop(columns=['female_smokers'])\n",
    "newDataFrame = newDataFrame.drop(columns=['male_smokers'])\n",
    "newDataFrame = newDataFrame.drop(columns=['extreme_poverty'])\n",
    "newDataFrame.fillna(method='ffill', inplace=True)\n",
    "newDataFrame.fillna(0,inplace=True)\n",
    "\n",
    "newDataFrame.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>DaysTotal_Days</th>\n",
       "      <th>AverageInfectionRate</th>\n",
       "      <th>AverageDeathRate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>26310.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>44.956867</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.50</td>\n",
       "      <td>160</td>\n",
       "      <td>164.437500</td>\n",
       "      <td>3.068750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>1672.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>81.544747</td>\n",
       "      <td>2877800.0</td>\n",
       "      <td>104.871</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.188</td>\n",
       "      <td>8.643</td>\n",
       "      <td>11803.431</td>\n",
       "      <td>304.195</td>\n",
       "      <td>10.08</td>\n",
       "      <td>2.89</td>\n",
       "      <td>101</td>\n",
       "      <td>16.554455</td>\n",
       "      <td>0.366337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>11147.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>43851043.0</td>\n",
       "      <td>17.348</td>\n",
       "      <td>29.1</td>\n",
       "      <td>6.211</td>\n",
       "      <td>3.857</td>\n",
       "      <td>13913.839</td>\n",
       "      <td>278.364</td>\n",
       "      <td>6.73</td>\n",
       "      <td>1.90</td>\n",
       "      <td>165</td>\n",
       "      <td>67.557576</td>\n",
       "      <td>4.775758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andorra</th>\n",
       "      <td>854.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>47.253667</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>163.755</td>\n",
       "      <td>29.1</td>\n",
       "      <td>6.211</td>\n",
       "      <td>3.857</td>\n",
       "      <td>13913.839</td>\n",
       "      <td>109.135</td>\n",
       "      <td>7.97</td>\n",
       "      <td>1.90</td>\n",
       "      <td>96</td>\n",
       "      <td>8.895833</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>142.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.319770</td>\n",
       "      <td>32866268.0</td>\n",
       "      <td>23.890</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.405</td>\n",
       "      <td>1.362</td>\n",
       "      <td>5819.495</td>\n",
       "      <td>276.045</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1.90</td>\n",
       "      <td>88</td>\n",
       "      <td>1.613636</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.593533</td>\n",
       "      <td>97338583.0</td>\n",
       "      <td>308.127</td>\n",
       "      <td>32.6</td>\n",
       "      <td>7.150</td>\n",
       "      <td>4.718</td>\n",
       "      <td>6171.884</td>\n",
       "      <td>245.465</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>170</td>\n",
       "      <td>2.018072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western Sahara</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.593533</td>\n",
       "      <td>597330.0</td>\n",
       "      <td>308.127</td>\n",
       "      <td>28.4</td>\n",
       "      <td>7.150</td>\n",
       "      <td>1.380</td>\n",
       "      <td>6171.884</td>\n",
       "      <td>245.465</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>53</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.018868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yemen</th>\n",
       "      <td>889.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>51.665000</td>\n",
       "      <td>29825968.0</td>\n",
       "      <td>53.508</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2.922</td>\n",
       "      <td>1.583</td>\n",
       "      <td>1479.147</td>\n",
       "      <td>495.003</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.70</td>\n",
       "      <td>69</td>\n",
       "      <td>12.884058</td>\n",
       "      <td>3.115942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zambia</th>\n",
       "      <td>1405.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47.145393</td>\n",
       "      <td>18383956.0</td>\n",
       "      <td>22.995</td>\n",
       "      <td>17.7</td>\n",
       "      <td>2.480</td>\n",
       "      <td>1.542</td>\n",
       "      <td>3689.251</td>\n",
       "      <td>234.499</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.00</td>\n",
       "      <td>91</td>\n",
       "      <td>15.439560</td>\n",
       "      <td>0.120879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zimbabwe</th>\n",
       "      <td>394.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.550132</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.70</td>\n",
       "      <td>89</td>\n",
       "      <td>4.426966</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                new_cases  new_deaths  stringency_index  population  \\\n",
       "location                                                              \n",
       "Afghanistan       26310.0       491.0         44.956867  38928341.0   \n",
       "Albania            1672.0        37.0         81.544747   2877800.0   \n",
       "Algeria           11147.0       788.0         46.875000  43851043.0   \n",
       "Andorra             854.0        52.0         47.253667     77265.0   \n",
       "Angola              142.0         6.0         78.319770  32866268.0   \n",
       "...                   ...         ...               ...         ...   \n",
       "Vietnam             335.0         0.0         51.593533  97338583.0   \n",
       "Western Sahara       23.0         1.0         51.593533    597330.0   \n",
       "Yemen               889.0       215.0         51.665000  29825968.0   \n",
       "Zambia             1405.0        11.0         47.145393  18383956.0   \n",
       "Zimbabwe            394.0         4.0         83.550132  14862927.0   \n",
       "\n",
       "                population_density  median_age  aged_65_older  aged_70_older  \\\n",
       "location                                                                       \n",
       "Afghanistan                 54.422        18.6          2.581          1.337   \n",
       "Albania                    104.871        38.0         13.188          8.643   \n",
       "Algeria                     17.348        29.1          6.211          3.857   \n",
       "Andorra                    163.755        29.1          6.211          3.857   \n",
       "Angola                      23.890        16.8          2.405          1.362   \n",
       "...                            ...         ...            ...            ...   \n",
       "Vietnam                    308.127        32.6          7.150          4.718   \n",
       "Western Sahara             308.127        28.4          7.150          1.380   \n",
       "Yemen                       53.508        20.3          2.922          1.583   \n",
       "Zambia                      22.995        17.7          2.480          1.542   \n",
       "Zimbabwe                    42.729        19.6          2.822          1.882   \n",
       "\n",
       "                gdp_per_capita  cvd_death_rate  diabetes_prevalence  \\\n",
       "location                                                              \n",
       "Afghanistan           1803.987         597.029                 9.59   \n",
       "Albania              11803.431         304.195                10.08   \n",
       "Algeria              13913.839         278.364                 6.73   \n",
       "Andorra              13913.839         109.135                 7.97   \n",
       "Angola                5819.495         276.045                 3.94   \n",
       "...                        ...             ...                  ...   \n",
       "Vietnam               6171.884         245.465                 6.00   \n",
       "Western Sahara        6171.884         245.465                 6.00   \n",
       "Yemen                 1479.147         495.003                 5.35   \n",
       "Zambia                3689.251         234.499                 3.94   \n",
       "Zimbabwe              1899.775         307.846                 1.82   \n",
       "\n",
       "                hospital_beds_per_thousand  DaysTotal_Days  \\\n",
       "location                                                     \n",
       "Afghanistan                           0.50             160   \n",
       "Albania                               2.89             101   \n",
       "Algeria                               1.90             165   \n",
       "Andorra                               1.90              96   \n",
       "Angola                                1.90              88   \n",
       "...                                    ...             ...   \n",
       "Vietnam                               2.60             170   \n",
       "Western Sahara                        2.60              53   \n",
       "Yemen                                 0.70              69   \n",
       "Zambia                                2.00              91   \n",
       "Zimbabwe                              1.70              89   \n",
       "\n",
       "                AverageInfectionRate  AverageDeathRate  \n",
       "location                                                \n",
       "Afghanistan               164.437500          3.068750  \n",
       "Albania                    16.554455          0.366337  \n",
       "Algeria                    67.557576          4.775758  \n",
       "Andorra                     8.895833          0.541667  \n",
       "Angola                      1.613636          0.068182  \n",
       "...                              ...               ...  \n",
       "Vietnam                     2.018072          0.000000  \n",
       "Western Sahara              0.433962          0.018868  \n",
       "Yemen                      12.884058          3.115942  \n",
       "Zambia                     15.439560          0.120879  \n",
       "Zimbabwe                    4.426966          0.044944  \n",
       "\n",
       "[210 rows x 15 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                36        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 112 samples, validate on 28 samples\n",
      "Epoch 1/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2842 - val_loss: 0.2867\n",
      "Epoch 2/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1579 - val_loss: 0.2715\n",
      "Epoch 3/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1627 - val_loss: 0.2377\n",
      "Epoch 4/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.1436 - val_loss: 0.2293\n",
      "Epoch 5/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.086 - 0s 758us/step - loss: 1.1270 - val_loss: 0.2504\n",
      "Epoch 6/500\n",
      "112/112 [==============================] - 0s 758us/step - loss: 1.1299 - val_loss: 0.2294\n",
      "Epoch 7/500\n",
      "112/112 [==============================] - 0s 804us/step - loss: 1.1252 - val_loss: 0.2271\n",
      "Epoch 8/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1221 - val_loss: 0.2336\n",
      "Epoch 9/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1258 - val_loss: 0.2293\n",
      "Epoch 10/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1237 - val_loss: 0.2339\n",
      "Epoch 11/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.1321 - val_loss: 0.2227\n",
      "Epoch 12/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.1175 - val_loss: 0.2302\n",
      "Epoch 13/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.1238 - val_loss: 0.2271\n",
      "Epoch 14/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.1259 - val_loss: 0.2240\n",
      "Epoch 15/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1046 - val_loss: 0.2625\n",
      "Epoch 16/500\n",
      "112/112 [==============================] - 0s 847us/step - loss: 1.1249 - val_loss: 0.2288\n",
      "Epoch 17/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1235 - val_loss: 0.2230\n",
      "Epoch 18/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1221 - val_loss: 0.2236\n",
      "Epoch 19/500\n",
      "112/112 [==============================] - 0s 853us/step - loss: 1.1172 - val_loss: 0.2252\n",
      "Epoch 20/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.1160 - val_loss: 0.2575\n",
      "Epoch 21/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1240 - val_loss: 0.2286\n",
      "Epoch 22/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.1212 - val_loss: 0.2228\n",
      "Epoch 23/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1268 - val_loss: 0.2204\n",
      "Epoch 24/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.1109 - val_loss: 0.2593\n",
      "Epoch 25/500\n",
      "112/112 [==============================] - 0s 671us/step - loss: 1.1293 - val_loss: 0.2221\n",
      "Epoch 26/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1175 - val_loss: 0.2305\n",
      "Epoch 27/500\n",
      "112/112 [==============================] - 0s 798us/step - loss: 1.1209 - val_loss: 0.2184\n",
      "Epoch 28/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.1207 - val_loss: 0.2243\n",
      "Epoch 29/500\n",
      "112/112 [==============================] - 0s 756us/step - loss: 1.1129 - val_loss: 0.2225\n",
      "Epoch 30/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.1226 - val_loss: 0.2219\n",
      "Epoch 31/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1142 - val_loss: 0.2198\n",
      "Epoch 32/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.1129 - val_loss: 0.2203\n",
      "Epoch 33/500\n",
      "112/112 [==============================] - 0s 710us/step - loss: 1.1155 - val_loss: 0.2222\n",
      "Epoch 34/500\n",
      "112/112 [==============================] - 0s 758us/step - loss: 1.1195 - val_loss: 0.2221\n",
      "Epoch 35/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1049 - val_loss: 0.2308\n",
      "Epoch 36/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.1132 - val_loss: 0.2417\n",
      "Epoch 37/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 1.561 - 0s 760us/step - loss: 1.1180 - val_loss: 0.2190\n",
      "Epoch 38/500\n",
      "112/112 [==============================] - 0s 727us/step - loss: 1.1149 - val_loss: 0.2183\n",
      "Epoch 39/500\n",
      "112/112 [==============================] - 0s 747us/step - loss: 1.1133 - val_loss: 0.2186\n",
      "Epoch 40/500\n",
      "112/112 [==============================] - 0s 847us/step - loss: 1.1177 - val_loss: 0.2227\n",
      "Epoch 41/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.1107 - val_loss: 0.2200\n",
      "Epoch 42/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1184 - val_loss: 0.2205\n",
      "Epoch 43/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.1163 - val_loss: 0.2189\n",
      "Epoch 44/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1103 - val_loss: 0.2184\n",
      "Epoch 45/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1074 - val_loss: 0.2191\n",
      "Epoch 46/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1246 - val_loss: 0.2177\n",
      "Epoch 47/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1116 - val_loss: 0.2176\n",
      "Epoch 48/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1130 - val_loss: 0.2196\n",
      "Epoch 49/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1123 - val_loss: 0.2490\n",
      "Epoch 50/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1119 - val_loss: 0.2181\n",
      "Epoch 51/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1176 - val_loss: 0.2150\n",
      "Epoch 52/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1026 - val_loss: 0.2156\n",
      "Epoch 53/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1134 - val_loss: 0.2132\n",
      "Epoch 54/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1070 - val_loss: 0.2528\n",
      "Epoch 55/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.1057 - val_loss: 0.2155\n",
      "Epoch 56/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1133 - val_loss: 0.2187\n",
      "Epoch 57/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1177 - val_loss: 0.2132\n",
      "Epoch 58/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1114 - val_loss: 0.2143\n",
      "Epoch 59/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.1215 - val_loss: 0.2167\n",
      "Epoch 60/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1103 - val_loss: 0.2232\n",
      "Epoch 61/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1139 - val_loss: 0.2238\n",
      "Epoch 62/500\n",
      "112/112 [==============================] - 0s 764us/step - loss: 1.1015 - val_loss: 0.2175\n",
      "Epoch 63/500\n",
      "112/112 [==============================] - 0s 628us/step - loss: 1.1252 - val_loss: 0.2112\n",
      "Epoch 64/500\n",
      "112/112 [==============================] - 0s 760us/step - loss: 1.1127 - val_loss: 0.2238\n",
      "Epoch 65/500\n",
      "112/112 [==============================] - 0s 666us/step - loss: 1.1093 - val_loss: 0.2460\n",
      "Epoch 66/500\n",
      "112/112 [==============================] - 0s 624us/step - loss: 1.1114 - val_loss: 0.2173\n",
      "Epoch 67/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1063 - val_loss: 0.2516\n",
      "Epoch 68/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1143 - val_loss: 0.2160\n",
      "Epoch 69/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.1035 - val_loss: 0.2139\n",
      "Epoch 70/500\n",
      "112/112 [==============================] - 0s 753us/step - loss: 1.1145 - val_loss: 0.2155\n",
      "Epoch 71/500\n",
      "112/112 [==============================] - 0s 722us/step - loss: 1.1078 - val_loss: 0.2151\n",
      "Epoch 72/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1098 - val_loss: 0.2151\n",
      "Epoch 73/500\n",
      "112/112 [==============================] - 0s 974us/step - loss: 1.1099 - val_loss: 0.2371\n",
      "Epoch 74/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.1051 - val_loss: 0.2399\n",
      "Epoch 75/500\n",
      "112/112 [==============================] - 0s 677us/step - loss: 1.1078 - val_loss: 0.2141\n",
      "Epoch 76/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.1118 - val_loss: 0.2369\n",
      "Epoch 77/500\n",
      "112/112 [==============================] - 0s 766us/step - loss: 1.1099 - val_loss: 0.2533\n",
      "Epoch 78/500\n",
      "112/112 [==============================] - 0s 704us/step - loss: 1.1134 - val_loss: 0.2256\n",
      "Epoch 79/500\n",
      "112/112 [==============================] - 0s 748us/step - loss: 1.0907 - val_loss: 0.2175\n",
      "Epoch 80/500\n",
      "112/112 [==============================] - 0s 698us/step - loss: 1.1212 - val_loss: 0.2126\n",
      "Epoch 81/500\n",
      "112/112 [==============================] - 0s 690us/step - loss: 1.1193 - val_loss: 0.2116\n",
      "Epoch 82/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1063 - val_loss: 0.2147\n",
      "Epoch 83/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1173 - val_loss: 0.2179\n",
      "Epoch 84/500\n",
      "112/112 [==============================] - 0s 681us/step - loss: 1.1087 - val_loss: 0.2136\n",
      "Epoch 85/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.1039 - val_loss: 0.2174\n",
      "Epoch 86/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.1112 - val_loss: 0.2422\n",
      "Epoch 87/500\n",
      "112/112 [==============================] - 0s 703us/step - loss: 1.1105 - val_loss: 0.2157\n",
      "Epoch 88/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1059 - val_loss: 0.2147\n",
      "Epoch 89/500\n",
      "112/112 [==============================] - 0s 860us/step - loss: 1.1020 - val_loss: 0.2116\n",
      "Epoch 90/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.1159 - val_loss: 0.2145\n",
      "Epoch 91/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1057 - val_loss: 0.2313\n",
      "Epoch 92/500\n",
      "112/112 [==============================] - 0s 766us/step - loss: 1.1063 - val_loss: 0.2099\n",
      "Epoch 93/500\n",
      "112/112 [==============================] - 0s 703us/step - loss: 1.1114 - val_loss: 0.2126\n",
      "Epoch 94/500\n",
      "112/112 [==============================] - 0s 855us/step - loss: 1.1036 - val_loss: 0.2507\n",
      "Epoch 95/500\n",
      "112/112 [==============================] - 0s 659us/step - loss: 1.1057 - val_loss: 0.2115\n",
      "Epoch 96/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.1177 - val_loss: 0.2385\n",
      "Epoch 97/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.0937 - val_loss: 0.2123\n",
      "Epoch 98/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1119 - val_loss: 0.2900\n",
      "Epoch 99/500\n",
      "112/112 [==============================] - 0s 733us/step - loss: 1.1156 - val_loss: 0.2165\n",
      "Epoch 100/500\n",
      "112/112 [==============================] - 0s 722us/step - loss: 1.1093 - val_loss: 0.2285\n",
      "Epoch 101/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.1137 - val_loss: 0.2148\n",
      "Epoch 102/500\n",
      "112/112 [==============================] - 0s 730us/step - loss: 1.1087 - val_loss: 0.2570\n",
      "Epoch 103/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.0949 - val_loss: 0.2133\n",
      "Epoch 104/500\n",
      "112/112 [==============================] - 0s 792us/step - loss: 1.1139 - val_loss: 0.2152\n",
      "Epoch 105/500\n",
      "112/112 [==============================] - 0s 704us/step - loss: 1.1139 - val_loss: 0.2159\n",
      "Epoch 106/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.1121 - val_loss: 0.2100\n",
      "Epoch 107/500\n",
      "112/112 [==============================] - 0s 710us/step - loss: 1.0973 - val_loss: 0.2151\n",
      "Epoch 108/500\n",
      "112/112 [==============================] - 0s 730us/step - loss: 1.1183 - val_loss: 0.2113\n",
      "Epoch 109/500\n",
      "112/112 [==============================] - 0s 650us/step - loss: 1.1186 - val_loss: 0.2153\n",
      "Epoch 110/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1006 - val_loss: 0.2638\n",
      "Epoch 111/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.1097 - val_loss: 0.2125\n",
      "Epoch 112/500\n",
      "112/112 [==============================] - 0s 677us/step - loss: 1.1094 - val_loss: 0.2110\n",
      "Epoch 113/500\n",
      "112/112 [==============================] - 0s 694us/step - loss: 1.0904 - val_loss: 0.2163\n",
      "Epoch 114/500\n",
      "112/112 [==============================] - 0s 784us/step - loss: 1.1268 - val_loss: 0.2128\n",
      "Epoch 115/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.1053 - val_loss: 0.2120\n",
      "Epoch 116/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.1119 - val_loss: 0.2158\n",
      "Epoch 117/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1030 - val_loss: 0.2164\n",
      "Epoch 118/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.0961 - val_loss: 0.2164\n",
      "Epoch 119/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.1120 - val_loss: 0.2126\n",
      "Epoch 120/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1031 - val_loss: 0.2129\n",
      "Epoch 121/500\n",
      "112/112 [==============================] - 0s 684us/step - loss: 1.0970 - val_loss: 0.2139\n",
      "Epoch 122/500\n",
      "112/112 [==============================] - 0s 704us/step - loss: 1.1161 - val_loss: 0.2135\n",
      "Epoch 123/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1064 - val_loss: 0.2119\n",
      "Epoch 124/500\n",
      "112/112 [==============================] - 0s 730us/step - loss: 1.1065 - val_loss: 0.2115\n",
      "Epoch 125/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.1094 - val_loss: 0.2216\n",
      "Epoch 126/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0935 - val_loss: 0.2115\n",
      "Epoch 127/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1128 - val_loss: 0.2108\n",
      "Epoch 128/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.1048 - val_loss: 0.2237\n",
      "Epoch 129/500\n",
      "112/112 [==============================] - 0s 774us/step - loss: 1.1020 - val_loss: 0.2120\n",
      "Epoch 130/500\n",
      "112/112 [==============================] - 0s 748us/step - loss: 1.1082 - val_loss: 0.2141\n",
      "Epoch 131/500\n",
      "112/112 [==============================] - 0s 730us/step - loss: 1.1087 - val_loss: 0.2150\n",
      "Epoch 132/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.1002 - val_loss: 0.2106\n",
      "Epoch 133/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1000 - val_loss: 0.2573\n",
      "Epoch 134/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.1051 - val_loss: 0.2310\n",
      "Epoch 135/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.1068 - val_loss: 0.2384\n",
      "Epoch 136/500\n",
      "112/112 [==============================] - 0s 651us/step - loss: 1.1061 - val_loss: 0.2117\n",
      "Epoch 137/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1043 - val_loss: 0.2146\n",
      "Epoch 138/500\n",
      "112/112 [==============================] - 0s 804us/step - loss: 1.1075 - val_loss: 0.2138\n",
      "Epoch 139/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1019 - val_loss: 0.2149\n",
      "Epoch 140/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1056 - val_loss: 0.2139\n",
      "Epoch 141/500\n",
      "112/112 [==============================] - 0s 804us/step - loss: 1.1024 - val_loss: 0.2160\n",
      "Epoch 142/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.1087 - val_loss: 0.2126\n",
      "Epoch 143/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0996 - val_loss: 0.2123\n",
      "Epoch 144/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1088 - val_loss: 0.2151\n",
      "Epoch 145/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1024 - val_loss: 0.2281\n",
      "Epoch 146/500\n",
      "112/112 [==============================] - 0s 624us/step - loss: 1.1043 - val_loss: 0.2141\n",
      "Epoch 147/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1103 - val_loss: 0.2161\n",
      "Epoch 148/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0985 - val_loss: 0.2415\n",
      "Epoch 149/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1060 - val_loss: 0.2120\n",
      "Epoch 150/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0982 - val_loss: 0.2200\n",
      "Epoch 151/500\n",
      "112/112 [==============================] - 0s 789us/step - loss: 1.1038 - val_loss: 0.2131\n",
      "Epoch 152/500\n",
      "112/112 [==============================] - 0s 621us/step - loss: 1.1103 - val_loss: 0.2112\n",
      "Epoch 153/500\n",
      "112/112 [==============================] - 0s 850us/step - loss: 1.0981 - val_loss: 0.2122\n",
      "Epoch 154/500\n",
      "112/112 [==============================] - 0s 671us/step - loss: 1.1032 - val_loss: 0.2137\n",
      "Epoch 155/500\n",
      "112/112 [==============================] - 0s 801us/step - loss: 1.1083 - val_loss: 0.2121\n",
      "Epoch 156/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1023 - val_loss: 0.2131\n",
      "Epoch 157/500\n",
      "112/112 [==============================] - 0s 698us/step - loss: 1.1003 - val_loss: 0.2184\n",
      "Epoch 158/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1119 - val_loss: 0.2156\n",
      "Epoch 159/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 1.346 - 0s 623us/step - loss: 1.1005 - val_loss: 0.2162\n",
      "Epoch 160/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.072 - 0s 804us/step - loss: 1.1066 - val_loss: 0.2169\n",
      "Epoch 161/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0978 - val_loss: 0.2106\n",
      "Epoch 162/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1059 - val_loss: 0.2115\n",
      "Epoch 163/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.1025 - val_loss: 0.2146\n",
      "Epoch 164/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1075 - val_loss: 0.2174\n",
      "Epoch 165/500\n",
      "112/112 [==============================] - 0s 719us/step - loss: 1.1058 - val_loss: 0.2150\n",
      "Epoch 166/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.1062 - val_loss: 0.2117\n",
      "Epoch 167/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0984 - val_loss: 0.2121\n",
      "Epoch 168/500\n",
      "112/112 [==============================] - 0s 803us/step - loss: 1.1154 - val_loss: 0.2155\n",
      "Epoch 169/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0959 - val_loss: 0.2519\n",
      "Epoch 170/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1050 - val_loss: 0.2206\n",
      "Epoch 171/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1022 - val_loss: 0.2172\n",
      "Epoch 172/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.1063 - val_loss: 0.2161\n",
      "Epoch 173/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1046 - val_loss: 0.2158\n",
      "Epoch 174/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1018 - val_loss: 0.2220\n",
      "Epoch 175/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1016 - val_loss: 0.2105\n",
      "Epoch 176/500\n",
      "112/112 [==============================] - 0s 792us/step - loss: 1.1047 - val_loss: 0.2417\n",
      "Epoch 177/500\n",
      "112/112 [==============================] - 0s 623us/step - loss: 1.1021 - val_loss: 0.2111\n",
      "Epoch 178/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.1041 - val_loss: 0.2103\n",
      "Epoch 179/500\n",
      "112/112 [==============================] - 0s 762us/step - loss: 1.1045 - val_loss: 0.2135\n",
      "Epoch 180/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.0997 - val_loss: 0.2295\n",
      "Epoch 181/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0996 - val_loss: 0.2094\n",
      "Epoch 182/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1056 - val_loss: 0.2123\n",
      "Epoch 183/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1028 - val_loss: 0.2101\n",
      "Epoch 184/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0978 - val_loss: 0.2104\n",
      "Epoch 185/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1056 - val_loss: 0.2121\n",
      "Epoch 186/500\n",
      "112/112 [==============================] - 0s 890us/step - loss: 1.0954 - val_loss: 0.2128\n",
      "Epoch 187/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1073 - val_loss: 0.2296\n",
      "Epoch 188/500\n",
      "112/112 [==============================] - 0s 627us/step - loss: 1.1060 - val_loss: 0.2113\n",
      "Epoch 189/500\n",
      "112/112 [==============================] - 0s 622us/step - loss: 1.0914 - val_loss: 0.2121\n",
      "Epoch 190/500\n",
      "112/112 [==============================] - 0s 823us/step - loss: 1.1135 - val_loss: 0.2110\n",
      "Epoch 191/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0977 - val_loss: 0.2150\n",
      "Epoch 192/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1042 - val_loss: 0.2102\n",
      "Epoch 193/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.1007 - val_loss: 0.2121\n",
      "Epoch 194/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.0998 - val_loss: 0.2176\n",
      "Epoch 195/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.0981 - val_loss: 0.2379\n",
      "Epoch 196/500\n",
      "112/112 [==============================] - 0s 719us/step - loss: 1.0893 - val_loss: 0.2122\n",
      "Epoch 197/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.1178 - val_loss: 0.2113\n",
      "Epoch 198/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.1051 - val_loss: 0.2128\n",
      "Epoch 199/500\n",
      "112/112 [==============================] - 0s 622us/step - loss: 1.0955 - val_loss: 0.2416\n",
      "Epoch 200/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 1.250 - 0s 714us/step - loss: 1.1019 - val_loss: 0.2150\n",
      "Epoch 201/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1049 - val_loss: 0.2190\n",
      "Epoch 202/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.0956 - val_loss: 0.2114\n",
      "Epoch 203/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1017 - val_loss: 0.2197\n",
      "Epoch 204/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.1031 - val_loss: 0.2166\n",
      "Epoch 205/500\n",
      "112/112 [==============================] - 0s 731us/step - loss: 1.1067 - val_loss: 0.2130\n",
      "Epoch 206/500\n",
      "112/112 [==============================] - 0s 701us/step - loss: 1.0974 - val_loss: 0.2128\n",
      "Epoch 207/500\n",
      "112/112 [==============================] - 0s 801us/step - loss: 1.0862 - val_loss: 0.2099\n",
      "Epoch 208/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1208 - val_loss: 0.2175\n",
      "Epoch 209/500\n",
      "112/112 [==============================] - 0s 804us/step - loss: 1.1035 - val_loss: 0.2147\n",
      "Epoch 210/500\n",
      "112/112 [==============================] - 0s 801us/step - loss: 1.0898 - val_loss: 0.2110\n",
      "Epoch 211/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1170 - val_loss: 0.2142\n",
      "Epoch 212/500\n",
      "112/112 [==============================] - 0s 737us/step - loss: 1.1002 - val_loss: 0.2135\n",
      "Epoch 213/500\n",
      "112/112 [==============================] - 0s 690us/step - loss: 1.1000 - val_loss: 0.2099\n",
      "Epoch 214/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0968 - val_loss: 0.2151\n",
      "Epoch 215/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0957 - val_loss: 0.2134\n",
      "Epoch 216/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1041 - val_loss: 0.2267\n",
      "Epoch 217/500\n",
      "112/112 [==============================] - 0s 719us/step - loss: 1.1014 - val_loss: 0.2116\n",
      "Epoch 218/500\n",
      "112/112 [==============================] - 0s 618us/step - loss: 1.1005 - val_loss: 0.2107\n",
      "Epoch 219/500\n",
      "112/112 [==============================] - 0s 803us/step - loss: 1.0976 - val_loss: 0.2122\n",
      "Epoch 220/500\n",
      "112/112 [==============================] - 0s 719us/step - loss: 1.1046 - val_loss: 0.2151\n",
      "Epoch 221/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0948 - val_loss: 0.2432\n",
      "Epoch 222/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0898 - val_loss: 0.2118\n",
      "Epoch 223/500\n",
      "112/112 [==============================] - 0s 666us/step - loss: 1.1024 - val_loss: 0.2173\n",
      "Epoch 224/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1076 - val_loss: 0.2110\n",
      "Epoch 225/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.0948 - val_loss: 0.2559\n",
      "Epoch 226/500\n",
      "112/112 [==============================] - 0s 623us/step - loss: 1.1023 - val_loss: 0.2125\n",
      "Epoch 227/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.0786 - val_loss: 0.2192\n",
      "Epoch 228/500\n",
      "112/112 [==============================] - 0s 667us/step - loss: 1.1260 - val_loss: 0.2114\n",
      "Epoch 229/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.1022 - val_loss: 0.2105\n",
      "Epoch 230/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1077 - val_loss: 0.2223\n",
      "Epoch 231/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.0949 - val_loss: 0.2138\n",
      "Epoch 232/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1031 - val_loss: 0.2109\n",
      "Epoch 233/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.1008 - val_loss: 0.2275\n",
      "Epoch 234/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.0977 - val_loss: 0.2202\n",
      "Epoch 235/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.0957 - val_loss: 0.2360\n",
      "Epoch 236/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1014 - val_loss: 0.2128\n",
      "Epoch 237/500\n",
      "112/112 [==============================] - 0s 666us/step - loss: 1.0991 - val_loss: 0.2139\n",
      "Epoch 238/500\n",
      "112/112 [==============================] - 0s 670us/step - loss: 1.0985 - val_loss: 0.2141\n",
      "Epoch 239/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.0998 - val_loss: 0.2198\n",
      "Epoch 240/500\n",
      "112/112 [==============================] - 0s 720us/step - loss: 1.0945 - val_loss: 0.2136\n",
      "Epoch 241/500\n",
      "112/112 [==============================] - 0s 801us/step - loss: 1.1062 - val_loss: 0.2259\n",
      "Epoch 242/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1000 - val_loss: 0.2177\n",
      "Epoch 243/500\n",
      "112/112 [==============================] - 0s 727us/step - loss: 1.1044 - val_loss: 0.2143\n",
      "Epoch 244/500\n",
      "112/112 [==============================] - 0s 789us/step - loss: 1.0945 - val_loss: 0.2184\n",
      "Epoch 245/500\n",
      "112/112 [==============================] - 0s 762us/step - loss: 1.0911 - val_loss: 0.2139\n",
      "Epoch 246/500\n",
      "112/112 [==============================] - 0s 665us/step - loss: 1.1055 - val_loss: 0.2153\n",
      "Epoch 247/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.0908 - val_loss: 0.2153\n",
      "Epoch 248/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1067 - val_loss: 0.2134\n",
      "Epoch 249/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1049 - val_loss: 0.2194\n",
      "Epoch 250/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0974 - val_loss: 0.2126\n",
      "Epoch 251/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0910 - val_loss: 0.2103\n",
      "Epoch 252/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.1045 - val_loss: 0.2142\n",
      "Epoch 253/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.0963 - val_loss: 0.2104\n",
      "Epoch 254/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1106 - val_loss: 0.2150\n",
      "Epoch 255/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0937 - val_loss: 0.2198\n",
      "Epoch 256/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0964 - val_loss: 0.2139\n",
      "Epoch 257/500\n",
      "112/112 [==============================] - 0s 800us/step - loss: 1.1059 - val_loss: 0.2179\n",
      "Epoch 258/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.0881 - val_loss: 0.2150\n",
      "Epoch 259/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1081 - val_loss: 0.2138\n",
      "Epoch 260/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1055 - val_loss: 0.2133\n",
      "Epoch 261/500\n",
      "112/112 [==============================] - 0s 742us/step - loss: 1.0959 - val_loss: 0.2159\n",
      "Epoch 262/500\n",
      "112/112 [==============================] - 0s 685us/step - loss: 1.0960 - val_loss: 0.2106\n",
      "Epoch 263/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.0969 - val_loss: 0.2162\n",
      "Epoch 264/500\n",
      "112/112 [==============================] - 0s 803us/step - loss: 1.0989 - val_loss: 0.2171\n",
      "Epoch 265/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.0983 - val_loss: 0.2108\n",
      "Epoch 266/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0934 - val_loss: 0.2111\n",
      "Epoch 267/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0820 - val_loss: 0.2153\n",
      "Epoch 268/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1153 - val_loss: 0.2201\n",
      "Epoch 269/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.0987 - val_loss: 0.2164\n",
      "Epoch 270/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0936 - val_loss: 0.2099\n",
      "Epoch 271/500\n",
      "112/112 [==============================] - 0s 670us/step - loss: 1.0979 - val_loss: 0.2423\n",
      "Epoch 272/500\n",
      "112/112 [==============================] - 0s 755us/step - loss: 1.1013 - val_loss: 0.2145\n",
      "Epoch 273/500\n",
      "112/112 [==============================] - 0s 670us/step - loss: 1.0996 - val_loss: 0.2134\n",
      "Epoch 274/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.0937 - val_loss: 0.2146\n",
      "Epoch 275/500\n",
      "112/112 [==============================] - 0s 709us/step - loss: 1.1052 - val_loss: 0.2160\n",
      "Epoch 276/500\n",
      "112/112 [==============================] - 0s 719us/step - loss: 1.0958 - val_loss: 0.2197\n",
      "Epoch 277/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0906 - val_loss: 0.2127\n",
      "Epoch 278/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.1103 - val_loss: 0.2163\n",
      "Epoch 279/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0930 - val_loss: 0.2158\n",
      "Epoch 280/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1003 - val_loss: 0.2140\n",
      "Epoch 281/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.0872 - val_loss: 0.2129\n",
      "Epoch 282/500\n",
      "112/112 [==============================] - 0s 673us/step - loss: 1.1118 - val_loss: 0.2144\n",
      "Epoch 283/500\n",
      "112/112 [==============================] - 0s 849us/step - loss: 1.0971 - val_loss: 0.2192\n",
      "Epoch 284/500\n",
      "112/112 [==============================] - 0s 756us/step - loss: 1.0849 - val_loss: 0.2148\n",
      "Epoch 285/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.1137 - val_loss: 0.2164\n",
      "Epoch 286/500\n",
      "112/112 [==============================] - 0s 621us/step - loss: 1.0969 - val_loss: 0.2253\n",
      "Epoch 287/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.0930 - val_loss: 0.2123\n",
      "Epoch 288/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1058 - val_loss: 0.2133\n",
      "Epoch 289/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0988 - val_loss: 0.2186\n",
      "Epoch 290/500\n",
      "112/112 [==============================] - 0s 763us/step - loss: 1.0943 - val_loss: 0.2134\n",
      "Epoch 291/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.0926 - val_loss: 0.2279\n",
      "Epoch 292/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0828 - val_loss: 0.2120\n",
      "Epoch 293/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.1087 - val_loss: 0.2260\n",
      "Epoch 294/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.1001 - val_loss: 0.2128\n",
      "Epoch 295/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1032 - val_loss: 0.2167\n",
      "Epoch 296/500\n",
      "112/112 [==============================] - 0s 626us/step - loss: 1.0926 - val_loss: 0.2144\n",
      "Epoch 297/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0955 - val_loss: 0.2159\n",
      "Epoch 298/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1000 - val_loss: 0.2160\n",
      "Epoch 299/500\n",
      "112/112 [==============================] - 0s 801us/step - loss: 1.0861 - val_loss: 0.2132\n",
      "Epoch 300/500\n",
      "112/112 [==============================] - 0s 760us/step - loss: 1.1033 - val_loss: 0.2142\n",
      "Epoch 301/500\n",
      "112/112 [==============================] - 0s 758us/step - loss: 1.0995 - val_loss: 0.2117\n",
      "Epoch 302/500\n",
      "112/112 [==============================] - 0s 760us/step - loss: 1.0856 - val_loss: 0.2185\n",
      "Epoch 303/500\n",
      "112/112 [==============================] - 0s 667us/step - loss: 1.1109 - val_loss: 0.2138\n",
      "Epoch 304/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.0926 - val_loss: 0.2142\n",
      "Epoch 305/500\n",
      "112/112 [==============================] - 0s 710us/step - loss: 1.0889 - val_loss: 0.2176\n",
      "Epoch 306/500\n",
      "112/112 [==============================] - 0s 803us/step - loss: 1.1172 - val_loss: 0.2136\n",
      "Epoch 307/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.0966 - val_loss: 0.2148\n",
      "Epoch 308/500\n",
      "112/112 [==============================] - 0s 626us/step - loss: 1.0943 - val_loss: 0.2140\n",
      "Epoch 309/500\n",
      "112/112 [==============================] - 0s 626us/step - loss: 1.1015 - val_loss: 0.2168\n",
      "Epoch 310/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0888 - val_loss: 0.2158\n",
      "Epoch 311/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1049 - val_loss: 0.2129\n",
      "Epoch 312/500\n",
      "112/112 [==============================] - 0s 846us/step - loss: 1.0862 - val_loss: 0.2171\n",
      "Epoch 313/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.0778 - val_loss: 0.2109\n",
      "Epoch 314/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1151 - val_loss: 0.2130\n",
      "Epoch 315/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1148 - val_loss: 0.2153\n",
      "Epoch 316/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0888 - val_loss: 0.2135\n",
      "Epoch 317/500\n",
      "112/112 [==============================] - 0s 625us/step - loss: 1.0920 - val_loss: 0.2346\n",
      "Epoch 318/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0977 - val_loss: 0.2179\n",
      "Epoch 319/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0915 - val_loss: 0.2157\n",
      "Epoch 320/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.1040 - val_loss: 0.2167\n",
      "Epoch 321/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0882 - val_loss: 0.2295\n",
      "Epoch 322/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1024 - val_loss: 0.2178\n",
      "Epoch 323/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.0992 - val_loss: 0.2155\n",
      "Epoch 324/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0948 - val_loss: 0.2175\n",
      "Epoch 325/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0828 - val_loss: 0.2125\n",
      "Epoch 326/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1058 - val_loss: 0.2112\n",
      "Epoch 327/500\n",
      "112/112 [==============================] - 0s 626us/step - loss: 1.0895 - val_loss: 0.2113\n",
      "Epoch 328/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1027 - val_loss: 0.2261\n",
      "Epoch 329/500\n",
      "112/112 [==============================] - 0s 626us/step - loss: 1.1009 - val_loss: 0.2243\n",
      "Epoch 330/500\n",
      "112/112 [==============================] - 0s 800us/step - loss: 1.0899 - val_loss: 0.2317\n",
      "Epoch 331/500\n",
      "112/112 [==============================] - 0s 803us/step - loss: 1.0932 - val_loss: 0.2183\n",
      "Epoch 332/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0982 - val_loss: 0.2695\n",
      "Epoch 333/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0938 - val_loss: 0.2104\n",
      "Epoch 334/500\n",
      "112/112 [==============================] - 0s 669us/step - loss: 1.0971 - val_loss: 0.2171\n",
      "Epoch 335/500\n",
      "112/112 [==============================] - 0s 758us/step - loss: 1.1068 - val_loss: 0.2226\n",
      "Epoch 336/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0934 - val_loss: 0.2129\n",
      "Epoch 337/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.0989 - val_loss: 0.2133\n",
      "Epoch 338/500\n",
      "112/112 [==============================] - 0s 762us/step - loss: 1.0967 - val_loss: 0.2157\n",
      "Epoch 339/500\n",
      "112/112 [==============================] - 0s 625us/step - loss: 1.0935 - val_loss: 0.2130\n",
      "Epoch 340/500\n",
      "112/112 [==============================] - 0s 803us/step - loss: 1.0902 - val_loss: 0.2106\n",
      "Epoch 341/500\n",
      "112/112 [==============================] - 0s 626us/step - loss: 1.0999 - val_loss: 0.2314\n",
      "Epoch 342/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0887 - val_loss: 0.2176\n",
      "Epoch 343/500\n",
      "112/112 [==============================] - 0s 803us/step - loss: 1.0863 - val_loss: 0.2118\n",
      "Epoch 344/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0956 - val_loss: 0.2503\n",
      "Epoch 345/500\n",
      "112/112 [==============================] - 0s 720us/step - loss: 1.0968 - val_loss: 0.2143\n",
      "Epoch 346/500\n",
      "112/112 [==============================] - 0s 708us/step - loss: 1.1006 - val_loss: 0.2162\n",
      "Epoch 347/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.0882 - val_loss: 0.2143\n",
      "Epoch 348/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0948 - val_loss: 0.2129\n",
      "Epoch 349/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1027 - val_loss: 0.2135\n",
      "Epoch 350/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0835 - val_loss: 0.2142\n",
      "Epoch 351/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1010 - val_loss: 0.2140\n",
      "Epoch 352/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.0985 - val_loss: 0.2149\n",
      "Epoch 353/500\n",
      "112/112 [==============================] - 0s 670us/step - loss: 1.0958 - val_loss: 0.2283\n",
      "Epoch 354/500\n",
      "112/112 [==============================] - 0s 674us/step - loss: 1.0929 - val_loss: 0.2272\n",
      "Epoch 355/500\n",
      "112/112 [==============================] - 0s 623us/step - loss: 1.0932 - val_loss: 0.2199\n",
      "Epoch 356/500\n",
      "112/112 [==============================] - 0s 895us/step - loss: 1.0950 - val_loss: 0.2292\n",
      "Epoch 357/500\n",
      "112/112 [==============================] - 0s 624us/step - loss: 1.0854 - val_loss: 0.2129\n",
      "Epoch 358/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.1022 - val_loss: 0.2183\n",
      "Epoch 359/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.0943 - val_loss: 0.2191\n",
      "Epoch 360/500\n",
      "112/112 [==============================] - 0s 801us/step - loss: 1.0960 - val_loss: 0.2182\n",
      "Epoch 361/500\n",
      "112/112 [==============================] - 0s 635us/step - loss: 1.0939 - val_loss: 0.2408\n",
      "Epoch 362/500\n",
      "112/112 [==============================] - 0s 707us/step - loss: 1.0850 - val_loss: 0.2483\n",
      "Epoch 363/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.0961 - val_loss: 0.2264\n",
      "Epoch 364/500\n",
      "112/112 [==============================] - 0s 895us/step - loss: 1.0956 - val_loss: 0.2333\n",
      "Epoch 365/500\n",
      "112/112 [==============================] - 0s 625us/step - loss: 1.0949 - val_loss: 0.2160\n",
      "Epoch 366/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0808 - val_loss: 0.2147\n",
      "Epoch 367/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.1114 - val_loss: 0.2129\n",
      "Epoch 368/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0892 - val_loss: 0.2160\n",
      "Epoch 369/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.0941 - val_loss: 0.2267\n",
      "Epoch 370/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.0984 - val_loss: 0.2153\n",
      "Epoch 371/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.0859 - val_loss: 0.2339\n",
      "Epoch 372/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0801 - val_loss: 0.2268\n",
      "Epoch 373/500\n",
      "112/112 [==============================] - 0s 758us/step - loss: 1.1025 - val_loss: 0.2217\n",
      "Epoch 374/500\n",
      "112/112 [==============================] - 0s 670us/step - loss: 1.0936 - val_loss: 0.2171\n",
      "Epoch 375/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.0933 - val_loss: 0.2851\n",
      "Epoch 376/500\n",
      "112/112 [==============================] - 0s 718us/step - loss: 1.1006 - val_loss: 0.2113\n",
      "Epoch 377/500\n",
      "112/112 [==============================] - 0s 624us/step - loss: 1.0909 - val_loss: 0.2250\n",
      "Epoch 378/500\n",
      "112/112 [==============================] - 0s 804us/step - loss: 1.0914 - val_loss: 0.2143\n",
      "Epoch 379/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0971 - val_loss: 0.2214\n",
      "Epoch 380/500\n",
      "112/112 [==============================] - 0s 624us/step - loss: 1.0945 - val_loss: 0.2185\n",
      "Epoch 381/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.0926 - val_loss: 0.2147\n",
      "Epoch 382/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0813 - val_loss: 0.2168\n",
      "Epoch 383/500\n",
      "112/112 [==============================] - 0s 735us/step - loss: 1.1031 - val_loss: 0.2587\n",
      "Epoch 384/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.0985 - val_loss: 0.2166\n",
      "Epoch 385/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.1038 - val_loss: 0.2193\n",
      "Epoch 386/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.108 - 0s 803us/step - loss: 1.0927 - val_loss: 0.2214\n",
      "Epoch 387/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.0889 - val_loss: 0.2184\n",
      "Epoch 388/500\n",
      "112/112 [==============================] - 0s 627us/step - loss: 1.1046 - val_loss: 0.2142\n",
      "Epoch 389/500\n",
      "112/112 [==============================] - 0s 830us/step - loss: 1.0900 - val_loss: 0.2261\n",
      "Epoch 390/500\n",
      "112/112 [==============================] - 0s 689us/step - loss: 1.1007 - val_loss: 0.2185\n",
      "Epoch 391/500\n",
      "112/112 [==============================] - 0s 801us/step - loss: 1.0864 - val_loss: 0.2246\n",
      "Epoch 392/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0936 - val_loss: 0.2166\n",
      "Epoch 393/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 1.562 - 0s 713us/step - loss: 1.1007 - val_loss: 0.2167\n",
      "Epoch 394/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0847 - val_loss: 0.2315\n",
      "Epoch 395/500\n",
      "112/112 [==============================] - 0s 761us/step - loss: 1.1009 - val_loss: 0.2327\n",
      "Epoch 396/500\n",
      "112/112 [==============================] - 0s 667us/step - loss: 1.0786 - val_loss: 0.2163\n",
      "Epoch 397/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1086 - val_loss: 0.2170\n",
      "Epoch 398/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.124 - 0s 714us/step - loss: 1.1033 - val_loss: 0.2216\n",
      "Epoch 399/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0885 - val_loss: 0.2192\n",
      "Epoch 400/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.1090 - val_loss: 0.2140\n",
      "Epoch 401/500\n",
      "112/112 [==============================] - 0s 757us/step - loss: 1.0931 - val_loss: 0.2146\n",
      "Epoch 402/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.0925 - val_loss: 0.2272\n",
      "Epoch 403/500\n",
      "112/112 [==============================] - 0s 622us/step - loss: 1.0904 - val_loss: 0.2157\n",
      "Epoch 404/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0956 - val_loss: 0.2226\n",
      "Epoch 405/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0914 - val_loss: 0.2301\n",
      "Epoch 406/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0958 - val_loss: 0.2316\n",
      "Epoch 407/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0926 - val_loss: 0.2271\n",
      "Epoch 408/500\n",
      "112/112 [==============================] - 0s 713us/step - loss: 1.0833 - val_loss: 0.2166\n",
      "Epoch 409/500\n",
      "112/112 [==============================] - 0s 717us/step - loss: 1.1033 - val_loss: 0.2189\n",
      "Epoch 410/500\n",
      "112/112 [==============================] - 0s 625us/step - loss: 1.0952 - val_loss: 0.2171\n",
      "Epoch 411/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0905 - val_loss: 0.2565\n",
      "Epoch 412/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0937 - val_loss: 0.2296\n",
      "Epoch 413/500\n",
      "112/112 [==============================] - 0s 624us/step - loss: 1.0744 - val_loss: 0.2149\n",
      "Epoch 414/500\n",
      "112/112 [==============================] - 0s 719us/step - loss: 1.1116 - val_loss: 0.2196\n",
      "Epoch 415/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.0883 - val_loss: 0.2230\n",
      "Epoch 416/500\n",
      "112/112 [==============================] - 0s 626us/step - loss: 1.0959 - val_loss: 0.2192\n",
      "Epoch 417/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0894 - val_loss: 0.2178\n",
      "Epoch 418/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0849 - val_loss: 0.2164\n",
      "Epoch 419/500\n",
      "112/112 [==============================] - 0s 754us/step - loss: 1.0995 - val_loss: 0.2178\n",
      "Epoch 420/500\n",
      "112/112 [==============================] - 0s 764us/step - loss: 1.0919 - val_loss: 0.2188\n",
      "Epoch 421/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.0999 - val_loss: 0.2169\n",
      "Epoch 422/500\n",
      "112/112 [==============================] - 0s 710us/step - loss: 1.0937 - val_loss: 0.2180\n",
      "Epoch 423/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0841 - val_loss: 0.2219\n",
      "Epoch 424/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.1019 - val_loss: 0.2207\n",
      "Epoch 425/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0935 - val_loss: 0.2264\n",
      "Epoch 426/500\n",
      "112/112 [==============================] - 0s 716us/step - loss: 1.0987 - val_loss: 0.2206\n",
      "Epoch 427/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0893 - val_loss: 0.2230\n",
      "Epoch 428/500\n",
      "112/112 [==============================] - 0s 714us/step - loss: 1.0898 - val_loss: 0.2330\n",
      "Epoch 429/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.0913 - val_loss: 0.2264\n",
      "Epoch 430/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.0944 - val_loss: 0.2197\n",
      "Epoch 431/500\n",
      "112/112 [==============================] - 0s 805us/step - loss: 1.0920 - val_loss: 0.2218\n",
      "Epoch 432/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0893 - val_loss: 0.2248\n",
      "Epoch 433/500\n",
      "112/112 [==============================] - 0s 759us/step - loss: 1.0913 - val_loss: 0.2187\n",
      "Epoch 434/500\n",
      "112/112 [==============================] - 0s 672us/step - loss: 1.0913 - val_loss: 0.2187\n",
      "Epoch 435/500\n",
      "112/112 [==============================] - 0s 802us/step - loss: 1.0947 - val_loss: 0.2165\n",
      "Epoch 436/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0904 - val_loss: 0.2255\n",
      "Epoch 437/500\n",
      "112/112 [==============================] - 0s 715us/step - loss: 1.0819 - val_loss: 0.2321\n",
      "Epoch 438/500\n",
      "112/112 [==============================] - 0s 895us/step - loss: 1.1135 - val_loss: 0.2276\n",
      "Epoch 439/500\n",
      "112/112 [==============================] - 0s 793us/step - loss: 1.0869 - val_loss: 0.2226\n",
      "Epoch 440/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0971 - val_loss: 0.2370\n",
      "Epoch 441/500\n",
      "112/112 [==============================] - 0s 704us/step - loss: 1.0901 - val_loss: 0.2199\n",
      "Epoch 442/500\n",
      "112/112 [==============================] - 0s 793us/step - loss: 1.0913 - val_loss: 0.2145\n",
      "Epoch 443/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.0931 - val_loss: 0.2203\n",
      "Epoch 444/500\n",
      "112/112 [==============================] - 0s 793us/step - loss: 1.0923 - val_loss: 0.2160\n",
      "Epoch 445/500\n",
      "112/112 [==============================] - 0s 748us/step - loss: 1.0938 - val_loss: 0.2220\n",
      "Epoch 446/500\n",
      "112/112 [==============================] - 0s 659us/step - loss: 1.0900 - val_loss: 0.2229\n",
      "Epoch 447/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0944 - val_loss: 0.2223\n",
      "Epoch 448/500\n",
      "112/112 [==============================] - 0s 691us/step - loss: 1.0866 - val_loss: 0.2183\n",
      "Epoch 449/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0963 - val_loss: 0.2265\n",
      "Epoch 450/500\n",
      "112/112 [==============================] - 0s 728us/step - loss: 1.0840 - val_loss: 0.2194\n",
      "Epoch 451/500\n",
      "112/112 [==============================] - 0s 722us/step - loss: 1.0817 - val_loss: 0.2682\n",
      "Epoch 452/500\n",
      "112/112 [==============================] - 0s 690us/step - loss: 1.0915 - val_loss: 0.2221\n",
      "Epoch 453/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0938 - val_loss: 0.2164\n",
      "Epoch 454/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0912 - val_loss: 0.2138\n",
      "Epoch 455/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.0986 - val_loss: 0.2265\n",
      "Epoch 456/500\n",
      "112/112 [==============================] - 0s 882us/step - loss: 1.0894 - val_loss: 0.2187\n",
      "Epoch 457/500\n",
      "112/112 [==============================] - 0s 676us/step - loss: 1.0947 - val_loss: 0.2375\n",
      "Epoch 458/500\n",
      "112/112 [==============================] - 0s 727us/step - loss: 1.0789 - val_loss: 0.2191\n",
      "Epoch 459/500\n",
      "112/112 [==============================] - 0s 735us/step - loss: 1.0998 - val_loss: 0.2287\n",
      "Epoch 460/500\n",
      "112/112 [==============================] - 0s 681us/step - loss: 1.0922 - val_loss: 0.2280\n",
      "Epoch 461/500\n",
      "112/112 [==============================] - 0s 846us/step - loss: 1.0964 - val_loss: 0.2174\n",
      "Epoch 462/500\n",
      "112/112 [==============================] - 0s 703us/step - loss: 1.0831 - val_loss: 0.2268\n",
      "Epoch 463/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.1069 - val_loss: 0.2343\n",
      "Epoch 464/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0818 - val_loss: 0.2512\n",
      "Epoch 465/500\n",
      "112/112 [==============================] - 0s 730us/step - loss: 1.0769 - val_loss: 0.2174\n",
      "Epoch 466/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.0960 - val_loss: 0.2236\n",
      "Epoch 467/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.1035 - val_loss: 0.2208\n",
      "Epoch 468/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.0849 - val_loss: 0.2210\n",
      "Epoch 469/500\n",
      "112/112 [==============================] - 0s 650us/step - loss: 1.0938 - val_loss: 0.2173\n",
      "Epoch 470/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0879 - val_loss: 0.2366\n",
      "Epoch 471/500\n",
      "112/112 [==============================] - 0s 681us/step - loss: 1.0803 - val_loss: 0.2203\n",
      "Epoch 472/500\n",
      "112/112 [==============================] - 0s 688us/step - loss: 1.0989 - val_loss: 0.2180\n",
      "Epoch 473/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0869 - val_loss: 0.2219\n",
      "Epoch 474/500\n",
      "112/112 [==============================] - 0s 694us/step - loss: 1.0971 - val_loss: 0.2223\n",
      "Epoch 475/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.0961 - val_loss: 0.2215\n",
      "Epoch 476/500\n",
      "112/112 [==============================] - 0s 703us/step - loss: 1.0895 - val_loss: 0.2187\n",
      "Epoch 477/500\n",
      "112/112 [==============================] - 0s 677us/step - loss: 1.0927 - val_loss: 0.2188\n",
      "Epoch 478/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.0785 - val_loss: 0.2239\n",
      "Epoch 479/500\n",
      "112/112 [==============================] - 0s 691us/step - loss: 1.1059 - val_loss: 0.2200\n",
      "Epoch 480/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0887 - val_loss: 0.2207\n",
      "Epoch 481/500\n",
      "112/112 [==============================] - 0s 703us/step - loss: 1.0954 - val_loss: 0.2190\n",
      "Epoch 482/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0918 - val_loss: 0.2164\n",
      "Epoch 483/500\n",
      "112/112 [==============================] - 0s 721us/step - loss: 1.0855 - val_loss: 0.2436\n",
      "Epoch 484/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.0992 - val_loss: 0.2223\n",
      "Epoch 485/500\n",
      "112/112 [==============================] - 0s 677us/step - loss: 1.0897 - val_loss: 0.2240\n",
      "Epoch 486/500\n",
      "112/112 [==============================] - 0s 650us/step - loss: 1.0906 - val_loss: 0.2133\n",
      "Epoch 487/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0949 - val_loss: 0.2217\n",
      "Epoch 488/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.0836 - val_loss: 0.2217\n",
      "Epoch 489/500\n",
      "112/112 [==============================] - 0s 730us/step - loss: 1.0998 - val_loss: 0.2197\n",
      "Epoch 490/500\n",
      "112/112 [==============================] - 0s 668us/step - loss: 1.0860 - val_loss: 0.2278\n",
      "Epoch 491/500\n",
      "112/112 [==============================] - 0s 686us/step - loss: 1.0820 - val_loss: 0.2493\n",
      "Epoch 492/500\n",
      "112/112 [==============================] - 0s 677us/step - loss: 1.0935 - val_loss: 0.2184\n",
      "Epoch 493/500\n",
      "112/112 [==============================] - 0s 739us/step - loss: 1.0876 - val_loss: 0.2221\n",
      "Epoch 494/500\n",
      "112/112 [==============================] - 0s 692us/step - loss: 1.0922 - val_loss: 0.2254\n",
      "Epoch 495/500\n",
      "112/112 [==============================] - 0s 711us/step - loss: 1.0847 - val_loss: 0.2432\n",
      "Epoch 496/500\n",
      "112/112 [==============================] - 0s 730us/step - loss: 1.0938 - val_loss: 0.2311\n",
      "Epoch 497/500\n",
      "112/112 [==============================] - 0s 712us/step - loss: 1.0878 - val_loss: 0.2314\n",
      "Epoch 498/500\n",
      "112/112 [==============================] - 0s 705us/step - loss: 1.0929 - val_loss: 0.2282\n",
      "Epoch 499/500\n",
      "112/112 [==============================] - 0s 695us/step - loss: 1.0900 - val_loss: 0.2287\n",
      "Epoch 500/500\n",
      "112/112 [==============================] - 0s 663us/step - loss: 1.0901 - val_loss: 0.2139\n",
      "70/70 [==============================] - 0s 28us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9245880740029472"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "requiredData = newDataFrame.loc[:, newDataFrame.columns.intersection([\"DaysTotal_Days\" ,\"new_cases\",\"stringency_index\"])]\n",
    "# requiredData = newDataFrame.drop(columns=['AverageInfectionRate'])\n",
    "# requiredData = requiredData.drop(columns=['AverageDeathRate'])\n",
    "\n",
    "\n",
    "X = requiredData.drop(columns=['new_cases'])\n",
    "y = requiredData.loc[:, requiredData.columns.intersection([\"new_cases\"])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "\n",
    "mean_train = X_train.mean(axis = 0)\n",
    "std_dev_train = X_train.std(axis = 0)\n",
    "std_dev_test = X_test.std(axis = 0)\n",
    "\n",
    "X_train = (X_train - mean_train)/std_dev_train\n",
    "X_test = (X_test - mean_train)/std_dev_test\n",
    "\n",
    "\n",
    "mean_train = y_train.mean(axis = 0)\n",
    "std_dev_train = y_train.std(axis = 0)\n",
    "std_dev_test = y_test.std(axis = 0)\n",
    "\n",
    "y_train = (y_train - mean_train)/std_dev_train\n",
    "y_test = (y_test  - mean_train)/std_dev_test\n",
    "\n",
    "# X_train[\"location\"] = ohe.transform(categories).todense() \n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "in_shape = X_train.iloc[0].shape\n",
    "model = Sequential()\n",
    "# model.add(Dense(4, activation = 'relu', input_shape = in_shape))\n",
    "# model.add(Dense(4, activation = 'relu', input_shape = in_shape))\n",
    "# model.add(Dense(1, activation = 'linear'))\n",
    "model.add(Dense(12, activation = 'relu', input_shape = in_shape))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 500, batch_size = 1, validation_split = 0.2, verbose = 1)\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>DaysTotal_Days</th>\n",
       "      <th>AverageInfectionRate</th>\n",
       "      <th>AverageDeathRate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>26310.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>44.956867</td>\n",
       "      <td>38928341.0</td>\n",
       "      <td>54.422</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.581</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.50</td>\n",
       "      <td>160</td>\n",
       "      <td>164.437500</td>\n",
       "      <td>3.068750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>1672.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>81.544747</td>\n",
       "      <td>2877800.0</td>\n",
       "      <td>104.871</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.188</td>\n",
       "      <td>8.643</td>\n",
       "      <td>11803.431</td>\n",
       "      <td>304.195</td>\n",
       "      <td>10.08</td>\n",
       "      <td>2.89</td>\n",
       "      <td>101</td>\n",
       "      <td>16.554455</td>\n",
       "      <td>0.366337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>11147.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>43851043.0</td>\n",
       "      <td>17.348</td>\n",
       "      <td>29.1</td>\n",
       "      <td>6.211</td>\n",
       "      <td>3.857</td>\n",
       "      <td>13913.839</td>\n",
       "      <td>278.364</td>\n",
       "      <td>6.73</td>\n",
       "      <td>1.90</td>\n",
       "      <td>165</td>\n",
       "      <td>67.557576</td>\n",
       "      <td>4.775758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andorra</th>\n",
       "      <td>854.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>47.253667</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>163.755</td>\n",
       "      <td>29.1</td>\n",
       "      <td>6.211</td>\n",
       "      <td>3.857</td>\n",
       "      <td>13913.839</td>\n",
       "      <td>109.135</td>\n",
       "      <td>7.97</td>\n",
       "      <td>1.90</td>\n",
       "      <td>96</td>\n",
       "      <td>8.895833</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>142.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.319770</td>\n",
       "      <td>32866268.0</td>\n",
       "      <td>23.890</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.405</td>\n",
       "      <td>1.362</td>\n",
       "      <td>5819.495</td>\n",
       "      <td>276.045</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1.90</td>\n",
       "      <td>88</td>\n",
       "      <td>1.613636</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.593533</td>\n",
       "      <td>97338583.0</td>\n",
       "      <td>308.127</td>\n",
       "      <td>32.6</td>\n",
       "      <td>7.150</td>\n",
       "      <td>4.718</td>\n",
       "      <td>6171.884</td>\n",
       "      <td>245.465</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>170</td>\n",
       "      <td>2.018072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western Sahara</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.593533</td>\n",
       "      <td>597330.0</td>\n",
       "      <td>308.127</td>\n",
       "      <td>28.4</td>\n",
       "      <td>7.150</td>\n",
       "      <td>1.380</td>\n",
       "      <td>6171.884</td>\n",
       "      <td>245.465</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>53</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.018868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yemen</th>\n",
       "      <td>889.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>51.665000</td>\n",
       "      <td>29825968.0</td>\n",
       "      <td>53.508</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2.922</td>\n",
       "      <td>1.583</td>\n",
       "      <td>1479.147</td>\n",
       "      <td>495.003</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.70</td>\n",
       "      <td>69</td>\n",
       "      <td>12.884058</td>\n",
       "      <td>3.115942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zambia</th>\n",
       "      <td>1405.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47.145393</td>\n",
       "      <td>18383956.0</td>\n",
       "      <td>22.995</td>\n",
       "      <td>17.7</td>\n",
       "      <td>2.480</td>\n",
       "      <td>1.542</td>\n",
       "      <td>3689.251</td>\n",
       "      <td>234.499</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.00</td>\n",
       "      <td>91</td>\n",
       "      <td>15.439560</td>\n",
       "      <td>0.120879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zimbabwe</th>\n",
       "      <td>394.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.550132</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.70</td>\n",
       "      <td>89</td>\n",
       "      <td>4.426966</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                new_cases  new_deaths  stringency_index  population  \\\n",
       "location                                                              \n",
       "Afghanistan       26310.0       491.0         44.956867  38928341.0   \n",
       "Albania            1672.0        37.0         81.544747   2877800.0   \n",
       "Algeria           11147.0       788.0         46.875000  43851043.0   \n",
       "Andorra             854.0        52.0         47.253667     77265.0   \n",
       "Angola              142.0         6.0         78.319770  32866268.0   \n",
       "...                   ...         ...               ...         ...   \n",
       "Vietnam             335.0         0.0         51.593533  97338583.0   \n",
       "Western Sahara       23.0         1.0         51.593533    597330.0   \n",
       "Yemen               889.0       215.0         51.665000  29825968.0   \n",
       "Zambia             1405.0        11.0         47.145393  18383956.0   \n",
       "Zimbabwe            394.0         4.0         83.550132  14862927.0   \n",
       "\n",
       "                population_density  median_age  aged_65_older  aged_70_older  \\\n",
       "location                                                                       \n",
       "Afghanistan                 54.422        18.6          2.581          1.337   \n",
       "Albania                    104.871        38.0         13.188          8.643   \n",
       "Algeria                     17.348        29.1          6.211          3.857   \n",
       "Andorra                    163.755        29.1          6.211          3.857   \n",
       "Angola                      23.890        16.8          2.405          1.362   \n",
       "...                            ...         ...            ...            ...   \n",
       "Vietnam                    308.127        32.6          7.150          4.718   \n",
       "Western Sahara             308.127        28.4          7.150          1.380   \n",
       "Yemen                       53.508        20.3          2.922          1.583   \n",
       "Zambia                      22.995        17.7          2.480          1.542   \n",
       "Zimbabwe                    42.729        19.6          2.822          1.882   \n",
       "\n",
       "                gdp_per_capita  cvd_death_rate  diabetes_prevalence  \\\n",
       "location                                                              \n",
       "Afghanistan           1803.987         597.029                 9.59   \n",
       "Albania              11803.431         304.195                10.08   \n",
       "Algeria              13913.839         278.364                 6.73   \n",
       "Andorra              13913.839         109.135                 7.97   \n",
       "Angola                5819.495         276.045                 3.94   \n",
       "...                        ...             ...                  ...   \n",
       "Vietnam               6171.884         245.465                 6.00   \n",
       "Western Sahara        6171.884         245.465                 6.00   \n",
       "Yemen                 1479.147         495.003                 5.35   \n",
       "Zambia                3689.251         234.499                 3.94   \n",
       "Zimbabwe              1899.775         307.846                 1.82   \n",
       "\n",
       "                hospital_beds_per_thousand  DaysTotal_Days  \\\n",
       "location                                                     \n",
       "Afghanistan                           0.50             160   \n",
       "Albania                               2.89             101   \n",
       "Algeria                               1.90             165   \n",
       "Andorra                               1.90              96   \n",
       "Angola                                1.90              88   \n",
       "...                                    ...             ...   \n",
       "Vietnam                               2.60             170   \n",
       "Western Sahara                        2.60              53   \n",
       "Yemen                                 0.70              69   \n",
       "Zambia                                2.00              91   \n",
       "Zimbabwe                              1.70              89   \n",
       "\n",
       "                AverageInfectionRate  AverageDeathRate  \n",
       "location                                                \n",
       "Afghanistan               164.437500          3.068750  \n",
       "Albania                    16.554455          0.366337  \n",
       "Algeria                    67.557576          4.775758  \n",
       "Andorra                     8.895833          0.541667  \n",
       "Angola                      1.613636          0.068182  \n",
       "...                              ...               ...  \n",
       "Vietnam                     2.018072          0.000000  \n",
       "Western Sahara              0.433962          0.018868  \n",
       "Yemen                      12.884058          3.115942  \n",
       "Zambia                     15.439560          0.120879  \n",
       "Zimbabwe                    4.426966          0.044944  \n",
       "\n",
       "[210 rows x 15 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_98 (Dense)             (None, 4)                 8         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 112 samples, validate on 28 samples\n",
      "Epoch 1/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.2635 - val_loss: 0.1976\n",
      "Epoch 2/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2141 - val_loss: 0.2113\n",
      "Epoch 3/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2012 - val_loss: 0.1923\n",
      "Epoch 4/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2109 - val_loss: 0.2218\n",
      "Epoch 5/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2072 - val_loss: 0.2013\n",
      "Epoch 6/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2101 - val_loss: 0.2074\n",
      "Epoch 7/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2050 - val_loss: 0.1951\n",
      "Epoch 8/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2026 - val_loss: 0.1985\n",
      "Epoch 9/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1826 - val_loss: 0.2847\n",
      "Epoch 10/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2006 - val_loss: 0.1985\n",
      "Epoch 11/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.2069 - val_loss: 0.2301\n",
      "Epoch 12/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.2006 - val_loss: 0.2105\n",
      "Epoch 13/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1979 - val_loss: 0.2313\n",
      "Epoch 14/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.2016 - val_loss: 0.2148\n",
      "Epoch 15/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1909 - val_loss: 0.2613\n",
      "Epoch 16/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1992 - val_loss: 0.2149\n",
      "Epoch 17/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1831 - val_loss: 0.2468\n",
      "Epoch 18/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1905 - val_loss: 0.2170\n",
      "Epoch 19/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1898 - val_loss: 0.2237\n",
      "Epoch 20/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1770 - val_loss: 0.2286\n",
      "Epoch 21/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1754 - val_loss: 0.2547\n",
      "Epoch 22/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1772 - val_loss: 0.2814\n",
      "Epoch 23/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1842 - val_loss: 0.2839\n",
      "Epoch 24/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1810 - val_loss: 0.2359\n",
      "Epoch 25/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1721 - val_loss: 0.3239\n",
      "Epoch 26/500\n",
      "112/112 [==============================] - 0s 896us/step - loss: 1.1819 - val_loss: 0.2716\n",
      "Epoch 27/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1798 - val_loss: 0.2642\n",
      "Epoch 28/500\n",
      "112/112 [==============================] - 0s 890us/step - loss: 1.1801 - val_loss: 0.2908\n",
      "Epoch 29/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1749 - val_loss: 0.3106\n",
      "Epoch 30/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1792 - val_loss: 0.2981\n",
      "Epoch 31/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1786 - val_loss: 0.2815\n",
      "Epoch 32/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1810 - val_loss: 0.2861\n",
      "Epoch 33/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1731 - val_loss: 0.2669\n",
      "Epoch 34/500\n",
      "112/112 [==============================] - 0s 891us/step - loss: 1.1756 - val_loss: 0.2936\n",
      "Epoch 35/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1623 - val_loss: 0.2826\n",
      "Epoch 36/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.1642 - val_loss: 0.3517\n",
      "Epoch 37/500\n",
      "112/112 [==============================] - 0s 852us/step - loss: 1.1767 - val_loss: 0.2721\n",
      "Epoch 38/500\n",
      "112/112 [==============================] - 0s 890us/step - loss: 1.1767 - val_loss: 0.2634\n",
      "Epoch 39/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1724 - val_loss: 0.2982\n",
      "Epoch 40/500\n",
      "112/112 [==============================] - 0s 935us/step - loss: 1.1750 - val_loss: 0.2827\n",
      "Epoch 41/500\n",
      "112/112 [==============================] - 0s 806us/step - loss: 1.1718 - val_loss: 0.2696\n",
      "Epoch 42/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1686 - val_loss: 0.3194\n",
      "Epoch 43/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1664 - val_loss: 0.2898\n",
      "Epoch 44/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1751 - val_loss: 0.3201\n",
      "Epoch 45/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1666 - val_loss: 0.3279\n",
      "Epoch 46/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1693 - val_loss: 0.3230\n",
      "Epoch 47/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1677 - val_loss: 0.3472\n",
      "Epoch 48/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1672 - val_loss: 0.2776\n",
      "Epoch 49/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1760 - val_loss: 0.2906\n",
      "Epoch 50/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1695 - val_loss: 0.3007\n",
      "Epoch 51/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1777 - val_loss: 0.2906\n",
      "Epoch 52/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1666 - val_loss: 0.3160\n",
      "Epoch 53/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1759 - val_loss: 0.3101\n",
      "Epoch 54/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1595 - val_loss: 0.2753\n",
      "Epoch 55/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1699 - val_loss: 0.2804\n",
      "Epoch 56/500\n",
      "112/112 [==============================] - 0s 891us/step - loss: 1.1511 - val_loss: 0.3315\n",
      "Epoch 57/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1715 - val_loss: 0.2897\n",
      "Epoch 58/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1465 - val_loss: 0.3468\n",
      "Epoch 59/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1687 - val_loss: 0.2936\n",
      "Epoch 60/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1698 - val_loss: 0.2796\n",
      "Epoch 61/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1557 - val_loss: 0.3611\n",
      "Epoch 62/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1681 - val_loss: 0.3115\n",
      "Epoch 63/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1662 - val_loss: 0.2786\n",
      "Epoch 64/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1658 - val_loss: 0.2708\n",
      "Epoch 65/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1673 - val_loss: 0.3052\n",
      "Epoch 66/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1632 - val_loss: 0.2800\n",
      "Epoch 67/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.1658 - val_loss: 0.3160\n",
      "Epoch 68/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1628 - val_loss: 0.3199\n",
      "Epoch 69/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1656 - val_loss: 0.2780\n",
      "Epoch 70/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1565 - val_loss: 0.2925\n",
      "Epoch 71/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1662 - val_loss: 0.3025\n",
      "Epoch 72/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1666 - val_loss: 0.2810\n",
      "Epoch 73/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1592 - val_loss: 0.2909\n",
      "Epoch 74/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1439 - val_loss: 0.3687\n",
      "Epoch 75/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1686 - val_loss: 0.3116\n",
      "Epoch 76/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1565 - val_loss: 0.3055\n",
      "Epoch 77/500\n",
      "112/112 [==============================] - 0s 896us/step - loss: 1.1646 - val_loss: 0.3600\n",
      "Epoch 78/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1620 - val_loss: 0.3701\n",
      "Epoch 79/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1713 - val_loss: 0.3008\n",
      "Epoch 80/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1626 - val_loss: 0.3151\n",
      "Epoch 81/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1634 - val_loss: 0.3162\n",
      "Epoch 82/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1664 - val_loss: 0.3042\n",
      "Epoch 83/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1597 - val_loss: 0.3079\n",
      "Epoch 84/500\n",
      "112/112 [==============================] - 0s 978us/step - loss: 1.1588 - val_loss: 0.2971\n",
      "Epoch 85/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1622 - val_loss: 0.3466\n",
      "Epoch 86/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1630 - val_loss: 0.2985\n",
      "Epoch 87/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1389 - val_loss: 0.4096\n",
      "Epoch 88/500\n",
      "112/112 [==============================] - 0s 938us/step - loss: 1.1622 - val_loss: 0.4158\n",
      "Epoch 89/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1585 - val_loss: 0.3580\n",
      "Epoch 90/500\n",
      "112/112 [==============================] - 0s 989us/step - loss: 1.1617 - val_loss: 0.2856\n",
      "Epoch 91/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1639 - val_loss: 0.3135\n",
      "Epoch 92/500\n",
      "112/112 [==============================] - 0s 986us/step - loss: 1.1508 - val_loss: 0.3713\n",
      "Epoch 93/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1601 - val_loss: 0.3018\n",
      "Epoch 94/500\n",
      "112/112 [==============================] - 0s 889us/step - loss: 1.1643 - val_loss: 0.3148\n",
      "Epoch 95/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1595 - val_loss: 0.3462\n",
      "Epoch 96/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1664 - val_loss: 0.3386\n",
      "Epoch 97/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1582 - val_loss: 0.3057\n",
      "Epoch 98/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1537 - val_loss: 0.2930\n",
      "Epoch 99/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1590 - val_loss: 0.3131\n",
      "Epoch 100/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1372 - val_loss: 0.2932\n",
      "Epoch 101/500\n",
      "112/112 [==============================] - 0s 985us/step - loss: 1.1598 - val_loss: 0.3381\n",
      "Epoch 102/500\n",
      "112/112 [==============================] - 0s 939us/step - loss: 1.1578 - val_loss: 0.3250\n",
      "Epoch 103/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1617 - val_loss: 0.2971\n",
      "Epoch 104/500\n",
      "112/112 [==============================] - 0s 938us/step - loss: 1.1565 - val_loss: 0.3508\n",
      "Epoch 105/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1613 - val_loss: 0.3101\n",
      "Epoch 106/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.1445 - val_loss: 0.4251\n",
      "Epoch 107/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1614 - val_loss: 0.3494\n",
      "Epoch 108/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1585 - val_loss: 0.3659\n",
      "Epoch 109/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1639 - val_loss: 0.3372\n",
      "Epoch 110/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1623 - val_loss: 0.3319\n",
      "Epoch 111/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1595 - val_loss: 0.3455\n",
      "Epoch 112/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1464 - val_loss: 0.3010\n",
      "Epoch 113/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1651 - val_loss: 0.3111\n",
      "Epoch 114/500\n",
      "112/112 [==============================] - 0s 935us/step - loss: 1.1583 - val_loss: 0.3415\n",
      "Epoch 115/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1518 - val_loss: 0.3030\n",
      "Epoch 116/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1654 - val_loss: 0.3349\n",
      "Epoch 117/500\n",
      "112/112 [==============================] - 0s 938us/step - loss: 1.1616 - val_loss: 0.3247\n",
      "Epoch 118/500\n",
      "112/112 [==============================] - 0s 904us/step - loss: 1.1597 - val_loss: 0.3099\n",
      "Epoch 119/500\n",
      "112/112 [==============================] - 0s 895us/step - loss: 1.1621 - val_loss: 0.3134\n",
      "Epoch 120/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1598 - val_loss: 0.3295\n",
      "Epoch 121/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1470 - val_loss: 0.4042\n",
      "Epoch 122/500\n",
      "112/112 [==============================] - 0s 937us/step - loss: 1.1601 - val_loss: 0.3688\n",
      "Epoch 123/500\n",
      "112/112 [==============================] - 0s 938us/step - loss: 1.1630 - val_loss: 0.3155\n",
      "Epoch 124/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1550 - val_loss: 0.3216\n",
      "Epoch 125/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1537 - val_loss: 0.2948\n",
      "Epoch 126/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1565 - val_loss: 0.3550\n",
      "Epoch 127/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1583 - val_loss: 0.2944\n",
      "Epoch 128/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1600 - val_loss: 0.2981\n",
      "Epoch 129/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1578 - val_loss: 0.3036\n",
      "Epoch 130/500\n",
      "112/112 [==============================] - 0s 990us/step - loss: 1.1567 - val_loss: 0.3631\n",
      "Epoch 131/500\n",
      "112/112 [==============================] - 0s 932us/step - loss: 1.1615 - val_loss: 0.3129\n",
      "Epoch 132/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1437 - val_loss: 0.3276\n",
      "Epoch 133/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1575 - val_loss: 0.2924\n",
      "Epoch 134/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1624 - val_loss: 0.2923\n",
      "Epoch 135/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1460 - val_loss: 0.3843\n",
      "Epoch 136/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1532 - val_loss: 0.3551\n",
      "Epoch 137/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1568 - val_loss: 0.3334\n",
      "Epoch 138/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1587 - val_loss: 0.3269\n",
      "Epoch 139/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1548 - val_loss: 0.3071\n",
      "Epoch 140/500\n",
      "112/112 [==============================] - 0s 978us/step - loss: 1.1604 - val_loss: 0.3293\n",
      "Epoch 141/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1558 - val_loss: 0.3268\n",
      "Epoch 142/500\n",
      "112/112 [==============================] - 0s 986us/step - loss: 1.1537 - val_loss: 0.3192\n",
      "Epoch 143/500\n",
      "112/112 [==============================] - 0s 890us/step - loss: 1.1536 - val_loss: 0.2975\n",
      "Epoch 144/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.888 - 0s 983us/step - loss: 1.1551 - val_loss: 0.3506\n",
      "Epoch 145/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1607 - val_loss: 0.3431\n",
      "Epoch 146/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1557 - val_loss: 0.3302\n",
      "Epoch 147/500\n",
      "112/112 [==============================] - 0s 961us/step - loss: 1.1555 - val_loss: 0.3265\n",
      "Epoch 148/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1597 - val_loss: 0.3208\n",
      "Epoch 149/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1562 - val_loss: 0.3409\n",
      "Epoch 150/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1544 - val_loss: 0.3458\n",
      "Epoch 151/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1355 - val_loss: 0.2986\n",
      "Epoch 152/500\n",
      "112/112 [==============================] - 0s 976us/step - loss: 1.1623 - val_loss: 0.3297\n",
      "Epoch 153/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1570 - val_loss: 0.3092\n",
      "Epoch 154/500\n",
      "112/112 [==============================] - 0s 891us/step - loss: 1.1569 - val_loss: 0.3233\n",
      "Epoch 155/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1570 - val_loss: 0.3124\n",
      "Epoch 156/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1592 - val_loss: 0.3078\n",
      "Epoch 157/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1646 - val_loss: 0.3135\n",
      "Epoch 158/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1564 - val_loss: 0.3144\n",
      "Epoch 159/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1464 - val_loss: 0.4958\n",
      "Epoch 160/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1656 - val_loss: 0.3408\n",
      "Epoch 161/500\n",
      "112/112 [==============================] - 0s 931us/step - loss: 1.1529 - val_loss: 0.3941\n",
      "Epoch 162/500\n",
      "112/112 [==============================] - 0s 942us/step - loss: 1.1495 - val_loss: 0.4631\n",
      "Epoch 163/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1578 - val_loss: 0.3817\n",
      "Epoch 164/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1558 - val_loss: 0.4224\n",
      "Epoch 165/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1627 - val_loss: 0.3215\n",
      "Epoch 166/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1590 - val_loss: 0.3197\n",
      "Epoch 167/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1570 - val_loss: 0.3108\n",
      "Epoch 168/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1512 - val_loss: 0.3331\n",
      "Epoch 169/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1435 - val_loss: 0.5029\n",
      "Epoch 170/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1638 - val_loss: 0.3613\n",
      "Epoch 171/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1569 - val_loss: 0.3249\n",
      "Epoch 172/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1504 - val_loss: 0.3679\n",
      "Epoch 173/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1555 - val_loss: 0.2981\n",
      "Epoch 174/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1554 - val_loss: 0.3478\n",
      "Epoch 175/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1615 - val_loss: 0.4327\n",
      "Epoch 176/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1600 - val_loss: 0.3967\n",
      "Epoch 177/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1602 - val_loss: 0.3527\n",
      "Epoch 178/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1384 - val_loss: 0.4894\n",
      "Epoch 179/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1559 - val_loss: 0.4688\n",
      "Epoch 180/500\n",
      "112/112 [==============================] - 0s 986us/step - loss: 1.1557 - val_loss: 0.3129\n",
      "Epoch 181/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1597 - val_loss: 0.3053\n",
      "Epoch 182/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.1592 - val_loss: 0.3591\n",
      "Epoch 183/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1563 - val_loss: 0.3218\n",
      "Epoch 184/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1605 - val_loss: 0.3288\n",
      "Epoch 185/500\n",
      "112/112 [==============================] - 0s 937us/step - loss: 1.1555 - val_loss: 0.3199\n",
      "Epoch 186/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1508 - val_loss: 0.3265\n",
      "Epoch 187/500\n",
      "112/112 [==============================] - 0s 891us/step - loss: 1.1541 - val_loss: 0.3053\n",
      "Epoch 188/500\n",
      "112/112 [==============================] - 0s 913us/step - loss: 1.1482 - val_loss: 0.3268\n",
      "Epoch 189/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1579 - val_loss: 0.4144\n",
      "Epoch 190/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1620 - val_loss: 0.3357\n",
      "Epoch 191/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.1517 - val_loss: 0.3377\n",
      "Epoch 192/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1591 - val_loss: 0.3327\n",
      "Epoch 193/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1472 - val_loss: 0.3104\n",
      "Epoch 194/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1597 - val_loss: 0.3142\n",
      "Epoch 195/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1585 - val_loss: 0.3131\n",
      "Epoch 196/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1555 - val_loss: 0.3571\n",
      "Epoch 197/500\n",
      "112/112 [==============================] - 0s 987us/step - loss: 1.1609 - val_loss: 0.3480\n",
      "Epoch 198/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1559 - val_loss: 0.4182\n",
      "Epoch 199/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1591 - val_loss: 0.3775\n",
      "Epoch 200/500\n",
      "112/112 [==============================] - 0s 896us/step - loss: 1.1434 - val_loss: 0.3145\n",
      "Epoch 201/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1548 - val_loss: 0.3084\n",
      "Epoch 202/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1472 - val_loss: 0.4606\n",
      "Epoch 203/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1639 - val_loss: 0.3393\n",
      "Epoch 204/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1559 - val_loss: 0.4138\n",
      "Epoch 205/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1601 - val_loss: 0.3257\n",
      "Epoch 206/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1399 - val_loss: 0.3182\n",
      "Epoch 207/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1573 - val_loss: 0.3280\n",
      "Epoch 208/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1542 - val_loss: 0.3091\n",
      "Epoch 209/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1491 - val_loss: 0.3028\n",
      "Epoch 210/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1525 - val_loss: 0.3418\n",
      "Epoch 211/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1450 - val_loss: 0.3626\n",
      "Epoch 212/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1432 - val_loss: 0.3132\n",
      "Epoch 213/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1638 - val_loss: 0.3137\n",
      "Epoch 214/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1516 - val_loss: 0.3281\n",
      "Epoch 215/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1554 - val_loss: 0.3143\n",
      "Epoch 216/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1410 - val_loss: 0.4322\n",
      "Epoch 217/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1519 - val_loss: 0.4310\n",
      "Epoch 218/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1623 - val_loss: 0.3165\n",
      "Epoch 219/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1616 - val_loss: 0.3312\n",
      "Epoch 220/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1614 - val_loss: 0.3279\n",
      "Epoch 221/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1571 - val_loss: 0.3219\n",
      "Epoch 222/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1494 - val_loss: 0.3495\n",
      "Epoch 223/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1524 - val_loss: 0.3045\n",
      "Epoch 224/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1555 - val_loss: 0.2999\n",
      "Epoch 225/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1456 - val_loss: 0.4089\n",
      "Epoch 226/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1510 - val_loss: 0.4358\n",
      "Epoch 227/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1562 - val_loss: 0.3507\n",
      "Epoch 228/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1490 - val_loss: 0.3198\n",
      "Epoch 229/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1402 - val_loss: 0.3112\n",
      "Epoch 230/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1593 - val_loss: 0.3653\n",
      "Epoch 231/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1612 - val_loss: 0.3621\n",
      "Epoch 232/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1532 - val_loss: 0.4428\n",
      "Epoch 233/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1538 - val_loss: 0.3634\n",
      "Epoch 234/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1590 - val_loss: 0.3214\n",
      "Epoch 235/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1569 - val_loss: 0.3461\n",
      "Epoch 236/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.1576 - val_loss: 0.3358\n",
      "Epoch 237/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1472 - val_loss: 0.3288\n",
      "Epoch 238/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1587 - val_loss: 0.3316\n",
      "Epoch 239/500\n",
      "112/112 [==============================] - 0s 986us/step - loss: 1.1552 - val_loss: 0.3158\n",
      "Epoch 240/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1433 - val_loss: 0.3823\n",
      "Epoch 241/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1582 - val_loss: 0.3743\n",
      "Epoch 242/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1627 - val_loss: 0.3869\n",
      "Epoch 243/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1535 - val_loss: 0.3452\n",
      "Epoch 244/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1555 - val_loss: 0.3187\n",
      "Epoch 245/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1537 - val_loss: 0.3126\n",
      "Epoch 246/500\n",
      "112/112 [==============================] - 0s 937us/step - loss: 1.1498 - val_loss: 0.3434\n",
      "Epoch 247/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1590 - val_loss: 0.3141\n",
      "Epoch 248/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1467 - val_loss: 0.3126\n",
      "Epoch 249/500\n",
      "112/112 [==============================] - 0s 929us/step - loss: 1.1492 - val_loss: 0.3723\n",
      "Epoch 250/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1525 - val_loss: 0.3563\n",
      "Epoch 251/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1502 - val_loss: 0.3209\n",
      "Epoch 252/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1441 - val_loss: 0.3215\n",
      "Epoch 253/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1551 - val_loss: 0.3210\n",
      "Epoch 254/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1512 - val_loss: 0.5240\n",
      "Epoch 255/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1633 - val_loss: 0.3274\n",
      "Epoch 256/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1563 - val_loss: 0.3938\n",
      "Epoch 257/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1517 - val_loss: 0.5378\n",
      "Epoch 258/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1638 - val_loss: 0.3292\n",
      "Epoch 259/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1477 - val_loss: 0.3095\n",
      "Epoch 260/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1590 - val_loss: 0.3156\n",
      "Epoch 261/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1645 - val_loss: 0.3358\n",
      "Epoch 262/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1615 - val_loss: 0.3442\n",
      "Epoch 263/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1479 - val_loss: 0.3189\n",
      "Epoch 264/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1492 - val_loss: 0.3391\n",
      "Epoch 265/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1553 - val_loss: 0.3286\n",
      "Epoch 266/500\n",
      "112/112 [==============================] - 0s 956us/step - loss: 1.1571 - val_loss: 0.4034\n",
      "Epoch 267/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1562 - val_loss: 0.3371\n",
      "Epoch 268/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1565 - val_loss: 0.3333\n",
      "Epoch 269/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1575 - val_loss: 0.3231\n",
      "Epoch 270/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1464 - val_loss: 0.3626\n",
      "Epoch 271/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1474 - val_loss: 0.3088\n",
      "Epoch 272/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1592 - val_loss: 0.3246\n",
      "Epoch 273/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1547 - val_loss: 0.3361\n",
      "Epoch 274/500\n",
      "112/112 [==============================] - 0s 937us/step - loss: 1.1579 - val_loss: 0.3223\n",
      "Epoch 275/500\n",
      "112/112 [==============================] - 0s 852us/step - loss: 1.1471 - val_loss: 0.3416\n",
      "Epoch 276/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1484 - val_loss: 0.4213\n",
      "Epoch 277/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1607 - val_loss: 0.3482\n",
      "Epoch 278/500\n",
      "112/112 [==============================] - 0s 896us/step - loss: 1.1559 - val_loss: 0.4718\n",
      "Epoch 279/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1400 - val_loss: 0.3103\n",
      "Epoch 280/500\n",
      "112/112 [==============================] - 0s 990us/step - loss: 1.1559 - val_loss: 0.3744\n",
      "Epoch 281/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1389 - val_loss: 0.3034\n",
      "Epoch 282/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1622 - val_loss: 0.3453\n",
      "Epoch 283/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1541 - val_loss: 0.3948\n",
      "Epoch 284/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1635 - val_loss: 0.3198\n",
      "Epoch 285/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1539 - val_loss: 0.4240\n",
      "Epoch 286/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1581 - val_loss: 0.3332\n",
      "Epoch 287/500\n",
      "112/112 [==============================] - 0s 850us/step - loss: 1.1603 - val_loss: 0.3122\n",
      "Epoch 288/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1536 - val_loss: 0.3035\n",
      "Epoch 289/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1507 - val_loss: 0.3019\n",
      "Epoch 290/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1639 - val_loss: 0.3127\n",
      "Epoch 291/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1607 - val_loss: 0.3288\n",
      "Epoch 292/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1571 - val_loss: 0.3378\n",
      "Epoch 293/500\n",
      "112/112 [==============================] - 0s 888us/step - loss: 1.1633 - val_loss: 0.3303\n",
      "Epoch 294/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1572 - val_loss: 0.3370\n",
      "Epoch 295/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1548 - val_loss: 0.3593\n",
      "Epoch 296/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1431 - val_loss: 0.3337\n",
      "Epoch 297/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1591 - val_loss: 0.3226\n",
      "Epoch 298/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1604 - val_loss: 0.3236\n",
      "Epoch 299/500\n",
      "112/112 [==============================] - 0s 952us/step - loss: 1.1425 - val_loss: 0.3866\n",
      "Epoch 300/500\n",
      "112/112 [==============================] - 0s 989us/step - loss: 1.1561 - val_loss: 0.3576\n",
      "Epoch 301/500\n",
      "112/112 [==============================] - 0s 974us/step - loss: 1.1567 - val_loss: 0.3712\n",
      "Epoch 302/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1595 - val_loss: 0.3564\n",
      "Epoch 303/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1458 - val_loss: 0.3214\n",
      "Epoch 304/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1616 - val_loss: 0.3291\n",
      "Epoch 305/500\n",
      "112/112 [==============================] - 0s 938us/step - loss: 1.1547 - val_loss: 0.3290\n",
      "Epoch 306/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1584 - val_loss: 0.3254\n",
      "Epoch 307/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1388 - val_loss: 0.4039\n",
      "Epoch 308/500\n",
      "112/112 [==============================] - 0s 938us/step - loss: 1.1634 - val_loss: 0.3567\n",
      "Epoch 309/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1540 - val_loss: 0.3244\n",
      "Epoch 310/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1581 - val_loss: 0.3554\n",
      "Epoch 311/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1592 - val_loss: 0.3524\n",
      "Epoch 312/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1311 - val_loss: 0.3244\n",
      "Epoch 313/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1462 - val_loss: 0.3738\n",
      "Epoch 314/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1500 - val_loss: 0.3436\n",
      "Epoch 315/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1568 - val_loss: 0.3452\n",
      "Epoch 316/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1483 - val_loss: 0.3302\n",
      "Epoch 317/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1520 - val_loss: 0.4274\n",
      "Epoch 318/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1615 - val_loss: 0.3318\n",
      "Epoch 319/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1427 - val_loss: 0.3770\n",
      "Epoch 320/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1603 - val_loss: 0.3564\n",
      "Epoch 321/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1571 - val_loss: 0.4064\n",
      "Epoch 322/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1537 - val_loss: 0.4037\n",
      "Epoch 323/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1622 - val_loss: 0.3768\n",
      "Epoch 324/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.1405 - val_loss: 0.3223\n",
      "Epoch 325/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1579 - val_loss: 0.3238\n",
      "Epoch 326/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1523 - val_loss: 0.4510\n",
      "Epoch 327/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1619 - val_loss: 0.3369\n",
      "Epoch 328/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1525 - val_loss: 0.3214\n",
      "Epoch 329/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1566 - val_loss: 0.3487\n",
      "Epoch 330/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1597 - val_loss: 0.3341\n",
      "Epoch 331/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1512 - val_loss: 0.3513\n",
      "Epoch 332/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1507 - val_loss: 0.3311\n",
      "Epoch 333/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1526 - val_loss: 0.3850\n",
      "Epoch 334/500\n",
      "112/112 [==============================] - 0s 938us/step - loss: 1.1584 - val_loss: 0.3314\n",
      "Epoch 335/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1585 - val_loss: 0.3705\n",
      "Epoch 336/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1588 - val_loss: 0.3346\n",
      "Epoch 337/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1596 - val_loss: 0.3353\n",
      "Epoch 338/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1525 - val_loss: 0.3208\n",
      "Epoch 339/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1494 - val_loss: 0.3049\n",
      "Epoch 340/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1374 - val_loss: 0.5239\n",
      "Epoch 341/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1645 - val_loss: 0.3524\n",
      "Epoch 342/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1481 - val_loss: 0.3224\n",
      "Epoch 343/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1594 - val_loss: 0.3211\n",
      "Epoch 344/500\n",
      "112/112 [==============================] - 0s 894us/step - loss: 1.1520 - val_loss: 0.3329\n",
      "Epoch 345/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1583 - val_loss: 0.3421\n",
      "Epoch 346/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1579 - val_loss: 0.3238\n",
      "Epoch 347/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1506 - val_loss: 0.3799\n",
      "Epoch 348/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1545 - val_loss: 0.3506\n",
      "Epoch 349/500\n",
      "112/112 [==============================] - 0s 986us/step - loss: 1.1459 - val_loss: 0.3189\n",
      "Epoch 350/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1441 - val_loss: 0.3415\n",
      "Epoch 351/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1530 - val_loss: 0.3105\n",
      "Epoch 352/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1505 - val_loss: 0.3145\n",
      "Epoch 353/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1478 - val_loss: 0.3151\n",
      "Epoch 354/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1582 - val_loss: 0.3283\n",
      "Epoch 355/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1560 - val_loss: 0.3292\n",
      "Epoch 356/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1489 - val_loss: 0.4911\n",
      "Epoch 357/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1528 - val_loss: 0.3047\n",
      "Epoch 358/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1604 - val_loss: 0.3119\n",
      "Epoch 359/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1557 - val_loss: 0.3155\n",
      "Epoch 360/500\n",
      "112/112 [==============================] - 0s 966us/step - loss: 1.1595 - val_loss: 0.3538\n",
      "Epoch 361/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1551 - val_loss: 0.3226\n",
      "Epoch 362/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1570 - val_loss: 0.3410\n",
      "Epoch 363/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1498 - val_loss: 0.3262\n",
      "Epoch 364/500\n",
      "112/112 [==============================] - 0s 939us/step - loss: 1.1608 - val_loss: 0.3244\n",
      "Epoch 365/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1498 - val_loss: 0.3858\n",
      "Epoch 366/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1612 - val_loss: 0.3793\n",
      "Epoch 367/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1512 - val_loss: 0.4029\n",
      "Epoch 368/500\n",
      "112/112 [==============================] - 0s 985us/step - loss: 1.1575 - val_loss: 0.3466\n",
      "Epoch 369/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1591 - val_loss: 0.3562\n",
      "Epoch 370/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1585 - val_loss: 0.3479\n",
      "Epoch 371/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1379 - val_loss: 0.3930\n",
      "Epoch 372/500\n",
      "112/112 [==============================] - 0s 985us/step - loss: 1.1602 - val_loss: 0.3498\n",
      "Epoch 373/500\n",
      "112/112 [==============================] - 0s 892us/step - loss: 1.1424 - val_loss: 0.5057\n",
      "Epoch 374/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1651 - val_loss: 0.3420\n",
      "Epoch 375/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1536 - val_loss: 0.4336\n",
      "Epoch 376/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1532 - val_loss: 0.3484\n",
      "Epoch 377/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1423 - val_loss: 0.3859\n",
      "Epoch 378/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1518 - val_loss: 0.4684\n",
      "Epoch 379/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1560 - val_loss: 0.3341\n",
      "Epoch 380/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1598 - val_loss: 0.3212\n",
      "Epoch 381/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1489 - val_loss: 0.3379\n",
      "Epoch 382/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1570 - val_loss: 0.3171\n",
      "Epoch 383/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1503 - val_loss: 0.4767\n",
      "Epoch 384/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1488 - val_loss: 0.3407\n",
      "Epoch 385/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1450 - val_loss: 0.3057\n",
      "Epoch 386/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1625 - val_loss: 0.3271\n",
      "Epoch 387/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1541 - val_loss: 0.3341\n",
      "Epoch 388/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1596 - val_loss: 0.3652\n",
      "Epoch 389/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1596 - val_loss: 0.3318\n",
      "Epoch 390/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1598 - val_loss: 0.3496\n",
      "Epoch 391/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1561 - val_loss: 0.3366\n",
      "Epoch 392/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1598 - val_loss: 0.3461\n",
      "Epoch 393/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1542 - val_loss: 0.3147\n",
      "Epoch 394/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1565 - val_loss: 0.3355\n",
      "Epoch 395/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1486 - val_loss: 0.3170\n",
      "Epoch 396/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1476 - val_loss: 0.5140\n",
      "Epoch 397/500\n",
      "112/112 [==============================] - 0s 958us/step - loss: 1.1535 - val_loss: 0.4222\n",
      "Epoch 398/500\n",
      "112/112 [==============================] - 0s 936us/step - loss: 1.1604 - val_loss: 0.3796\n",
      "Epoch 399/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1506 - val_loss: 0.3253\n",
      "Epoch 400/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1633 - val_loss: 0.3535\n",
      "Epoch 401/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1435 - val_loss: 0.4127\n",
      "Epoch 402/500\n",
      "112/112 [==============================] - 0s 993us/step - loss: 1.1579 - val_loss: 0.3830\n",
      "Epoch 403/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1493 - val_loss: 0.3461\n",
      "Epoch 404/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1625 - val_loss: 0.3304\n",
      "Epoch 405/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1516 - val_loss: 0.3643\n",
      "Epoch 406/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1616 - val_loss: 0.3359\n",
      "Epoch 407/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1602 - val_loss: 0.3517\n",
      "Epoch 408/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1439 - val_loss: 0.3060\n",
      "Epoch 409/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1563 - val_loss: 0.3258\n",
      "Epoch 410/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1548 - val_loss: 0.3260\n",
      "Epoch 411/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1375 - val_loss: 0.5540\n",
      "Epoch 412/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1527 - val_loss: 0.3311\n",
      "Epoch 413/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1554 - val_loss: 0.3329\n",
      "Epoch 414/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1593 - val_loss: 0.3296\n",
      "Epoch 415/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1516 - val_loss: 0.3481\n",
      "Epoch 416/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1557 - val_loss: 0.3244\n",
      "Epoch 417/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1566 - val_loss: 0.3367\n",
      "Epoch 418/500\n",
      "112/112 [==============================] - 0s 952us/step - loss: 1.1506 - val_loss: 0.3932\n",
      "Epoch 419/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1517 - val_loss: 0.3250\n",
      "Epoch 420/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1522 - val_loss: 0.3204\n",
      "Epoch 421/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1483 - val_loss: 0.3087\n",
      "Epoch 422/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1559 - val_loss: 0.3370\n",
      "Epoch 423/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1570 - val_loss: 0.3635\n",
      "Epoch 424/500\n",
      "112/112 [==============================] - 0s 971us/step - loss: 1.1451 - val_loss: 0.3641\n",
      "Epoch 425/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1435 - val_loss: 0.5964\n",
      "Epoch 426/500\n",
      "112/112 [==============================] - 0s 967us/step - loss: 1.1591 - val_loss: 0.4483\n",
      "Epoch 427/500\n",
      "112/112 [==============================] - 0s 997us/step - loss: 1.1574 - val_loss: 0.3424\n",
      "Epoch 428/500\n",
      "112/112 [==============================] - 0s 927us/step - loss: 1.1623 - val_loss: 0.3596\n",
      "Epoch 429/500\n",
      "112/112 [==============================] - 0s 986us/step - loss: 1.1618 - val_loss: 0.3441\n",
      "Epoch 430/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1586 - val_loss: 0.3614\n",
      "Epoch 431/500\n",
      "112/112 [==============================] - 0s 954us/step - loss: 1.1587 - val_loss: 0.3299\n",
      "Epoch 432/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1582 - val_loss: 0.3626\n",
      "Epoch 433/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1600 - val_loss: 0.3573\n",
      "Epoch 434/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1541 - val_loss: 0.3432\n",
      "Epoch 435/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1564 - val_loss: 0.3384\n",
      "Epoch 436/500\n",
      "112/112 [==============================] - 0s 940us/step - loss: 1.1480 - val_loss: 0.4549\n",
      "Epoch 437/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1582 - val_loss: 0.3530\n",
      "Epoch 438/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1322 - val_loss: 0.4652\n",
      "Epoch 439/500\n",
      "112/112 [==============================] - 0s 893us/step - loss: 1.1724 - val_loss: 0.3531\n",
      "Epoch 440/500\n",
      "112/112 [==============================] - 0s 978us/step - loss: 1.1530 - val_loss: 0.4267\n",
      "Epoch 441/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1575 - val_loss: 0.3372\n",
      "Epoch 442/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1361 - val_loss: 0.3232\n",
      "Epoch 443/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1629 - val_loss: 0.3427\n",
      "Epoch 444/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 1.1572 - val_loss: 0.4106\n",
      "Epoch 445/500\n",
      "112/112 [==============================] - 0s 985us/step - loss: 1.1538 - val_loss: 0.4123\n",
      "Epoch 446/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1601 - val_loss: 0.3441\n",
      "Epoch 447/500\n",
      "112/112 [==============================] - 0s 937us/step - loss: 1.1486 - val_loss: 0.4869\n",
      "Epoch 448/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1598 - val_loss: 0.3293\n",
      "Epoch 449/500\n",
      "112/112 [==============================] - 0s 978us/step - loss: 1.1438 - val_loss: 0.3359\n",
      "Epoch 450/500\n",
      "112/112 [==============================] - 0s 985us/step - loss: 1.1552 - val_loss: 0.4135\n",
      "Epoch 451/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1607 - val_loss: 0.3378\n",
      "Epoch 452/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1449 - val_loss: 0.3178\n",
      "Epoch 453/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1584 - val_loss: 0.3440\n",
      "Epoch 454/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1563 - val_loss: 0.3279\n",
      "Epoch 455/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1459 - val_loss: 0.4250\n",
      "Epoch 456/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1526 - val_loss: 0.3387\n",
      "Epoch 457/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1599 - val_loss: 0.3253\n",
      "Epoch 458/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1548 - val_loss: 0.3403\n",
      "Epoch 459/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1480 - val_loss: 0.5003\n",
      "Epoch 460/500\n",
      "112/112 [==============================] - 0s 983us/step - loss: 1.1585 - val_loss: 0.4460\n",
      "Epoch 461/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1599 - val_loss: 0.3162\n",
      "Epoch 462/500\n",
      "112/112 [==============================] - 0s 982us/step - loss: 1.1533 - val_loss: 0.3214\n",
      "Epoch 463/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1548 - val_loss: 0.3719\n",
      "Epoch 464/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1571 - val_loss: 0.3319\n",
      "Epoch 465/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1506 - val_loss: 0.3751\n",
      "Epoch 466/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1446 - val_loss: 0.3203\n",
      "Epoch 467/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1409 - val_loss: 0.3815\n",
      "Epoch 468/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1481 - val_loss: 0.3722\n",
      "Epoch 469/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1522 - val_loss: 0.3300\n",
      "Epoch 470/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1550 - val_loss: 0.3612\n",
      "Epoch 471/500\n",
      "112/112 [==============================] - 0s 985us/step - loss: 1.1552 - val_loss: 0.3249\n",
      "Epoch 472/500\n",
      "112/112 [==============================] - 0s 977us/step - loss: 1.1595 - val_loss: 0.3236\n",
      "Epoch 473/500\n",
      "112/112 [==============================] - 0s 984us/step - loss: 1.1517 - val_loss: 0.3243\n",
      "Epoch 474/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 1.1482 - val_loss: 0.4034\n",
      "Epoch 475/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1532 - val_loss: 0.4745\n",
      "Epoch 476/500\n",
      "112/112 [==============================] - 0s 985us/step - loss: 1.1559 - val_loss: 0.3767\n",
      "Epoch 477/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1555 - val_loss: 0.3225\n",
      "Epoch 478/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1561 - val_loss: 0.3791\n",
      "Epoch 479/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1519 - val_loss: 0.3440\n",
      "Epoch 480/500\n",
      "112/112 [==============================] - 0s 883us/step - loss: 1.1498 - val_loss: 0.3665\n",
      "Epoch 481/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1453 - val_loss: 0.3307\n",
      "Epoch 482/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1491 - val_loss: 0.5198\n",
      "Epoch 483/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1588 - val_loss: 0.3332\n",
      "Epoch 484/500\n",
      "112/112 [==============================] - 0s 964us/step - loss: 1.1532 - val_loss: 0.3248\n",
      "Epoch 485/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1595 - val_loss: 0.3279\n",
      "Epoch 486/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1598 - val_loss: 0.3522\n",
      "Epoch 487/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1536 - val_loss: 0.3186\n",
      "Epoch 488/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1497 - val_loss: 0.4164\n",
      "Epoch 489/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1561 - val_loss: 0.4259\n",
      "Epoch 490/500\n",
      "112/112 [==============================] - 0s 976us/step - loss: 1.1566 - val_loss: 0.3191\n",
      "Epoch 491/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1668 - val_loss: 0.3446\n",
      "Epoch 492/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1502 - val_loss: 0.5162\n",
      "Epoch 493/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1412 - val_loss: 0.7090\n",
      "Epoch 494/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1722 - val_loss: 0.3317\n",
      "Epoch 495/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1565 - val_loss: 0.3347\n",
      "Epoch 496/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1506 - val_loss: 0.3181\n",
      "Epoch 497/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1583 - val_loss: 0.3235\n",
      "Epoch 498/500\n",
      "112/112 [==============================] - 0s 981us/step - loss: 1.1600 - val_loss: 0.3343\n",
      "Epoch 499/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1588 - val_loss: 0.3267\n",
      "Epoch 500/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 1.1548 - val_loss: 0.3560\n",
      "70/70 [==============================] - 0s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9581789987427848"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "requiredData = newDataFrame.loc[:, newDataFrame.columns.intersection([\"DaysTotal_Days\" ,\"new_deaths\", \"hospital_beds_per_thousand\",\"diabetes_prevalence\",\"median_age\"])]\n",
    "# requiredData = newDataFrame.drop(columns=['AverageInfectionRate'])\n",
    "# requiredData = requiredData.drop(columns=['AverageDeathRate'])\n",
    "\n",
    "\n",
    "X = requiredData.drop(columns=['new_deaths'])\n",
    "y = requiredData.loc[:, requiredData.columns.intersection([\"new_deaths\"])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "\n",
    "mean_train = X_train.mean(axis = 0)\n",
    "std_dev_train = X_train.std(axis = 0)\n",
    "std_dev_test = X_test.std(axis = 0)\n",
    "\n",
    "X_train = (X_train - mean_train)/std_dev_train\n",
    "X_test = (X_test - mean_train)/std_dev_test\n",
    "\n",
    "\n",
    "mean_train = y_train.mean(axis = 0)\n",
    "std_dev_train = y_train.std(axis = 0)\n",
    "std_dev_test = y_test.std(axis = 0)\n",
    "\n",
    "y_train = (y_train - mean_train)/std_dev_train\n",
    "y_test = (y_test  - mean_train)/std_dev_test\n",
    "\n",
    "# X_train[\"location\"] = ohe.transform(categories).todense() \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "in_shape = X_train.iloc[0].shape\n",
    "model = Sequential()\n",
    "model.add(Dense(4, activation = 'relu', input_shape = in_shape))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "# model.add(Dense(12, activation = 'relu', input_shape = in_shape))\n",
    "# model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 500, batch_size = 1, validation_split = 0.2, verbose = 1)\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_24: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-f581ace49917>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;31m#model.add(Dropout(0.2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_24: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "Pakistan = dataFrame[dataFrame.location == \"Pakistan\"]\n",
    "Pakistan = Pakistan.loc[:, Pakistan.columns.intersection([\"date\" ,\"new_deaths\",\"stringency_index\"])]\n",
    "Pakistan.set_index('date', inplace=True)\n",
    "Pakistan\n",
    "\n",
    "X = Pakistan.drop(columns=['new_deaths'])\n",
    "y = Pakistan.loc[:, Pakistan.columns.intersection([\"new_deaths\"])]\n",
    "\n",
    "x=len(Pakistan)-5\n",
    "train=Pakistan.iloc[:x]\n",
    "test = Pakistan.iloc[x:]\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Dense, Embedding, LSTM, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "scaled_train = scaler.transform(train)#and divide every point by max value\n",
    "scaled_test = scaler.transform(test)\n",
    "\n",
    "\n",
    "n_input = 5  ## number of steps\n",
    "n_features = 2 ## number of features you want to predict (for univariate time series n_features=1)\n",
    "in_shape = X_train.iloc[0].shape\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(150,activation=\"relu\",input_shape=(n_input,n_features)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(75, activation='relu'))\n",
    "model.add(Dense(units=2))\n",
    "#model.add(Activation('softmax'))\n",
    "#model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=200, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x201ea4ccc08>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(model.history.history).plot(title=\"loss vs epochs curve\")\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.33)\n",
    "\n",
    "# # X_train = PakistanV1[\"Days\"][0:110]\n",
    "# # X_test = PakistanV1[\"Days\"][111:]\n",
    "# # y_train = PakistanV1[\"total_deaths\"][0:110]\n",
    "# # y_test = PakistanV1[\"total_deaths\"][111:]\n",
    "\n",
    "# X_train, y_train\n",
    "\n",
    "\n",
    "# # X_train[\"location\"] = ohe.transform(categories).todense() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                36        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "in_shape = X_train.iloc[0].shape\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation = 'relu', input_shape = in_shape))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = X_train.mean(axis = 0)\n",
    "std_dev_train = X_train.std(axis = 0)\n",
    "\n",
    "X_train = (X_train - mean_train)/std_dev_train\n",
    "X_test = (X_test - mean_train)/std_dev_train\n",
    "\n",
    "\n",
    "mean_train = y_train.mean(axis = 0)\n",
    "std_dev_train = y_train.std(axis = 0)\n",
    "\n",
    "y_train = (y_train - mean_train)/std_dev_train\n",
    "y_test = (y_test  - mean_train)/std_dev_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kuwait</th>\n",
       "      <td>0.024153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ecuador</th>\n",
       "      <td>0.083871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hong Kong</th>\n",
       "      <td>-0.176763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swaziland</th>\n",
       "      <td>-0.173936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romania</th>\n",
       "      <td>-0.054908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gambia</th>\n",
       "      <td>-0.176578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saint Lucia</th>\n",
       "      <td>-0.176659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Armenia</th>\n",
       "      <td>-0.081687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gabon</th>\n",
       "      <td>-0.154398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ukraine</th>\n",
       "      <td>-0.000213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             new_cases\n",
       "location              \n",
       "Kuwait        0.024153\n",
       "Ecuador       0.083871\n",
       "Hong Kong    -0.176763\n",
       "Swaziland    -0.173936\n",
       "Romania      -0.054908\n",
       "...                ...\n",
       "Gambia       -0.176578\n",
       "Saint Lucia  -0.176659\n",
       "Armenia      -0.081687\n",
       "Gabon        -0.154398\n",
       "Ukraine      -0.000213\n",
       "\n",
       "[140 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112 samples, validate on 28 samples\n",
      "Epoch 1/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 4.8403\n",
      "Epoch 2/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0461 - val_loss: 4.7122\n",
      "Epoch 3/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 4.6502\n",
      "Epoch 4/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 4.7767\n",
      "Epoch 5/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 4.7284\n",
      "Epoch 6/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 4.7783\n",
      "Epoch 7/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 4.7351\n",
      "Epoch 8/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 4.7852\n",
      "Epoch 9/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 4.7328\n",
      "Epoch 10/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 4.6467\n",
      "Epoch 11/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 4.8412\n",
      "Epoch 12/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 4.7201\n",
      "Epoch 13/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 4.7577\n",
      "Epoch 14/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0375 - val_loss: 4.7009\n",
      "Epoch 15/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 4.6674\n",
      "Epoch 16/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 4.8048\n",
      "Epoch 17/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0380 - val_loss: 4.6504\n",
      "Epoch 18/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 4.8177\n",
      "Epoch 19/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 4.8310\n",
      "Epoch 20/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0383 - val_loss: 4.6654\n",
      "Epoch 21/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 4.6462\n",
      "Epoch 22/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 4.8258\n",
      "Epoch 23/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.5982\n",
      "Epoch 24/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 4.8019\n",
      "Epoch 25/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 4.7426\n",
      "Epoch 26/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 4.8257\n",
      "Epoch 27/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 4.7901\n",
      "Epoch 28/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 4.7968\n",
      "Epoch 29/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 4.7565\n",
      "Epoch 30/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 4.8168\n",
      "Epoch 31/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0382 - val_loss: 4.6970\n",
      "Epoch 32/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.8280\n",
      "Epoch 33/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 4.6893\n",
      "Epoch 34/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 4.7133\n",
      "Epoch 35/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 4.8153\n",
      "Epoch 36/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 4.8003\n",
      "Epoch 37/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 4.7269\n",
      "Epoch 38/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.6509\n",
      "Epoch 39/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 4.8132\n",
      "Epoch 40/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 4.8123\n",
      "Epoch 41/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 4.8331\n",
      "Epoch 42/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 4.8020\n",
      "Epoch 43/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 4.5939\n",
      "Epoch 44/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 4.8569\n",
      "Epoch 45/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 4.6871\n",
      "Epoch 46/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0382 - val_loss: 4.6580\n",
      "Epoch 47/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.8224\n",
      "Epoch 48/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 4.7465\n",
      "Epoch 49/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 4.8490\n",
      "Epoch 50/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.8283\n",
      "Epoch 51/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 4.8304\n",
      "Epoch 52/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7590\n",
      "Epoch 53/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 4.8261\n",
      "Epoch 54/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7609\n",
      "Epoch 55/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 4.7685\n",
      "Epoch 56/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 4.7642\n",
      "Epoch 57/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 4.7562\n",
      "Epoch 58/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 4.8233\n",
      "Epoch 59/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 4.7893\n",
      "Epoch 60/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 4.7627\n",
      "Epoch 61/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0382 - val_loss: 4.7693\n",
      "Epoch 62/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.8286\n",
      "Epoch 63/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 4.8303\n",
      "Epoch 64/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.6106\n",
      "Epoch 65/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 4.7923\n",
      "Epoch 66/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.8208\n",
      "Epoch 67/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 4.8221\n",
      "Epoch 68/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 4.6928\n",
      "Epoch 69/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 4.7660\n",
      "Epoch 70/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7691\n",
      "Epoch 71/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0382 - val_loss: 4.7532\n",
      "Epoch 72/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.8304\n",
      "Epoch 73/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 4.7124\n",
      "Epoch 74/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 4.8142\n",
      "Epoch 75/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0382 - val_loss: 4.7914\n",
      "Epoch 76/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 4.7981\n",
      "Epoch 77/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 4.7342\n",
      "Epoch 78/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 4.7553\n",
      "Epoch 79/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 4.7351\n",
      "Epoch 80/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 4.8072\n",
      "Epoch 81/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 4.6857\n",
      "Epoch 82/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.8440\n",
      "Epoch 83/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 4.7774\n",
      "Epoch 84/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.6539\n",
      "Epoch 85/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7928\n",
      "Epoch 86/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.7497\n",
      "Epoch 87/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 4.8001\n",
      "Epoch 88/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 4.7714\n",
      "Epoch 89/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.8262\n",
      "Epoch 90/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.6717\n",
      "Epoch 91/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.8511\n",
      "Epoch 92/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 4.7634\n",
      "Epoch 93/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 4.7287\n",
      "Epoch 94/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 4.8337\n",
      "Epoch 95/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.6242\n",
      "Epoch 96/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 4.8239\n",
      "Epoch 97/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.7173\n",
      "Epoch 98/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 4.7132\n",
      "Epoch 99/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 4.8059\n",
      "Epoch 100/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.8238\n",
      "Epoch 101/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7535\n",
      "Epoch 102/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8721\n",
      "Epoch 103/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7707\n",
      "Epoch 104/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7257\n",
      "Epoch 105/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 4.7876\n",
      "Epoch 106/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.7488\n",
      "Epoch 107/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.8358\n",
      "Epoch 108/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 4.7906\n",
      "Epoch 109/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 4.7792\n",
      "Epoch 110/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7602\n",
      "Epoch 111/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 4.7838\n",
      "Epoch 112/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 4.7411\n",
      "Epoch 113/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 4.8082\n",
      "Epoch 114/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.6814\n",
      "Epoch 115/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 4.7776\n",
      "Epoch 116/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.8131\n",
      "Epoch 117/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7320\n",
      "Epoch 118/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 4.7219\n",
      "Epoch 119/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 4.7643\n",
      "Epoch 120/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 4.7927\n",
      "Epoch 121/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.8067\n",
      "Epoch 122/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.041 - 0s 2ms/step - loss: 0.0373 - val_loss: 4.7866\n",
      "Epoch 123/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7823\n",
      "Epoch 124/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.8201\n",
      "Epoch 125/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 4.8111\n",
      "Epoch 126/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7155\n",
      "Epoch 127/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7695\n",
      "Epoch 128/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7987\n",
      "Epoch 129/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.6630\n",
      "Epoch 130/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 4.7087\n",
      "Epoch 131/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 4.7196\n",
      "Epoch 132/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.8520\n",
      "Epoch 133/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 4.7507\n",
      "Epoch 134/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7873\n",
      "Epoch 135/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 4.7927\n",
      "Epoch 136/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 4.8059\n",
      "Epoch 137/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.6789\n",
      "Epoch 138/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 4.7488\n",
      "Epoch 139/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.7093\n",
      "Epoch 140/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.8011\n",
      "Epoch 141/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.8069\n",
      "Epoch 142/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7533\n",
      "Epoch 143/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7790\n",
      "Epoch 144/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 4.7439\n",
      "Epoch 145/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.8059\n",
      "Epoch 146/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.7950\n",
      "Epoch 147/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.7975\n",
      "Epoch 148/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7781\n",
      "Epoch 149/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 4.7375\n",
      "Epoch 150/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7421\n",
      "Epoch 151/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.8493\n",
      "Epoch 152/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 4.7962\n",
      "Epoch 153/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7208\n",
      "Epoch 154/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.8201\n",
      "Epoch 155/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7431\n",
      "Epoch 156/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7956\n",
      "Epoch 157/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.6915\n",
      "Epoch 158/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.8150\n",
      "Epoch 159/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.8091\n",
      "Epoch 160/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7510\n",
      "Epoch 161/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.8093\n",
      "Epoch 162/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 4.7436\n",
      "Epoch 163/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7540\n",
      "Epoch 164/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8404\n",
      "Epoch 165/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 4.8051\n",
      "Epoch 166/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7420\n",
      "Epoch 167/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 4.8060\n",
      "Epoch 168/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 4.7986\n",
      "Epoch 169/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7610\n",
      "Epoch 170/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 4.8483\n",
      "Epoch 171/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 4.8113\n",
      "Epoch 172/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 4.7992\n",
      "Epoch 173/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 4.7781\n",
      "Epoch 174/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 4.7357\n",
      "Epoch 175/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 4.8170\n",
      "Epoch 176/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0359 - val_loss: 4.7956\n",
      "Epoch 177/500\n",
      "112/112 [==============================] - 0s 965us/step - loss: 0.0357 - val_loss: 4.7199\n",
      "Epoch 178/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0368 - val_loss: 4.8089\n",
      "Epoch 179/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7271\n",
      "Epoch 180/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.8134\n",
      "Epoch 181/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.7533\n",
      "Epoch 182/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.7517\n",
      "Epoch 183/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.7595\n",
      "Epoch 184/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0369 - val_loss: 4.7578\n",
      "Epoch 185/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.8001\n",
      "Epoch 186/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.8324\n",
      "Epoch 187/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 4.8005\n",
      "Epoch 188/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7778\n",
      "Epoch 189/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.7822\n",
      "Epoch 190/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.7493\n",
      "Epoch 191/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.8009\n",
      "Epoch 192/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.8170\n",
      "Epoch 193/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.7033\n",
      "Epoch 194/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7399\n",
      "Epoch 195/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.8180\n",
      "Epoch 196/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7580\n",
      "Epoch 197/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.8352\n",
      "Epoch 198/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 4.8320\n",
      "Epoch 199/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.8032\n",
      "Epoch 200/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7587\n",
      "Epoch 201/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.7466\n",
      "Epoch 202/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.6918\n",
      "Epoch 203/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7912\n",
      "Epoch 204/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.8244\n",
      "Epoch 205/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.7512\n",
      "Epoch 206/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.7490\n",
      "Epoch 207/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.7363\n",
      "Epoch 208/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.7571\n",
      "Epoch 209/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 4.7979\n",
      "Epoch 210/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.7465\n",
      "Epoch 211/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7919\n",
      "Epoch 212/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.7329\n",
      "Epoch 213/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7652\n",
      "Epoch 214/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7313\n",
      "Epoch 215/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0360 - val_loss: 4.8139\n",
      "Epoch 216/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7254\n",
      "Epoch 217/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.8226\n",
      "Epoch 218/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 4.7266\n",
      "Epoch 219/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7354\n",
      "Epoch 220/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 4.7871\n",
      "Epoch 221/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.7767\n",
      "Epoch 222/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 4.7467\n",
      "Epoch 223/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 4.7503\n",
      "Epoch 224/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 4.7838\n",
      "Epoch 225/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8078\n",
      "Epoch 226/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 4.7946\n",
      "Epoch 227/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8181\n",
      "Epoch 228/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7870\n",
      "Epoch 229/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 4.8029\n",
      "Epoch 230/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.8352\n",
      "Epoch 231/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7823\n",
      "Epoch 232/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.7853\n",
      "Epoch 233/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.8187\n",
      "Epoch 234/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8161\n",
      "Epoch 235/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 4.7436\n",
      "Epoch 236/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.8124\n",
      "Epoch 237/500\n",
      "112/112 [==============================] - 0s 979us/step - loss: 0.0356 - val_loss: 4.7834\n",
      "Epoch 238/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.8261\n",
      "Epoch 239/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 4.7744\n",
      "Epoch 240/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7707\n",
      "Epoch 241/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0362 - val_loss: 4.7791\n",
      "Epoch 242/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7776\n",
      "Epoch 243/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 4.7367\n",
      "Epoch 244/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.8095\n",
      "Epoch 245/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.7668\n",
      "Epoch 246/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.7481\n",
      "Epoch 247/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.8229\n",
      "Epoch 248/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.8225\n",
      "Epoch 249/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8245\n",
      "Epoch 250/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7203\n",
      "Epoch 251/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.7399\n",
      "Epoch 252/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7580\n",
      "Epoch 253/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7321\n",
      "Epoch 254/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.7555\n",
      "Epoch 255/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 4.6854\n",
      "Epoch 256/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.8089\n",
      "Epoch 257/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.8375\n",
      "Epoch 258/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.8393\n",
      "Epoch 259/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7790\n",
      "Epoch 260/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7969\n",
      "Epoch 261/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 4.7783\n",
      "Epoch 262/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7929\n",
      "Epoch 263/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.041 - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7976\n",
      "Epoch 264/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 4.8300\n",
      "Epoch 265/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7506\n",
      "Epoch 266/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 4.7692\n",
      "Epoch 267/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0360 - val_loss: 4.8056\n",
      "Epoch 268/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 4.7984\n",
      "Epoch 269/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.8111\n",
      "Epoch 270/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.7253\n",
      "Epoch 271/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0362 - val_loss: 4.7535\n",
      "Epoch 272/500\n",
      "112/112 [==============================] - 0s 997us/step - loss: 0.0346 - val_loss: 4.8266\n",
      "Epoch 273/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0359 - val_loss: 4.8313\n",
      "Epoch 274/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 4.7692\n",
      "Epoch 275/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7896\n",
      "Epoch 276/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8082\n",
      "Epoch 277/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7919\n",
      "Epoch 278/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.8222\n",
      "Epoch 279/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.7589\n",
      "Epoch 280/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.8305\n",
      "Epoch 281/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.7680\n",
      "Epoch 282/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7814\n",
      "Epoch 283/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7881\n",
      "Epoch 284/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7230\n",
      "Epoch 285/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.8117\n",
      "Epoch 286/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.7946\n",
      "Epoch 287/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7780\n",
      "Epoch 288/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0351 - val_loss: 4.7405\n",
      "Epoch 289/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 4.7619\n",
      "Epoch 290/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 4.7778\n",
      "Epoch 291/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.8079\n",
      "Epoch 292/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 4.7390\n",
      "Epoch 293/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0359 - val_loss: 4.7583\n",
      "Epoch 294/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 4.7345\n",
      "Epoch 295/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7293\n",
      "Epoch 296/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7173\n",
      "Epoch 297/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7924\n",
      "Epoch 298/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7683\n",
      "Epoch 299/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0360 - val_loss: 4.7928\n",
      "Epoch 300/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7642\n",
      "Epoch 301/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0351 - val_loss: 4.8043\n",
      "Epoch 302/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 4.8216\n",
      "Epoch 303/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7412\n",
      "Epoch 304/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 4.8295\n",
      "Epoch 305/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7303\n",
      "Epoch 306/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7404\n",
      "Epoch 307/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 4.7871\n",
      "Epoch 308/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7979\n",
      "Epoch 309/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7949\n",
      "Epoch 310/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.8036\n",
      "Epoch 311/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7530\n",
      "Epoch 312/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.7359\n",
      "Epoch 313/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.7464\n",
      "Epoch 314/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7986\n",
      "Epoch 315/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 4.7738\n",
      "Epoch 316/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0335 - val_loss: 4.6722\n",
      "Epoch 317/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 4.7726\n",
      "Epoch 318/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7302\n",
      "Epoch 319/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0360 - val_loss: 4.7727\n",
      "Epoch 320/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7795\n",
      "Epoch 321/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8297\n",
      "Epoch 322/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 4.8016\n",
      "Epoch 323/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 4.7671\n",
      "Epoch 324/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7267\n",
      "Epoch 325/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0360 - val_loss: 4.7659\n",
      "Epoch 326/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.8276\n",
      "Epoch 327/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7364\n",
      "Epoch 328/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7515\n",
      "Epoch 329/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7809\n",
      "Epoch 330/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.8235\n",
      "Epoch 331/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7535\n",
      "Epoch 332/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 4.7276\n",
      "Epoch 333/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7733\n",
      "Epoch 334/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0351 - val_loss: 4.7455\n",
      "Epoch 335/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0348 - val_loss: 4.8142\n",
      "Epoch 336/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7894\n",
      "Epoch 337/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 4.8062\n",
      "Epoch 338/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7713\n",
      "Epoch 339/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7791\n",
      "Epoch 340/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7495\n",
      "Epoch 341/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.7341\n",
      "Epoch 342/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7954\n",
      "Epoch 343/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.8094\n",
      "Epoch 344/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 4.7700\n",
      "Epoch 345/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0356 - val_loss: 4.7687\n",
      "Epoch 346/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0343 - val_loss: 4.8207\n",
      "Epoch 347/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.8281\n",
      "Epoch 348/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7209\n",
      "Epoch 349/500\n",
      "112/112 [==============================] - 0s 980us/step - loss: 0.0348 - val_loss: 4.7072\n",
      "Epoch 350/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.7699\n",
      "Epoch 351/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.8397\n",
      "Epoch 352/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0358 - val_loss: 4.8036\n",
      "Epoch 353/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 4.7845\n",
      "Epoch 354/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7884\n",
      "Epoch 355/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7212\n",
      "Epoch 356/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7560\n",
      "Epoch 357/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7586\n",
      "Epoch 358/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 4.7366\n",
      "Epoch 359/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 4.8122\n",
      "Epoch 360/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.8015\n",
      "Epoch 361/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.8059\n",
      "Epoch 362/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.8238\n",
      "Epoch 363/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7764\n",
      "Epoch 364/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7242\n",
      "Epoch 365/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.8099\n",
      "Epoch 366/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8333\n",
      "Epoch 367/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.8200\n",
      "Epoch 368/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.8372\n",
      "Epoch 369/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0354 - val_loss: 4.8212\n",
      "Epoch 370/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.8046\n",
      "Epoch 371/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7815\n",
      "Epoch 372/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7981\n",
      "Epoch 373/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7471\n",
      "Epoch 374/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0351 - val_loss: 4.7380\n",
      "Epoch 375/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 4.7764\n",
      "Epoch 376/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7353\n",
      "Epoch 377/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 4.7805\n",
      "Epoch 378/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.7180\n",
      "Epoch 379/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0351 - val_loss: 4.7209\n",
      "Epoch 380/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7313\n",
      "Epoch 381/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7807\n",
      "Epoch 382/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7905\n",
      "Epoch 383/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.8246\n",
      "Epoch 384/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7958\n",
      "Epoch 385/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0350 - val_loss: 4.7545\n",
      "Epoch 386/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7663\n",
      "Epoch 387/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7448\n",
      "Epoch 388/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7863\n",
      "Epoch 389/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0340 - val_loss: 4.7316\n",
      "Epoch 390/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7950\n",
      "Epoch 391/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7985\n",
      "Epoch 392/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7799\n",
      "Epoch 393/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7700\n",
      "Epoch 394/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.8178\n",
      "Epoch 395/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7921\n",
      "Epoch 396/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7842\n",
      "Epoch 397/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7958\n",
      "Epoch 398/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 4.7657\n",
      "Epoch 399/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 4.7937\n",
      "Epoch 400/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7905\n",
      "Epoch 401/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7845\n",
      "Epoch 402/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 4.7977\n",
      "Epoch 403/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7871\n",
      "Epoch 404/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.7932\n",
      "Epoch 405/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 4.8101\n",
      "Epoch 406/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7509\n",
      "Epoch 407/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7619\n",
      "Epoch 408/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.8230\n",
      "Epoch 409/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8345\n",
      "Epoch 410/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8374\n",
      "Epoch 411/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.8054\n",
      "Epoch 412/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7783\n",
      "Epoch 413/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7995\n",
      "Epoch 414/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7824\n",
      "Epoch 415/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7644\n",
      "Epoch 416/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7727\n",
      "Epoch 417/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.8067\n",
      "Epoch 418/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7810\n",
      "Epoch 419/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8384\n",
      "Epoch 420/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7901\n",
      "Epoch 421/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7773\n",
      "Epoch 422/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7757\n",
      "Epoch 423/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.8279\n",
      "Epoch 424/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.8198\n",
      "Epoch 425/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7846\n",
      "Epoch 426/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7696\n",
      "Epoch 427/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7832\n",
      "Epoch 428/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7711\n",
      "Epoch 429/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.8014\n",
      "Epoch 430/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7935\n",
      "Epoch 431/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.7848\n",
      "Epoch 432/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7633\n",
      "Epoch 433/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7425\n",
      "Epoch 434/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7692\n",
      "Epoch 435/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7545\n",
      "Epoch 436/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7429\n",
      "Epoch 437/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 4.7622\n",
      "Epoch 438/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7578\n",
      "Epoch 439/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7519\n",
      "Epoch 440/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7726\n",
      "Epoch 441/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7772\n",
      "Epoch 442/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 4.7433\n",
      "Epoch 443/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7839\n",
      "Epoch 444/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 4.7852\n",
      "Epoch 445/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7897\n",
      "Epoch 446/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 4.7711\n",
      "Epoch 447/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8028\n",
      "Epoch 448/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.8123\n",
      "Epoch 449/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7879\n",
      "Epoch 450/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7726\n",
      "Epoch 451/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.7882\n",
      "Epoch 452/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 4.7850\n",
      "Epoch 453/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 4.7941\n",
      "Epoch 454/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8137\n",
      "Epoch 455/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7834\n",
      "Epoch 456/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.7741\n",
      "Epoch 457/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7734\n",
      "Epoch 458/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 4.8036\n",
      "Epoch 459/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.8007\n",
      "Epoch 460/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0350 - val_loss: 4.8104\n",
      "Epoch 461/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7813\n",
      "Epoch 462/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.8247\n",
      "Epoch 463/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7828\n",
      "Epoch 464/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 4.7877\n",
      "Epoch 465/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 4.7661\n",
      "Epoch 466/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0346 - val_loss: 4.7762\n",
      "Epoch 467/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0351 - val_loss: 4.7965\n",
      "Epoch 468/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.7836\n",
      "Epoch 469/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0347 - val_loss: 4.8213\n",
      "Epoch 470/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0351 - val_loss: 4.7917\n",
      "Epoch 471/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.8004\n",
      "Epoch 472/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.7995\n",
      "Epoch 473/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 4.7713\n",
      "Epoch 474/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.8160\n",
      "Epoch 475/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0350 - val_loss: 4.8115\n",
      "Epoch 476/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8120\n",
      "Epoch 477/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 4.8008\n",
      "Epoch 478/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.8013\n",
      "Epoch 479/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7847\n",
      "Epoch 480/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.8132\n",
      "Epoch 481/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.7982\n",
      "Epoch 482/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7760\n",
      "Epoch 483/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7467\n",
      "Epoch 484/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7888\n",
      "Epoch 485/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8061\n",
      "Epoch 486/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.8257\n",
      "Epoch 487/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.7446\n",
      "Epoch 488/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7980\n",
      "Epoch 489/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.8028\n",
      "Epoch 490/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8110\n",
      "Epoch 491/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.7610\n",
      "Epoch 492/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.8081\n",
      "Epoch 493/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.8114\n",
      "Epoch 494/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.8036\n",
      "Epoch 495/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7980\n",
      "Epoch 496/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.7446\n",
      "Epoch 497/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.8000\n",
      "Epoch 498/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7854\n",
      "Epoch 499/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7812\n",
      "Epoch 500/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7537\n",
      "70/70 [==============================] - 0s 100us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5730263606778213"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 500, batch_size = 1, validation_split = 0.2, verbose = 1)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 112 samples, validate on 28 samples\n",
      "Epoch 1/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 4.9332\n",
      "Epoch 2/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 4.8265\n",
      "Epoch 3/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 4.7962\n",
      "Epoch 4/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 4.7605\n",
      "Epoch 5/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 4.7676\n",
      "Epoch 6/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 4.7756\n",
      "Epoch 7/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 4.7685\n",
      "Epoch 8/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 4.7767\n",
      "Epoch 9/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 4.7766\n",
      "Epoch 10/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 4.7738\n",
      "Epoch 11/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 4.7780\n",
      "Epoch 12/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.7852\n",
      "Epoch 13/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7951\n",
      "Epoch 14/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 4.7904\n",
      "Epoch 15/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7917\n",
      "Epoch 16/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.7841\n",
      "Epoch 17/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7853\n",
      "Epoch 18/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 4.7745\n",
      "Epoch 19/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0365 - val_loss: 4.7836\n",
      "Epoch 20/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 4.7803\n",
      "Epoch 21/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 4.7869\n",
      "Epoch 22/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 4.7862\n",
      "Epoch 23/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7675\n",
      "Epoch 24/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 4.7646\n",
      "Epoch 25/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0362 - val_loss: 4.7699\n",
      "Epoch 26/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7793\n",
      "Epoch 27/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0363 - val_loss: 4.7799\n",
      "Epoch 28/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 4.7732\n",
      "Epoch 29/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 4.7722\n",
      "Epoch 30/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 4.7771\n",
      "Epoch 31/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 4.7715\n",
      "Epoch 32/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0357 - val_loss: 4.7857\n",
      "Epoch 33/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 4.7789\n",
      "Epoch 34/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 4.7818\n",
      "Epoch 35/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 4.7697\n",
      "Epoch 36/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 4.7830\n",
      "Epoch 37/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0359 - val_loss: 4.7764\n",
      "Epoch 38/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 4.7840\n",
      "Epoch 39/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 4.7815\n",
      "Epoch 40/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 4.7747\n",
      "Epoch 41/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 4.7779\n",
      "Epoch 42/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 4.7808\n",
      "Epoch 43/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 4.7721\n",
      "Epoch 44/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 4.7765\n",
      "Epoch 45/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0359 - val_loss: 4.7815\n",
      "Epoch 46/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 4.7775\n",
      "Epoch 47/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 4.8003\n",
      "Epoch 48/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 4.7817\n",
      "Epoch 49/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7890\n",
      "Epoch 50/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 4.7908\n",
      "Epoch 51/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7887\n",
      "Epoch 52/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 4.7763\n",
      "Epoch 53/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 4.7844\n",
      "Epoch 54/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7757\n",
      "Epoch 55/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 4.7770\n",
      "Epoch 56/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 4.7777\n",
      "Epoch 57/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 4.7866\n",
      "Epoch 58/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7872\n",
      "Epoch 59/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 4.7820\n",
      "Epoch 60/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 4.7822\n",
      "Epoch 61/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7914\n",
      "Epoch 62/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 4.7841\n",
      "Epoch 63/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 4.7928\n",
      "Epoch 64/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7805\n",
      "Epoch 65/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7845\n",
      "Epoch 66/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 4.7927\n",
      "Epoch 67/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 4.7969\n",
      "Epoch 68/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7795\n",
      "Epoch 69/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 4.7886\n",
      "Epoch 70/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.7843\n",
      "Epoch 71/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0350 - val_loss: 4.7848\n",
      "Epoch 72/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 4.7909\n",
      "Epoch 73/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7884\n",
      "Epoch 74/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.7833\n",
      "Epoch 75/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.7762\n",
      "Epoch 76/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7772\n",
      "Epoch 77/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7742\n",
      "Epoch 78/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.7757\n",
      "Epoch 79/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0344 - val_loss: 4.7815\n",
      "Epoch 80/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 4.7814\n",
      "Epoch 81/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 4.7792\n",
      "Epoch 82/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.7948\n",
      "Epoch 83/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.7821\n",
      "Epoch 84/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7887\n",
      "Epoch 85/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.7870\n",
      "Epoch 86/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7899\n",
      "Epoch 87/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.7847\n",
      "Epoch 88/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.7947\n",
      "Epoch 89/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 4.7898\n",
      "Epoch 90/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 4.7900\n",
      "Epoch 91/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.7973\n",
      "Epoch 92/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.7821\n",
      "Epoch 93/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.7782\n",
      "Epoch 94/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.7796\n",
      "Epoch 95/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.7911\n",
      "Epoch 96/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.7919\n",
      "Epoch 97/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0342 - val_loss: 4.7941\n",
      "Epoch 98/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.7954\n",
      "Epoch 99/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.8017\n",
      "Epoch 100/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.7988\n",
      "Epoch 101/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.7852\n",
      "Epoch 102/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.7771\n",
      "Epoch 103/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.7882\n",
      "Epoch 104/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.7820\n",
      "Epoch 105/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7844\n",
      "Epoch 106/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.7926\n",
      "Epoch 107/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.7939\n",
      "Epoch 108/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.7985\n",
      "Epoch 109/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 4.7886\n",
      "Epoch 110/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.7914\n",
      "Epoch 111/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.7993\n",
      "Epoch 112/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.7978\n",
      "Epoch 113/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.7926\n",
      "Epoch 114/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.7858\n",
      "Epoch 115/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.7973\n",
      "Epoch 116/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.7944\n",
      "Epoch 117/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.7942\n",
      "Epoch 118/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.7922\n",
      "Epoch 119/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.7943\n",
      "Epoch 120/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.7845\n",
      "Epoch 121/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8031\n",
      "Epoch 122/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.7914\n",
      "Epoch 123/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.7916\n",
      "Epoch 124/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.7992\n",
      "Epoch 125/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.7874\n",
      "Epoch 126/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8024\n",
      "Epoch 127/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.7949\n",
      "Epoch 128/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 4.7884\n",
      "Epoch 129/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 4.7859\n",
      "Epoch 130/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8039\n",
      "Epoch 131/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.7979\n",
      "Epoch 132/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.7994\n",
      "Epoch 133/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.7969\n",
      "Epoch 134/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.7906\n",
      "Epoch 135/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.7908\n",
      "Epoch 136/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8024\n",
      "Epoch 137/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.7990\n",
      "Epoch 138/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8045\n",
      "Epoch 139/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.7878\n",
      "Epoch 140/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8074\n",
      "Epoch 141/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.7897\n",
      "Epoch 142/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 4.7950\n",
      "Epoch 143/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.7939\n",
      "Epoch 144/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 4.7834\n",
      "Epoch 145/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.7984\n",
      "Epoch 146/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0337 - val_loss: 4.7955\n",
      "Epoch 147/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0337 - val_loss: 4.7890\n",
      "Epoch 148/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.7915\n",
      "Epoch 149/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8035\n",
      "Epoch 150/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 4.7989\n",
      "Epoch 151/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0336 - val_loss: 4.7932\n",
      "Epoch 152/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0335 - val_loss: 4.8044\n",
      "Epoch 153/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8049\n",
      "Epoch 154/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.7935\n",
      "Epoch 155/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.7928\n",
      "Epoch 156/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.7993\n",
      "Epoch 157/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8057\n",
      "Epoch 158/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8072\n",
      "Epoch 159/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8029\n",
      "Epoch 160/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8008\n",
      "Epoch 161/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.7993\n",
      "Epoch 162/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8049\n",
      "Epoch 163/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.7894\n",
      "Epoch 164/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8043\n",
      "Epoch 165/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0337 - val_loss: 4.8074\n",
      "Epoch 166/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.7998\n",
      "Epoch 167/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 4.8111\n",
      "Epoch 168/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 4.8009\n",
      "Epoch 169/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.7965\n",
      "Epoch 170/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.7970\n",
      "Epoch 171/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8027\n",
      "Epoch 172/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.7993\n",
      "Epoch 173/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8082\n",
      "Epoch 174/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.7943\n",
      "Epoch 175/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.7970\n",
      "Epoch 176/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8033\n",
      "Epoch 177/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.7921\n",
      "Epoch 178/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0335 - val_loss: 4.8067\n",
      "Epoch 179/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.7977\n",
      "Epoch 180/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8056\n",
      "Epoch 181/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8062\n",
      "Epoch 182/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8114\n",
      "Epoch 183/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.7994\n",
      "Epoch 184/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0334 - val_loss: 4.8129\n",
      "Epoch 185/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.7954\n",
      "Epoch 186/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8106\n",
      "Epoch 187/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8057\n",
      "Epoch 188/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8016\n",
      "Epoch 189/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8054\n",
      "Epoch 190/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8009\n",
      "Epoch 191/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8171\n",
      "Epoch 192/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8142\n",
      "Epoch 193/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8035\n",
      "Epoch 194/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8002\n",
      "Epoch 195/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8056\n",
      "Epoch 196/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.8000\n",
      "Epoch 197/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8033\n",
      "Epoch 198/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.7936\n",
      "Epoch 199/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.7953\n",
      "Epoch 200/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8047\n",
      "Epoch 201/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.7969\n",
      "Epoch 202/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8041\n",
      "Epoch 203/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.7948\n",
      "Epoch 204/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8095\n",
      "Epoch 205/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 4.8046\n",
      "Epoch 206/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.7985\n",
      "Epoch 207/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 4.8003\n",
      "Epoch 208/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.7996\n",
      "Epoch 209/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8026\n",
      "Epoch 210/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8031\n",
      "Epoch 211/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8080\n",
      "Epoch 212/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.8043\n",
      "Epoch 213/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.7957\n",
      "Epoch 214/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.8014\n",
      "Epoch 215/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8021\n",
      "Epoch 216/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.7948\n",
      "Epoch 217/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.7980\n",
      "Epoch 218/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 4.8063\n",
      "Epoch 219/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8069\n",
      "Epoch 220/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8092\n",
      "Epoch 221/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8056\n",
      "Epoch 222/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8056\n",
      "Epoch 223/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.7968\n",
      "Epoch 224/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.7979\n",
      "Epoch 225/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.8024\n",
      "Epoch 226/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8075\n",
      "Epoch 227/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8121\n",
      "Epoch 228/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8125\n",
      "Epoch 229/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8161\n",
      "Epoch 230/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8145\n",
      "Epoch 231/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.7932\n",
      "Epoch 232/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.7968\n",
      "Epoch 233/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 4.8063\n",
      "Epoch 234/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8087\n",
      "Epoch 235/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.7991\n",
      "Epoch 236/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 4.8035\n",
      "Epoch 237/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8064\n",
      "Epoch 238/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8013\n",
      "Epoch 239/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 4.8053\n",
      "Epoch 240/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8052\n",
      "Epoch 241/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8092\n",
      "Epoch 242/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0329 - val_loss: 4.8104\n",
      "Epoch 243/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8092\n",
      "Epoch 244/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8043\n",
      "Epoch 245/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8090\n",
      "Epoch 246/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8089\n",
      "Epoch 247/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8093\n",
      "Epoch 248/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8049\n",
      "Epoch 249/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0329 - val_loss: 4.7996\n",
      "Epoch 250/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0329 - val_loss: 4.8029\n",
      "Epoch 251/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 4.8078\n",
      "Epoch 252/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0331 - val_loss: 4.8010\n",
      "Epoch 253/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 4.8062\n",
      "Epoch 254/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8097\n",
      "Epoch 255/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8182\n",
      "Epoch 256/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8089\n",
      "Epoch 257/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8153\n",
      "Epoch 258/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.8135\n",
      "Epoch 259/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8032\n",
      "Epoch 260/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8006\n",
      "Epoch 261/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.8042\n",
      "Epoch 262/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.8096\n",
      "Epoch 263/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.8042\n",
      "Epoch 264/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8057\n",
      "Epoch 265/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.8029\n",
      "Epoch 266/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8135\n",
      "Epoch 267/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 4.8075\n",
      "Epoch 268/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8027\n",
      "Epoch 269/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 4.8242\n",
      "Epoch 270/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 4.8122\n",
      "Epoch 271/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8127\n",
      "Epoch 272/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 4.8196\n",
      "Epoch 273/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 4.8001\n",
      "Epoch 274/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 4.8198\n",
      "Epoch 275/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 4.8180\n",
      "Epoch 276/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 4.8098\n",
      "Epoch 277/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8084\n",
      "Epoch 278/500\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.0349  - 0s 2ms/step - loss: 0.0333 - val_loss: 4.7987\n",
      "Epoch 279/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0330 - val_loss: 4.8089\n",
      "Epoch 280/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8075\n",
      "Epoch 281/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 4.8052\n",
      "Epoch 282/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0330 - val_loss: 4.8048\n",
      "Epoch 283/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8109\n",
      "Epoch 284/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 4.8129\n",
      "Epoch 285/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8269\n",
      "Epoch 286/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 4.8083\n",
      "Epoch 287/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8098\n",
      "Epoch 288/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8174\n",
      "Epoch 289/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8070\n",
      "Epoch 290/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8041\n",
      "Epoch 291/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8174\n",
      "Epoch 292/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8122\n",
      "Epoch 293/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8011\n",
      "Epoch 294/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8195\n",
      "Epoch 295/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8172\n",
      "Epoch 296/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8060\n",
      "Epoch 297/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8061\n",
      "Epoch 298/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.8039\n",
      "Epoch 299/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.7980\n",
      "Epoch 300/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8048\n",
      "Epoch 301/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8080\n",
      "Epoch 302/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8029\n",
      "Epoch 303/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8084\n",
      "Epoch 304/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8029\n",
      "Epoch 305/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 4.8080\n",
      "Epoch 306/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8176\n",
      "Epoch 307/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8132\n",
      "Epoch 308/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0331 - val_loss: 4.8049\n",
      "Epoch 309/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8261\n",
      "Epoch 310/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 4.8201\n",
      "Epoch 311/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8215\n",
      "Epoch 312/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 4.8149\n",
      "Epoch 313/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8206\n",
      "Epoch 314/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 4.8206\n",
      "Epoch 315/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8229\n",
      "Epoch 316/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8156\n",
      "Epoch 317/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8197\n",
      "Epoch 318/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8208\n",
      "Epoch 319/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8174\n",
      "Epoch 320/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8217\n",
      "Epoch 321/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8118\n",
      "Epoch 322/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8199\n",
      "Epoch 323/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8148\n",
      "Epoch 324/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8136\n",
      "Epoch 325/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8099\n",
      "Epoch 326/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8169\n",
      "Epoch 327/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8205\n",
      "Epoch 328/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8009\n",
      "Epoch 329/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8175\n",
      "Epoch 330/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 4.8158\n",
      "Epoch 331/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0327 - val_loss: 4.8246\n",
      "Epoch 332/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8223\n",
      "Epoch 333/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 4.8165\n",
      "Epoch 334/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 4.8134\n",
      "Epoch 335/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0327 - val_loss: 4.8072\n",
      "Epoch 336/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.8052\n",
      "Epoch 337/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 4.8134\n",
      "Epoch 338/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 4.8217\n",
      "Epoch 339/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8130\n",
      "Epoch 340/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8146\n",
      "Epoch 341/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.8043\n",
      "Epoch 342/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8119\n",
      "Epoch 343/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8112\n",
      "Epoch 344/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8058\n",
      "Epoch 345/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8113\n",
      "Epoch 346/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 4.8235\n",
      "Epoch 347/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8216\n",
      "Epoch 348/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8188\n",
      "Epoch 349/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8193\n",
      "Epoch 350/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8214\n",
      "Epoch 351/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8147\n",
      "Epoch 352/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 4.8210\n",
      "Epoch 353/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 4.8263\n",
      "Epoch 354/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8160\n",
      "Epoch 355/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8128\n",
      "Epoch 356/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8055\n",
      "Epoch 357/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8144\n",
      "Epoch 358/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8243\n",
      "Epoch 359/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8164\n",
      "Epoch 360/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 4.8140\n",
      "Epoch 361/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8185\n",
      "Epoch 362/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8232\n",
      "Epoch 363/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8078\n",
      "Epoch 364/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 4.8039\n",
      "Epoch 365/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8207\n",
      "Epoch 366/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.8163\n",
      "Epoch 367/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8287\n",
      "Epoch 368/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8125\n",
      "Epoch 369/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8101\n",
      "Epoch 370/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8214\n",
      "Epoch 371/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8157\n",
      "Epoch 372/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8120\n",
      "Epoch 373/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 4.8351\n",
      "Epoch 374/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8198\n",
      "Epoch 375/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8085\n",
      "Epoch 376/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8270\n",
      "Epoch 377/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8255\n",
      "Epoch 378/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8226\n",
      "Epoch 379/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8277\n",
      "Epoch 380/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8141\n",
      "Epoch 381/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8179\n",
      "Epoch 382/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 4.8218\n",
      "Epoch 383/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 4.8093\n",
      "Epoch 384/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 4.8129\n",
      "Epoch 385/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 4.8238\n",
      "Epoch 386/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8270\n",
      "Epoch 387/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 4.8056\n",
      "Epoch 388/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 4.8172\n",
      "Epoch 389/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8124\n",
      "Epoch 390/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8208\n",
      "Epoch 391/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8123\n",
      "Epoch 392/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8181\n",
      "Epoch 393/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8209\n",
      "Epoch 394/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8125\n",
      "Epoch 395/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8086\n",
      "Epoch 396/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 4.8156\n",
      "Epoch 397/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 4.8096\n",
      "Epoch 398/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 4.8108\n",
      "Epoch 399/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0324 - val_loss: 4.8230\n",
      "Epoch 400/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0321 - val_loss: 4.8247\n",
      "Epoch 401/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0329 - val_loss: 4.8198\n",
      "Epoch 402/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8226\n",
      "Epoch 403/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8207\n",
      "Epoch 404/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8196\n",
      "Epoch 405/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8116\n",
      "Epoch 406/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8103\n",
      "Epoch 407/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8250\n",
      "Epoch 408/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8124\n",
      "Epoch 409/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8185\n",
      "Epoch 410/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 4.8302\n",
      "Epoch 411/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 4.8136\n",
      "Epoch 412/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8231\n",
      "Epoch 413/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8241\n",
      "Epoch 414/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8210\n",
      "Epoch 415/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8154\n",
      "Epoch 416/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 4.8214\n",
      "Epoch 417/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8103\n",
      "Epoch 418/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8178\n",
      "Epoch 419/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 4.8198\n",
      "Epoch 420/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8192\n",
      "Epoch 421/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 4.8242\n",
      "Epoch 422/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8270\n",
      "Epoch 423/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8323\n",
      "Epoch 424/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8374\n",
      "Epoch 425/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 4.8260\n",
      "Epoch 426/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 4.8179\n",
      "Epoch 427/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 4.8045\n",
      "Epoch 428/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8197\n",
      "Epoch 429/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8209\n",
      "Epoch 430/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8181\n",
      "Epoch 431/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8220\n",
      "Epoch 432/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8198\n",
      "Epoch 433/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0323 - val_loss: 4.8122\n",
      "Epoch 434/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0323 - val_loss: 4.8166\n",
      "Epoch 435/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8152\n",
      "Epoch 436/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8148\n",
      "Epoch 437/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8150\n",
      "Epoch 438/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 4.8204\n",
      "Epoch 439/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8090\n",
      "Epoch 440/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8203\n",
      "Epoch 441/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8248\n",
      "Epoch 442/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8240\n",
      "Epoch 443/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 4.8206\n",
      "Epoch 444/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8373\n",
      "Epoch 445/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8068\n",
      "Epoch 446/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8245\n",
      "Epoch 447/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8209\n",
      "Epoch 448/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8240\n",
      "Epoch 449/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8289\n",
      "Epoch 450/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 4.8345\n",
      "Epoch 451/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 4.8307\n",
      "Epoch 452/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8233\n",
      "Epoch 453/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8154\n",
      "Epoch 454/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8208\n",
      "Epoch 455/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8205\n",
      "Epoch 456/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8164\n",
      "Epoch 457/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 4.8161\n",
      "Epoch 458/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8272\n",
      "Epoch 459/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8283\n",
      "Epoch 460/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8184\n",
      "Epoch 461/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8233\n",
      "Epoch 462/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 4.8276\n",
      "Epoch 463/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8367\n",
      "Epoch 464/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8299\n",
      "Epoch 465/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8242\n",
      "Epoch 466/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8295\n",
      "Epoch 467/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 4.8146\n",
      "Epoch 468/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8259\n",
      "Epoch 469/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8254\n",
      "Epoch 470/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8306\n",
      "Epoch 471/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 4.8227\n",
      "Epoch 472/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 4.8192\n",
      "Epoch 473/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8343\n",
      "Epoch 474/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 4.8236\n",
      "Epoch 475/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8259\n",
      "Epoch 476/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8364\n",
      "Epoch 477/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8167\n",
      "Epoch 478/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8232\n",
      "Epoch 479/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 4.8223\n",
      "Epoch 480/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 4.8231\n",
      "Epoch 481/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8185\n",
      "Epoch 482/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8284\n",
      "Epoch 483/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 4.8173\n",
      "Epoch 484/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8300\n",
      "Epoch 485/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 4.8226\n",
      "Epoch 486/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 4.8260\n",
      "Epoch 487/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 4.8249\n",
      "Epoch 488/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 4.8279\n",
      "Epoch 489/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 4.8194\n",
      "Epoch 490/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 4.8350\n",
      "Epoch 491/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8108\n",
      "Epoch 492/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8169\n",
      "Epoch 493/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.8268\n",
      "Epoch 494/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 4.8172\n",
      "Epoch 495/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 4.8338\n",
      "Epoch 496/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 4.8178\n",
      "Epoch 497/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8394\n",
      "Epoch 498/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8146\n",
      "Epoch 499/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 4.8133\n",
      "Epoch 500/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 4.8226\n",
      "70/70 [==============================] - 0s 43us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5931160931076322"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note: Replace this with RNNs. Try RNNs. \n",
    "\n",
    "## Try different hidden layers. Reduce neurons and add more hiddean layers. \n",
    "    \n",
    "in_shape = X_train.iloc[0].shape\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation = 'relu', input_shape = in_shape))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 500, batch_size = 1, validation_split = 0.2, verbose = 1)\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 12)                36        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 112 samples, validate on 28 samples\n",
      "Epoch 1/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0630 - val_loss: 4.8834\n",
      "Epoch 2/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 4.8470\n",
      "Epoch 3/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 4.8212\n",
      "Epoch 4/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 4.8010\n",
      "Epoch 5/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 4.7910\n",
      "Epoch 6/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 4.7756\n",
      "Epoch 7/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0388 - val_loss: 4.7717\n",
      "Epoch 8/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0386 - val_loss: 4.7611\n",
      "Epoch 9/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 4.7578\n",
      "Epoch 10/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 4.7716\n",
      "Epoch 11/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 4.7638\n",
      "Epoch 12/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 4.7676\n",
      "Epoch 13/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 4.7650\n",
      "Epoch 14/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 4.7693\n",
      "Epoch 15/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0370 - val_loss: 4.7761\n",
      "Epoch 16/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0368 - val_loss: 4.7708\n",
      "Epoch 17/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 4.7740\n",
      "Epoch 18/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 4.7792\n",
      "Epoch 19/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 4.7754\n",
      "Epoch 20/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.7768\n",
      "Epoch 21/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.7857\n",
      "Epoch 22/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 4.7750\n",
      "Epoch 23/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 4.7810\n",
      "Epoch 24/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0361 - val_loss: 4.7868\n",
      "Epoch 25/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0359 - val_loss: 4.7788\n",
      "Epoch 26/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.7832\n",
      "Epoch 27/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 4.7776\n",
      "Epoch 28/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 4.7840\n",
      "Epoch 29/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0358 - val_loss: 4.7809\n",
      "Epoch 30/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 4.7851\n",
      "Epoch 31/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.7932\n",
      "Epoch 32/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7925\n",
      "Epoch 33/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 4.7957\n",
      "Epoch 34/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7869\n",
      "Epoch 35/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 4.7950\n",
      "Epoch 36/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 4.7838\n",
      "Epoch 37/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7931\n",
      "Epoch 38/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7837\n",
      "Epoch 39/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 4.7944\n",
      "Epoch 40/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7821\n",
      "Epoch 41/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7864\n",
      "Epoch 42/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7887\n",
      "Epoch 43/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7956\n",
      "Epoch 44/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 4.7876\n",
      "Epoch 45/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 4.7857\n",
      "Epoch 46/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7782\n",
      "Epoch 47/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7911\n",
      "Epoch 48/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 4.7844\n",
      "Epoch 49/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 4.7846\n",
      "Epoch 50/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 4.7876\n",
      "Epoch 51/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7931\n",
      "Epoch 52/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7881\n",
      "Epoch 53/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7940\n",
      "Epoch 54/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 4.7929\n",
      "Epoch 55/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 4.7963\n",
      "Epoch 56/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0349 - val_loss: 4.7928\n",
      "Epoch 57/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.7979\n",
      "Epoch 58/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7970\n",
      "Epoch 59/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7882\n",
      "Epoch 60/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 4.7880\n",
      "Epoch 61/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 4.7899\n",
      "Epoch 62/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 4.7959\n",
      "Epoch 63/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 4.7920\n",
      "Epoch 64/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7941\n",
      "Epoch 65/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 4.7983\n",
      "Epoch 66/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 4.7922\n",
      "Epoch 67/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 4.7948\n",
      "Epoch 68/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.8060\n",
      "Epoch 69/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.7895\n",
      "Epoch 70/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.7894\n",
      "Epoch 71/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.7937\n",
      "Epoch 72/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 4.7911\n",
      "Epoch 73/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 4.7907\n",
      "Epoch 74/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 4.7944\n",
      "Epoch 75/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 4.7894\n",
      "Epoch 76/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.8070\n",
      "Epoch 77/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7940\n",
      "Epoch 78/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 4.7943\n",
      "Epoch 79/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.8035\n",
      "Epoch 80/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.7984\n",
      "Epoch 81/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 4.7963\n",
      "Epoch 82/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7941\n",
      "Epoch 83/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 4.8067\n",
      "Epoch 84/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 4.7980\n",
      "Epoch 85/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 4.7875\n",
      "Epoch 86/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 4.7848\n",
      "Epoch 87/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.7872\n",
      "Epoch 88/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 4.7943\n",
      "Epoch 89/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 4.8003\n",
      "Epoch 90/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 4.7986\n",
      "Epoch 91/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.8018\n",
      "Epoch 92/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 4.7982\n",
      "Epoch 93/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 4.7966\n",
      "Epoch 94/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.7964\n",
      "Epoch 95/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.8010\n",
      "Epoch 96/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 4.7957\n",
      "Epoch 97/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 4.7896\n",
      "Epoch 98/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.7977\n",
      "Epoch 99/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.7993\n",
      "Epoch 100/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 4.8020\n",
      "Epoch 101/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.8007\n",
      "Epoch 102/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8050\n",
      "Epoch 103/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0343 - val_loss: 4.8000\n",
      "Epoch 104/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.8003\n",
      "Epoch 105/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.8037\n",
      "Epoch 106/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.8014\n",
      "Epoch 107/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8041\n",
      "Epoch 108/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8035\n",
      "Epoch 109/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.8009\n",
      "Epoch 110/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 4.8036\n",
      "Epoch 111/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8016\n",
      "Epoch 112/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8054\n",
      "Epoch 113/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.8077\n",
      "Epoch 114/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8135\n",
      "Epoch 115/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.8030\n",
      "Epoch 116/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8091\n",
      "Epoch 117/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 4.8060\n",
      "Epoch 118/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8098\n",
      "Epoch 119/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.8102\n",
      "Epoch 120/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8106\n",
      "Epoch 121/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 4.8065\n",
      "Epoch 122/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 4.8064\n",
      "Epoch 123/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.8112\n",
      "Epoch 124/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 4.8022\n",
      "Epoch 125/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 4.8034\n",
      "Epoch 126/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8079\n",
      "Epoch 127/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.8133\n",
      "Epoch 128/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.8050\n",
      "Epoch 129/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8044\n",
      "Epoch 130/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 4.8048\n",
      "Epoch 131/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.8087\n",
      "Epoch 132/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8037\n",
      "Epoch 133/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.7998\n",
      "Epoch 134/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 4.8091\n",
      "Epoch 135/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 4.8187\n",
      "Epoch 136/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.8073\n",
      "Epoch 137/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 4.8039\n",
      "Epoch 138/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0343 - val_loss: 4.8138\n",
      "Epoch 139/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.8056\n",
      "Epoch 140/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8065\n",
      "Epoch 141/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.8101\n",
      "Epoch 142/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8115\n",
      "Epoch 143/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.8067\n",
      "Epoch 144/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 4.8237\n",
      "Epoch 145/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8061\n",
      "Epoch 146/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8036\n",
      "Epoch 147/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.8110\n",
      "Epoch 148/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8067\n",
      "Epoch 149/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 4.8129\n",
      "Epoch 150/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 4.8016\n",
      "Epoch 151/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 4.8088\n",
      "Epoch 152/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8068\n",
      "Epoch 153/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 4.8082\n",
      "Epoch 154/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8117\n",
      "Epoch 155/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8100\n",
      "Epoch 156/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0341 - val_loss: 4.8179\n",
      "Epoch 157/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0343 - val_loss: 4.8169\n",
      "Epoch 158/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8123\n",
      "Epoch 159/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.8072\n",
      "Epoch 160/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8065\n",
      "Epoch 161/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8121\n",
      "Epoch 162/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8182\n",
      "Epoch 163/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8099\n",
      "Epoch 164/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8132\n",
      "Epoch 165/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 4.8091\n",
      "Epoch 166/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8126\n",
      "Epoch 167/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8195\n",
      "Epoch 168/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8141\n",
      "Epoch 169/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8006\n",
      "Epoch 170/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.8204\n",
      "Epoch 171/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8211\n",
      "Epoch 172/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.8134\n",
      "Epoch 173/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 4.8184\n",
      "Epoch 174/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 4.8112\n",
      "Epoch 175/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8099\n",
      "Epoch 176/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8211\n",
      "Epoch 177/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8202\n",
      "Epoch 178/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8149\n",
      "Epoch 179/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 4.8106\n",
      "Epoch 180/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8143\n",
      "Epoch 181/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8121\n",
      "Epoch 182/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8127\n",
      "Epoch 183/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8157\n",
      "Epoch 184/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8205\n",
      "Epoch 185/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8137\n",
      "Epoch 186/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8090\n",
      "Epoch 187/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8121\n",
      "Epoch 188/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 4.8109\n",
      "Epoch 189/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 4.8203\n",
      "Epoch 190/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 4.8174\n",
      "Epoch 191/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8223\n",
      "Epoch 192/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8261\n",
      "Epoch 193/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8207\n",
      "Epoch 194/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 4.8230\n",
      "Epoch 195/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8228\n",
      "Epoch 196/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8244\n",
      "Epoch 197/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8313\n",
      "Epoch 198/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8241\n",
      "Epoch 199/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8282\n",
      "Epoch 200/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8256\n",
      "Epoch 201/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0340 - val_loss: 4.8157\n",
      "Epoch 202/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0338 - val_loss: 4.8183\n",
      "Epoch 203/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0342 - val_loss: 4.8171\n",
      "Epoch 204/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8155\n",
      "Epoch 205/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0338 - val_loss: 4.8178\n",
      "Epoch 206/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0339 - val_loss: 4.8250\n",
      "Epoch 207/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8238\n",
      "Epoch 208/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8222\n",
      "Epoch 209/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8264\n",
      "Epoch 210/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8214\n",
      "Epoch 211/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8206\n",
      "Epoch 212/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8367\n",
      "Epoch 213/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8234\n",
      "Epoch 214/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8082\n",
      "Epoch 215/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8151\n",
      "Epoch 216/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8227\n",
      "Epoch 217/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8213\n",
      "Epoch 218/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8187\n",
      "Epoch 219/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8217\n",
      "Epoch 220/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8107\n",
      "Epoch 221/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8180\n",
      "Epoch 222/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8218\n",
      "Epoch 223/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8184\n",
      "Epoch 224/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 4.8258\n",
      "Epoch 225/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8211\n",
      "Epoch 226/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8239\n",
      "Epoch 227/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8335\n",
      "Epoch 228/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8232\n",
      "Epoch 229/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8305.\n",
      "Epoch 230/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8200\n",
      "Epoch 231/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8263\n",
      "Epoch 232/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8193\n",
      "Epoch 233/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 4.8245\n",
      "Epoch 234/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 4.8214\n",
      "Epoch 235/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8186\n",
      "Epoch 236/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8239\n",
      "Epoch 237/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8186\n",
      "Epoch 238/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8172\n",
      "Epoch 239/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8157\n",
      "Epoch 240/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8203\n",
      "Epoch 241/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0341 - val_loss: 4.8128\n",
      "Epoch 242/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8300\n",
      "Epoch 243/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8257\n",
      "Epoch 244/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8239\n",
      "Epoch 245/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8222\n",
      "Epoch 246/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8231\n",
      "Epoch 247/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 4.8150\n",
      "Epoch 248/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8251\n",
      "Epoch 249/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0338 - val_loss: 4.8269\n",
      "Epoch 250/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8162\n",
      "Epoch 251/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8150\n",
      "Epoch 252/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8202\n",
      "Epoch 253/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 4.8205\n",
      "Epoch 254/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8186\n",
      "Epoch 255/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8187\n",
      "Epoch 256/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8296\n",
      "Epoch 257/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8201\n",
      "Epoch 258/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8172\n",
      "Epoch 259/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8305\n",
      "Epoch 260/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8191\n",
      "Epoch 261/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8146\n",
      "Epoch 262/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8358\n",
      "Epoch 263/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8187\n",
      "Epoch 264/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0339 - val_loss: 4.8190\n",
      "Epoch 265/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8241\n",
      "Epoch 266/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8243\n",
      "Epoch 267/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8209\n",
      "Epoch 268/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8158\n",
      "Epoch 269/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8233\n",
      "Epoch 270/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8305\n",
      "Epoch 271/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8151\n",
      "Epoch 272/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8203\n",
      "Epoch 273/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8340\n",
      "Epoch 274/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8247\n",
      "Epoch 275/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8160\n",
      "Epoch 276/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8142\n",
      "Epoch 277/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8143\n",
      "Epoch 278/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8216\n",
      "Epoch 279/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8232\n",
      "Epoch 280/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8226\n",
      "Epoch 281/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8248\n",
      "Epoch 282/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8183\n",
      "Epoch 283/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8356\n",
      "Epoch 284/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8292\n",
      "Epoch 285/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8194\n",
      "Epoch 286/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8235\n",
      "Epoch 287/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8187\n",
      "Epoch 288/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8184\n",
      "Epoch 289/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 4.8144\n",
      "Epoch 290/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 4.8211\n",
      "Epoch 291/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0337 - val_loss: 4.8222\n",
      "Epoch 292/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8248\n",
      "Epoch 293/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8227\n",
      "Epoch 294/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8187\n",
      "Epoch 295/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8203\n",
      "Epoch 296/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8329\n",
      "Epoch 297/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8262\n",
      "Epoch 298/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8203\n",
      "Epoch 299/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8221\n",
      "Epoch 300/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8245\n",
      "Epoch 301/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8260\n",
      "Epoch 302/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8208\n",
      "Epoch 303/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8226\n",
      "Epoch 304/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8276\n",
      "Epoch 305/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8234\n",
      "Epoch 306/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8277\n",
      "Epoch 307/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8268\n",
      "Epoch 308/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0334 - val_loss: 4.8205\n",
      "Epoch 309/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8182\n",
      "Epoch 310/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8161\n",
      "Epoch 311/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8214\n",
      "Epoch 312/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8313\n",
      "Epoch 313/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 4.8483\n",
      "Epoch 314/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8268\n",
      "Epoch 315/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8409\n",
      "Epoch 316/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 4.8282\n",
      "Epoch 317/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 4.8173\n",
      "Epoch 318/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 4.8199\n",
      "Epoch 319/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8275\n",
      "Epoch 320/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8255\n",
      "Epoch 321/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8239\n",
      "Epoch 322/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8208\n",
      "Epoch 323/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8157\n",
      "Epoch 324/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 4.8245\n",
      "Epoch 325/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0338 - val_loss: 4.8409\n",
      "Epoch 326/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8287\n",
      "Epoch 327/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8330\n",
      "Epoch 328/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8248\n",
      "Epoch 329/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8167\n",
      "Epoch 330/500\n",
      " 52/112 [============>.................] - ETA: 0s - loss: 0.0416"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.163864). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0336 - val_loss: 4.8333\n",
      "Epoch 331/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8193\n",
      "Epoch 332/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8250\n",
      "Epoch 333/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8237\n",
      "Epoch 334/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8263\n",
      "Epoch 335/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8225\n",
      "Epoch 336/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8173\n",
      "Epoch 337/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8221\n",
      "Epoch 338/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8258\n",
      "Epoch 339/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8224\n",
      "Epoch 340/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8271\n",
      "Epoch 341/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8206\n",
      "Epoch 342/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8287\n",
      "Epoch 343/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0335 - val_loss: 4.8181\n",
      "Epoch 344/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8194\n",
      "Epoch 345/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8221\n",
      "Epoch 346/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8233\n",
      "Epoch 347/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 4.8195\n",
      "Epoch 348/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8139\n",
      "Epoch 349/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8246\n",
      "Epoch 350/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8286\n",
      "Epoch 351/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8286\n",
      "Epoch 352/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8267\n",
      "Epoch 353/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8289\n",
      "Epoch 354/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8234\n",
      "Epoch 355/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8228\n",
      "Epoch 356/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8220\n",
      "Epoch 357/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8231\n",
      "Epoch 358/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8181\n",
      "Epoch 359/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8232\n",
      "Epoch 360/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8194\n",
      "Epoch 361/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8212\n",
      "Epoch 362/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8170\n",
      "Epoch 363/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8206\n",
      "Epoch 364/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8161\n",
      "Epoch 365/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8127\n",
      "Epoch 366/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 4.8152\n",
      "Epoch 367/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8126\n",
      "Epoch 368/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8177\n",
      "Epoch 369/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8228\n",
      "Epoch 370/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8236\n",
      "Epoch 371/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8273\n",
      "Epoch 372/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8268\n",
      "Epoch 373/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8209\n",
      "Epoch 374/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8252\n",
      "Epoch 375/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8336\n",
      "Epoch 376/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8227\n",
      "Epoch 377/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8218\n",
      "Epoch 378/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8208\n",
      "Epoch 379/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8254\n",
      "Epoch 380/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8168\n",
      "Epoch 381/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8211\n",
      "Epoch 382/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8161\n",
      "Epoch 383/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8139\n",
      "Epoch 384/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8149\n",
      "Epoch 385/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8169\n",
      "Epoch 386/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8191\n",
      "Epoch 387/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8159\n",
      "Epoch 388/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8166\n",
      "Epoch 389/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8145\n",
      "Epoch 390/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 4.8153\n",
      "Epoch 391/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8288\n",
      "Epoch 392/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8264\n",
      "Epoch 393/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8232\n",
      "Epoch 394/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8191\n",
      "Epoch 395/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8191\n",
      "Epoch 396/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8156\n",
      "Epoch 397/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8222\n",
      "Epoch 398/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8203\n",
      "Epoch 399/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8159\n",
      "Epoch 400/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8182\n",
      "Epoch 401/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8252\n",
      "Epoch 402/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8238\n",
      "Epoch 403/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8284\n",
      "Epoch 404/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8176\n",
      "Epoch 405/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8240\n",
      "Epoch 406/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8194\n",
      "Epoch 407/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8260\n",
      "Epoch 408/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8211\n",
      "Epoch 409/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8216\n",
      "Epoch 410/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8216\n",
      "Epoch 411/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8273\n",
      "Epoch 412/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8252\n",
      "Epoch 413/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8353\n",
      "Epoch 414/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8257\n",
      "Epoch 415/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8193\n",
      "Epoch 416/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8254\n",
      "Epoch 417/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8228\n",
      "Epoch 418/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8198\n",
      "Epoch 419/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8216\n",
      "Epoch 420/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 4.8177\n",
      "Epoch 421/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8238\n",
      "Epoch 422/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0334 - val_loss: 4.8269\n",
      "Epoch 423/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8300\n",
      "Epoch 424/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 4.8204\n",
      "Epoch 425/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8197\n",
      "Epoch 426/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8299\n",
      "Epoch 427/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 4.8224\n",
      "Epoch 428/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0334 - val_loss: 4.8298\n",
      "Epoch 429/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 4.8264\n",
      "Epoch 430/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 4.8276\n",
      "Epoch 431/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8245\n",
      "Epoch 432/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8227\n",
      "Epoch 433/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8259\n",
      "Epoch 434/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8284\n",
      "Epoch 435/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8361\n",
      "Epoch 436/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8257\n",
      "Epoch 437/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 4.8265\n",
      "Epoch 438/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8288\n",
      "Epoch 439/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8242\n",
      "Epoch 440/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8239\n",
      "Epoch 441/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8298\n",
      "Epoch 442/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8305\n",
      "Epoch 443/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8285\n",
      "Epoch 444/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8305\n",
      "Epoch 445/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8223\n",
      "Epoch 446/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8222\n",
      "Epoch 447/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8231\n",
      "Epoch 448/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8314\n",
      "Epoch 449/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8163\n",
      "Epoch 450/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8217\n",
      "Epoch 451/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8301\n",
      "Epoch 452/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8251\n",
      "Epoch 453/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8272\n",
      "Epoch 454/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8425\n",
      "Epoch 455/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 4.8255\n",
      "Epoch 456/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8334\n",
      "Epoch 457/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8286\n",
      "Epoch 458/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8261\n",
      "Epoch 459/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8315\n",
      "Epoch 460/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8223\n",
      "Epoch 461/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8191\n",
      "Epoch 462/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 4.8204\n",
      "Epoch 463/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8314\n",
      "Epoch 464/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 4.8183\n",
      "Epoch 465/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8220\n",
      "Epoch 466/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8292\n",
      "Epoch 467/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.8255\n",
      "Epoch 468/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8274\n",
      "Epoch 469/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8259\n",
      "Epoch 470/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8241\n",
      "Epoch 471/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 4.8223\n",
      "Epoch 472/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 4.8309\n",
      "Epoch 473/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 4.8287\n",
      "Epoch 474/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 4.8241\n",
      "Epoch 475/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 4.8269\n",
      "Epoch 476/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8271\n",
      "Epoch 477/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8342\n",
      "Epoch 478/500\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 4.8217\n",
      "Epoch 479/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8264\n",
      "Epoch 480/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 4.8280\n",
      "Epoch 481/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8213\n",
      "Epoch 482/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 4.8245\n",
      "Epoch 483/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 4.8360\n",
      "Epoch 484/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8274\n",
      "Epoch 485/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8227\n",
      "Epoch 486/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8264\n",
      "Epoch 487/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8235\n",
      "Epoch 488/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8357\n",
      "Epoch 489/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8294\n",
      "Epoch 490/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 4.8300\n",
      "Epoch 491/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8315\n",
      "Epoch 492/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8331\n",
      "Epoch 493/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8345\n",
      "Epoch 494/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8436\n",
      "Epoch 495/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8278\n",
      "Epoch 496/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 4.8283\n",
      "Epoch 497/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8289\n",
      "Epoch 498/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 4.8257\n",
      "Epoch 499/500\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 4.8256\n",
      "Epoch 500/500\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 4.8186\n",
      "70/70 [==============================] - 0s 113us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5937402169619288"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "in_shape = X_train.iloc[0].shape\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation = 'relu', input_shape = in_shape))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adamax', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 500, batch_size = 1, validation_split = 0.2, verbose = 1)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      location        date  new_cases  new_deaths  stringency_index  \\\n",
       " 12170   Kuwait  2019-12-31        0.0         0.0               0.0   \n",
       " 12171   Kuwait  2020-01-01        0.0         0.0               0.0   \n",
       " 12172   Kuwait  2020-01-02        0.0         0.0               0.0   \n",
       " 12173   Kuwait  2020-01-03        0.0         0.0               0.0   \n",
       " 12174   Kuwait  2020-01-04        0.0         0.0               0.0   \n",
       " ...        ...         ...        ...         ...               ...   \n",
       " 12332   Kuwait  2020-06-13      520.0         6.0               0.0   \n",
       " 12333   Kuwait  2020-06-14      514.0         4.0               0.0   \n",
       " 12334   Kuwait  2020-06-15      454.0         7.0               0.0   \n",
       " 12335   Kuwait  2020-06-16      511.0         2.0               0.0   \n",
       " 12336   Kuwait  2020-06-17      527.0         5.0               0.0   \n",
       " \n",
       "        population  population_density  median_age  aged_65_older  \\\n",
       " 12170   4270563.0             232.128        33.7          2.345   \n",
       " 12171   4270563.0             232.128        33.7          2.345   \n",
       " 12172   4270563.0             232.128        33.7          2.345   \n",
       " 12173   4270563.0             232.128        33.7          2.345   \n",
       " 12174   4270563.0             232.128        33.7          2.345   \n",
       " ...           ...                 ...         ...            ...   \n",
       " 12332   4270563.0             232.128        33.7          2.345   \n",
       " 12333   4270563.0             232.128        33.7          2.345   \n",
       " 12334   4270563.0             232.128        33.7          2.345   \n",
       " 12335   4270563.0             232.128        33.7          2.345   \n",
       " 12336   4270563.0             232.128        33.7          2.345   \n",
       " \n",
       "        aged_70_older  gdp_per_capita  extreme_poverty  cvd_death_rate  \\\n",
       " 12170          1.114       65530.537              NaN         132.235   \n",
       " 12171          1.114       65530.537              NaN         132.235   \n",
       " 12172          1.114       65530.537              NaN         132.235   \n",
       " 12173          1.114       65530.537              NaN         132.235   \n",
       " 12174          1.114       65530.537              NaN         132.235   \n",
       " ...              ...             ...              ...             ...   \n",
       " 12332          1.114       65530.537              NaN         132.235   \n",
       " 12333          1.114       65530.537              NaN         132.235   \n",
       " 12334          1.114       65530.537              NaN         132.235   \n",
       " 12335          1.114       65530.537              NaN         132.235   \n",
       " 12336          1.114       65530.537              NaN         132.235   \n",
       " \n",
       "        diabetes_prevalence  female_smokers  male_smokers  \\\n",
       " 12170                15.84             2.7          37.0   \n",
       " 12171                15.84             2.7          37.0   \n",
       " 12172                15.84             2.7          37.0   \n",
       " 12173                15.84             2.7          37.0   \n",
       " 12174                15.84             2.7          37.0   \n",
       " ...                    ...             ...           ...   \n",
       " 12332                15.84             2.7          37.0   \n",
       " 12333                15.84             2.7          37.0   \n",
       " 12334                15.84             2.7          37.0   \n",
       " 12335                15.84             2.7          37.0   \n",
       " 12336                15.84             2.7          37.0   \n",
       " \n",
       "        handwashing_facilities  hospital_beds_per_thousand  Days  \n",
       " 12170                     NaN                         2.0     1  \n",
       " 12171                     NaN                         2.0     1  \n",
       " 12172                     NaN                         2.0     1  \n",
       " 12173                     NaN                         2.0     1  \n",
       " 12174                     NaN                         2.0     1  \n",
       " ...                       ...                         ...   ...  \n",
       " 12332                     NaN                         2.0     1  \n",
       " 12333                     NaN                         2.0     1  \n",
       " 12334                     NaN                         2.0     1  \n",
       " 12335                     NaN                         2.0     1  \n",
       " 12336                     NaN                         2.0     1  \n",
       " \n",
       " [167 rows x 19 columns], 241613.94510778785)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train\n",
    "Z = [0.5, 1]\n",
    "X_train.iloc[0]\n",
    "X = model.predict(X_train[0:1])\n",
    "\n",
    "\n",
    "# denormalized_d = X[0][0] * (max_d - min_d) + min_d\n",
    "\n",
    "llist = requiredData.new_cases.tolist()\n",
    "\n",
    "denormalized_d = X[0][0] * (max(llist) - min(llist)) + min(llist)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
